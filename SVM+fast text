# =================================================================
# Ø³Ù„ÙˆÙ„ Û±: Ù†ØµØ¨ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ (Word2Vec)
# =================================================================
%%time

# --- Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ---
# Ù†ØµØ¨ ØªÙ…Ø§Ù… Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø´Ø§Ù…Ù„ parsivarØŒ imbalanced-learn Ùˆ gensim
!pip install pandas numpy scikit-learn parsivar matplotlib seaborn imbalanced-learn openpyxl gensim --quiet

# --- Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ---
import pandas as pd
import numpy as np
import re
import io
import time
import math
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from parsivar import Normalizer, Tokenizer 
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, 
    cohen_kappa_score, recall_score, make_scorer, roc_auc_score, roc_curve, 
    precision_recall_fscore_support, precision_score, log_loss, mean_absolute_error,
    brier_score_loss
)
from sklearn.preprocessing import LabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.calibration import calibration_curve
from imblearn.pipeline import Pipeline as ImbPipeline 
from imblearn.over_sampling import SMOTE 
from sklearn.feature_extraction.text import TfidfVectorizer 
import gensim.models

# --- ØªÙˆØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§Ø±Ø³ÛŒ ---
normalizer = Normalizer()
tokenizer = Tokenizer()

def preprocess_text(text):
    """Ø§Ø¹Ù…Ø§Ù„ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø±ÙˆÛŒ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ."""
    if pd.isna(text):
        return ""
    # Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ ÙØ§Ø±Ø³ÛŒ (Ø§ØµÙ„Ø§Ø­ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡ØŒ Ú©â€ŒÙ‡Ø§ØŒ ÛŒâ€ŒÙ‡Ø§ Ùˆ...)
    text = normalizer.normalize(text)
    # Ø­Ø°Ù Ø§Ø¹Ø¯Ø§Ø¯
    text = re.sub(r'[Û°-Û¹]', '', text) 
    text = re.sub(r'[0-9]', '', text) 
    # Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ (Ø¨Ù‡ Ø¬Ø² Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ù…Ø¬Ø§Ø²)
    text = re.sub(r'[^\w\s]', '', text) 
    # Ø­Ø°Ù Ø²ÛŒØ±Ø®Ø·â€ŒÙ‡Ø§
    text = re.sub(r'_', '', text) 
    # ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ
    tokens = [token for token in tokenizer.tokenize_words(text) if token.strip()]
    return " ".join(tokens)


# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ùˆ Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ---
print("\nğŸ”„ Ù„Ø·ÙØ§ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ (Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'text' Ùˆ 'label') Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")
try:
    # Ø¨Ø®Ø´ Ø¢Ù¾Ù„ÙˆØ¯
    uploaded = files.upload()
    file_name = next(iter(uploaded))
    df = pd.read_excel(io.BytesIO(uploaded[file_name]))
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ú©ÙˆÚ†Ú©
    df.columns = df.columns.str.lower()
    if 'text' not in df.columns or 'label' not in df.columns:
         raise ValueError("ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'text' Ùˆ 'label' Ø¨Ø§Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† Ø­Ø³Ø§Ø³ÛŒØª Ø¨Ù‡ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯/Ú©ÙˆÚ†Ú©).")

    # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ NaN Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    df.dropna(subset=['text', 'label'], inplace=True)
    
    # Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
    df['cleaned_text'] = df['text'].apply(preprocess_text)
    
    # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­
    y_labels = df['label'].astype(int).values 
    LABELS = np.unique(y_labels).tolist() 
    
    # ğŸ’¡ ØªØ¹Ø±ÛŒÙ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø´Ù…Ø§)
    CLASS_NAMES = {
        0: 'Non-related', 
        1: 'Indirect Suicide Signs', 
        2: 'Direct Suicide Signs'
    }
    
    # --- ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (60% Ø¢Ù…ÙˆØ²Ø´ØŒ 30% Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒØŒ 10% ØªØ³Øª) ---
    X = df['cleaned_text']
    y = y_labels

    # 1. ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Training (60%) Ùˆ Temp (40%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=42, stratify=y
    )

    # 2. ØªÙ‚Ø³ÛŒÙ… Temp Ø¨Ù‡ Validation (30%) Ùˆ Test (10%)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )

    print(f"\nâœ… ÙØ§ÛŒÙ„ Ø¨Ø§ {len(df)} Ø³Ø·Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯.")
    print(f"\nğŸ“Š ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
    print(f"Ø¢Ù…ÙˆØ²Ø´ (Train): {len(X_train)} ({len(X_train) / len(df) * 100:.1f}%)")
    print(f"Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (Validation): {len(X_val)} ({len(X_val) / len(df) * 100:.1f}%)")
    print(f"ØªØ³Øª (Test): {len(X_test)} ({len(X_test) / len(df) * 100:.1f}%)")

except Exception as e:
    print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {e}")
    # Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´ÙˆØ¯ØŒ Ø§Ø¬Ø±Ø§ÛŒ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯
    # ØªÙˆØ¬Ù‡: Ø§Ú¯Ø± ÙØ§ÛŒÙ„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ø­Ø§ÙØ¸Ù‡ Colab Ø¨Ø§Ø´Ø¯ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØºÛŒÛŒØ± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.
    raise 
# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ SVM Ø¨Ø§ FastText Ùˆ SMOTE (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ)
# =================================================================
%%time

MODEL_NAME = "SVM_FastText_SMOTE" 

# --- Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (ÙØ±Ø¶ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¯Ø± Ø³Ù„ÙˆÙ„ Û± Ù…ÙˆØ¬ÙˆØ¯Ù†Ø¯) ---
from sklearn.base import BaseEstimator, TransformerMixin
import gensim.models.fasttext as FastTextModel 
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer, f1_score
from imblearn.pipeline import Pipeline as ImbPipeline # ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ImbPipeline
from imblearn.over_sampling import SMOTE 
from sklearn.svm import SVC
import numpy as np
import time

# --- Û±. ØªØ¹Ø±ÛŒÙ FastText Embedding (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‚Ø¨Ù„) ---

class FastTextVectorizer(BaseEstimator, TransformerMixin):
    """ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ù†Ø¯Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gensim FastText."""
    
    def __init__(self, vector_size=100, min_count=1, window=5, workers=4, min_n=3, max_n=6):
        self.vector_size = vector_size
        self.min_count = min_count
        self.window = window
        self.workers = workers
        self.min_n = min_n 
        self.max_n = max_n 
        self.fasttext_model = None

    def fit(self, X, y=None):
        tokenized_sentences = [text.split() for text in X]
        
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ FastText Ø¨Ø§ vector_size={self.vector_size}, min_n={self.min_n}...")
        
        self.fasttext_model = FastTextModel.FastText(
            sentences=tokenized_sentences,
            vector_size=self.vector_size,
            min_count=self.min_count,
            window=self.window,
            workers=self.workers,
            min_n=self.min_n, 
            max_n=self.max_n, 
            seed=42
        )
        self.fasttext_model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)
        print("âœ… Ø¢Ù…ÙˆØ²Ø´ FastText Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
        return self

    def transform(self, X):
        tokenized_sentences = [text.split() for text in X]
        
        def get_mean_vector(tokens):
            vectors = [self.fasttext_model.wv[word] for word in tokens]
            if vectors:
                return np.mean(vectors, axis=0)
            else:
                return np.zeros(self.vector_size)

        return np.array([get_mean_vector(tokens) for tokens in tokenized_sentences])


# --- ØªØ¹Ø±ÛŒÙ Ø§Ø¬Ø²Ø§ÛŒ Pipeline (Ø¨Ø§ SMOTE) ---
fasttext_vectorizer = FastTextVectorizer(vector_size=200) 
smote_sampler = SMOTE(random_state=42) # ğŸ‘ˆ Ù…Ø±Ø­Ù„Ù‡ Ø¬Ø¯ÛŒØ¯: Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ
svm_classifier = SVC(random_state=42, probability=True, decision_function_shape='ovr') 

# ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ImbPipeline Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…Ø±Ø§Ø­Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ù…ØªØ¹Ø§Ø¯Ù„â€ŒØ³Ø§Ø²ÛŒ
pipeline = ImbPipeline([
    ('embedding', fasttext_vectorizer),
    ('smote', smote_sampler), # ğŸ‘ˆ SMOTE Ø¨Ø¹Ø¯ Ø§Ø² Embedding
    ('clf', svm_classifier) 
])

# --- ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (Grid Search) ---
param_grid = {
    # FastText parameters
    'embedding__vector_size': [100, 200], 
    # SMOTE parameter (ØªØ¹Ø¯Ø§Ø¯ Ù‡Ù…Ø³Ø§ÛŒÙ‡â€ŒÙ‡Ø§)
    'smote__k_neighbors': [3, 5], 
    # SVC parameters
    'clf__C': [1, 10], 
    'clf__kernel': ['linear'] 
}

f1_macro_scorer = make_scorer(f1_score, average='macro')

print("ğŸš€ Ø´Ø±ÙˆØ¹ Grid Search Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
grid_search_start = time.time()

# ğŸš¨ Ø§Ø¬Ø±Ø§ÛŒ Grid Search
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring=f1_macro_scorer,
    cv=3, 
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {best_params}")

# Û´. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
# =================================================================
# Ø³Ù„ÙˆÙ„ Û³: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Word2Vec)
# =================================================================
%%time

from sklearn.metrics import log_loss, mean_absolute_error, roc_auc_score, precision_recall_fscore_support, confusion_matrix

# --- Ø§Ù„Ù: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û±) ---
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')
recall_macro = recall_score(y_test, y_pred, average='macro')
precision_macro = precision_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
logloss = log_loss(y_test, y_prob)
mae = mean_absolute_error(y_test, y_pred)

lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC OVR Ùˆ OVO
try:
    if len(LABELS) > 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob, multi_class='ovr')
        auc_roc_ovo = roc_auc_score(y_test, y_prob, multi_class='ovo')
    elif len(LABELS) == 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob[:, 1])
        auc_roc_ovo = "N/A (Binary)"
    else:
        auc_roc_ovr = "N/A"
        auc_roc_ovo = "N/A"
except Exception: 
    auc_roc_ovr = "Error"
    auc_roc_ovo = "Error"

results_overall = {
    'Model': [MODEL_NAME], 'Embedding': ['Word2Vec (Gensim)'],
    'Accuracy': [accuracy], 'F1-Macro': [f1_macro], 'Recall-Macro': [recall_macro],
    'Precision-Macro': [precision_macro], 'Kappa': [kappa], 'MCC': [mcc],
    'AUC-ROC (OVR)': [auc_roc_ovr], 'AUC-ROC (OVO)': [auc_roc_ovo],
    'Log Loss': [logloss], 'MAE': [mae],
    'Training Time (s)': [training_time]
}
df_overall = pd.DataFrame(results_overall)


# --- Ø¨: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û²) ---
precision_c, recall_c, f1_c, support_c = precision_recall_fscore_support(
    y_test, y_pred, labels=LABELS, average=None
)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ SpecificityØŒ NPV Ùˆ AUC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
specificity_c = []
npv_c = []
auc_c = []

for i, label in enumerate(LABELS):
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ TN, FP, FN, TP Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ (True vs. Not-True)
    cm_binary = confusion_matrix(y_test == label, y_pred == label, labels=[True, False])
    
    if cm_binary.size == 4:
        tn, fp, fn, tp = cm_binary.ravel()
    else:
        tn, fp, fn, tp = 0, 0, 0, 0

    # Specificity
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 
    specificity_c.append(specificity)

    # NPV
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0 
    npv_c.append(npv)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
    try:
        auc_class = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])
        auc_c.append(auc_class)
    except Exception:
        auc_c.append(np.nan)


df_class_metrics = pd.DataFrame({
    'Class': [CLASS_NAMES.get(i, f'Class {i}') for i in LABELS],
    'Precision': precision_c,
    'Recall (Sensitivity)': recall_c, 
    'Specificity': specificity_c,
    'F1-Score': f1_c,
    'NPV (Negative Predictive Value)': npv_c,
    'AUC (OVR)': auc_c,
    'Support': support_c
})

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø¬Ø¯ÙˆÙ„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
df_class_metrics.loc[len(df_class_metrics)] = {
    'Class': 'Macro Average',
    'Precision': precision_macro, 
    'Recall (Sensitivity)': recall_macro, 
    'F1-Score': f1_macro, 
    'Specificity': np.mean(specificity_c), 
    'NPV (Negative Predictive Value)': np.mean(npv_c),
    'AUC (OVR)': auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan,
    'Support': np.sum(support_c)
}


# --- Ø¬: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û³ - ØºÛŒØ±Ù‚Ø§Ø¨Ù„ Ø§Ø¬Ø±Ø§ Ø¨Ø±Ø§ÛŒ Word2Vec) ---
key_words_df = pd.DataFrame({
    'Feature': ['N/A (Word2Vec Embeddings are dense vectors)'],
    'Weight': ['Key word analysis not applicable for this embedding type.']
}) 
print("âš ï¸ Ø§Ø®Ø·Ø§Ø±: ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ EmbeddingÙ‡Ø§ÛŒ Word2Vec Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ù†ÛŒØ³Øª Ùˆ Ø­Ø°Ù Ø´Ø¯.")


# --- Ø¯: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ ---
output_file = f'{MODEL_NAME}_Evaluation_Reports.xlsx'
from google.colab import files # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ù…Ø¬Ø¯Ø¯ Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²

with pd.ExcelWriter(output_file) as writer:
    df_overall.to_excel(writer, sheet_name='1_Overall_Metrics', index=False)
    df_class_metrics.to_excel(writer, sheet_name='2_Class_Metrics', index=False)
    key_words_df.to_excel(writer, sheet_name='3_Key_Words_Analysis', index=False)

files.download(output_file)
print(f"\nâœ… Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û³ (Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ Ø¢Ù…Ø§Ø±ÛŒ):")
# =================================================================
# Ø³Ù„ÙˆÙ„ Û´: ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¨ØµØ±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ)
# =================================================================
%%time

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ ---
plt.style.use('seaborn-v0_8-whitegrid')
CLASS_LABELS_EN = list(CLASS_NAMES.values())
COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c'] 
# AUC OVR Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Û³ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
auc_roc_ovr_plot = auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan

# Û±. Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ (Metrics Bar Chart)
plt.figure(figsize=(14, 7))
metrics_to_plot = ['Precision', 'Recall (Sensitivity)', 'F1-Score', 'Specificity', 'NPV (Negative Predictive Value)'] 
df_plot = df_class_metrics[df_class_metrics['Class'] != 'Macro Average'].reset_index(drop=True)
macro_avg_row = df_class_metrics[df_class_metrics['Class'] == 'Macro Average']
width = 0.15 

x = np.arange(len(metrics_to_plot))
# ØªØ±Ø³ÛŒÙ… Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
for i, class_name in enumerate(df_plot['Class']):
    plt.bar(x + i * width, df_plot.iloc[i][metrics_to_plot], width=width, label=class_name, color=COLORS[i])

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§Ú©Ø±Ùˆ (Macro Average) Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ù‚Ø·Ù‡
for i, metric in enumerate(metrics_to_plot):
    plt.plot(x[i] + width * (len(LABELS) - 1) / 2, 
             macro_avg_row[metric].iloc[0], 
             'o', color='black', markersize=8, zorder=10, 
             label='Macro Avg' if i == 0 else "")

plt.xticks(x + width * (len(LABELS) - 1) / 2, metrics_to_plot, rotation=15)
plt.ylabel('Score Value', fontsize=12)
plt.title(f'Comparative Metrics per Class and Macro Average ({MODEL_NAME})', fontsize=14)
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()


# Û². Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix)
cm = confusion_matrix(y_test, y_pred, labels=LABELS)
cm_df = pd.DataFrame(cm, index=CLASS_LABELS_EN, columns=CLASS_LABELS_EN)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='black', 
            annot_kws={"size": 14})
plt.title('Confusion Matrix (Test Data)', fontsize=16)
plt.ylabel('True Label', fontsize=14)
plt.xlabel('Predicted Label', fontsize=14)
plt.show()

# Û³. Ù†Ù…ÙˆØ¯Ø§Ø± ROC (Receiver Operating Characteristic)
plt.figure(figsize=(10, 7))
auc_scores = []

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ ØªØ±Ø³ÛŒÙ… ROC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ (One-vs-Rest)
for i in range(len(LABELS)):
    try:
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
        auc_score = df_class_metrics.iloc[i]['AUC (OVR)'] 
        plt.plot(fpr, tpr, color=COLORS[i], label=f'{CLASS_LABELS_EN[i]} (AUC = {auc_score:.2f})')
    except Exception as e:
        print(f"âš ï¸ Ø§Ø®Ø·Ø§Ø±: Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªØ±Ø³ÛŒÙ… ROC Ú©Ù„Ø§Ø³ {CLASS_LABELS_EN[i]} Ø±Ø® Ø¯Ø§Ø¯: {e}")

# ØªØ±Ø³ÛŒÙ… Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§Ú©Ø±Ùˆ (ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
if not np.isnan(auc_roc_ovr_plot):
    mean_fpr = np.linspace(0.0, 1.0, 100) 
    # Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù…Ù†Ø­Ù†ÛŒâ€ŒÙ‡Ø§ÛŒ ROC Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø±ÙˆÙ†â€ŒÛŒØ§Ø¨ÛŒ
    mean_tpr = np.mean([np.interp(mean_fpr, *roc_curve(y_test_binarized[:, i], y_prob[:, i])[:2]) for i in range(len(LABELS))], axis=0)
    mean_tpr[0] = 0.0
    plt.plot(mean_fpr, mean_tpr, color='black', linestyle='--', label=f'Macro Average (AUC = {auc_roc_ovr_plot:.2f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve per Class (One-vs-Rest Strategy)', fontsize=16)
plt.legend(loc="lower right")
plt.show()


# Û´. Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† (Calibration Curve)
plt.figure(figsize=(10, 7))
plt.plot([0, 1], [0, 1], "r--", label="Perfectly Calibrated")
brier_scores = []

for i in range(len(LABELS)):
    y_true_class = (y_test == LABELS[i])
    y_prob_class = y_prob[:, i]
    
    brier = brier_score_loss(y_true_class, y_prob_class)
    brier_scores.append(brier)
    
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_true_class, y_prob_class, n_bins=10
    )
    
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", 
             label=f'{CLASS_LABELS_EN[i]} (Brier: {brier:.2f})', color=COLORS[i])

mean_brier = np.mean(brier_scores)
plt.text(0.1, 0.8, f'Mean Brier Score: {mean_brier:.2f}', fontsize=12, color='black', transform=plt.gca().transAxes)
plt.text(0.1, 0.75, f'Overall Log Loss: {logloss:.4f}', fontsize=12, color='black', transform=plt.gca().transAxes)


plt.ylabel("Fraction of Positives", fontsize=12)
plt.xlabel("Mean Predicted Probability", fontsize=12)
plt.title("Calibration Curve (Predicted vs. True Probability)", fontsize=16)
plt.legend(loc="lower right")
plt.show()

print("\nâœ… ØªÙ…Ø§Ù…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û´ (ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§):")

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û± (Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡):")
