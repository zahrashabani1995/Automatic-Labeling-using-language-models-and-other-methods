# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (Parsivar Ø¨Ø±Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ)
!pip install parsivar scikit-learn numpy scipy pandas openpyxl
!pip install --upgrade scikit-learn

# âš ï¸âš ï¸âš ï¸ Ø§Ø¬Ø±Ø§ÛŒ Ø¯Ø³ØªÙˆØ± Ø±ÛŒØ³ØªØ§Ø±Øª Ø¨Ø±Ø§ÛŒ ØªØ¶Ù…ÛŒÙ† Ù„ÙˆØ¯ Ø´Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ âš ï¸âš ï¸âš ï¸
# Ù¾Ø³ Ø§Ø² Ø§Ø¬Ø±Ø§ÛŒ Ù…ÙˆÙÙ‚ÛŒØªâ€ŒØ¢Ù…ÛŒØ² Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ØŒ Ø¨Ø§ÛŒØ¯ Runtime Ø±Ø§ Ø±ÛŒØ³ØªØ§Ø±Øª Ú©Ù†ÛŒØ¯.
import os
print("âœ… Ù†ØµØ¨â€ŒÙ‡Ø§ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Runtime > Restart runtime Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
# os.kill(os.getpid(), 9) # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡ Ø¯Ø§Ø¯ØŒ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯

# --- Ø¨Ø®Ø´ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (Ù¾Ø³ Ø§Ø² Ø±ÛŒØ³ØªØ§Ø±Øª) ---

import pandas as pd
from google.colab import files
import io
import re
import numpy as np
from parsivar import Normalizer, Tokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split

# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ---
print("Ù„Ø·ÙØ§ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded = files.upload()
file_name = next(iter(uploaded))

df = pd.read_excel(io.BytesIO(uploaded[file_name]))

# ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø³ØªÙˆÙ† Ø§ÙˆÙ„ Ù…ØªÙ† Ùˆ Ø³ØªÙˆÙ† Ø¯ÙˆÙ… Ù„ÛŒØ¨Ù„ Ø¨Ø§Ø´Ø¯.
df.columns = ['text', 'label'] 
print(f"âœ… ÙØ§ÛŒÙ„ Ø¨Ø§ {len(df)} Ø³Ø·Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯.")

# --- Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Parsivar) ---
normalizer = Normalizer()
tokenizer = Tokenizer()
# Ù„ÛŒØ³Øª Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù Ø¹Ù…ÙˆÙ…ÛŒ ÙØ§Ø±Ø³ÛŒ (Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ù„ÛŒØ³Øª Ø¯Ù‚ÛŒÙ‚ØªØ± Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†ÛŒØ¯)
# Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒØŒ ÛŒÚ© Ù„ÛŒØ³Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
stop_words = set([
    "Ø§Ø²", "Ø¨Ù‡", "Ø¨Ø§", "Ø¯Ø±", "Ø¨Ø±", "Ø¨Ø±Ø§ÛŒ", "Ú©Ù‡", "Ùˆ", "ÛŒØ§", "ÛŒÚ©", "Ø§ÛŒÙ†", "Ø¢Ù†",
    "Ù‡Ø§", "Ø§ÛŒ", "Ø±Ø§", "Ù‡Ù…", "Ø¨ÙˆØ¯", "Ø§Ø³Øª", "Ø¨Ø§Ø´Ø¯", "Ø´Ø¯", "Ù…ÛŒ", "Ù‡Ù…ÛŒÙ†", "Ú†Ù†ÛŒÙ†",
    "Ø§Ù…Ø§", "Ø§Ú¯Ø±", "Ú†ÙˆÙ†", "ØªØ§", "Ù…Ø§", "Ù…Ù†", "ØªÙˆ", "Ø§Ùˆ", "Ø´Ù…Ø§", "Ø§ÛŒØ´Ø§Ù†"
])

def preprocess_text(text):
    """ØªØ§Ø¨Ø¹ Ø¬Ø§Ù…Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Parsivar"""
    if pd.isna(text) or not text:
        return ""
    
    # Û±. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ
    text = normalizer.normalize(str(text))
    
    # Û². Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ùˆ Ù‡Ø´ØªÚ¯â€ŒÙ‡Ø§
    text = re.sub(r'http\S+|www\S+|#\w+|@\w+', '', text, flags=re.MULTILINE)
    
    # Û³. Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø¶Ø±ÙˆØ±ÛŒ
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text) 

    # Û´. Ø­Ø°Ù Ú†Ù†Ø¯ ÙØ§ØµÙ„Ù‡ Ø§Ø¶Ø§ÙÛŒ
    text = re.sub(r'\s+', ' ', text).strip()
    
    # Ûµ. ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª ØªÙˆÙ‚Ù
    tokens = tokenizer.tokenize_words(text)
    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]
    
    return ' '.join(filtered_tokens)

# Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
df['cleaned_text'] = df['text'].apply(preprocess_text)
print("âœ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒØŒ Ø­Ø°Ù Stop-Words Ùˆ...) Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

# --- Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ Ø¨Ø§ TF-IDF ---
vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)
X = vectorizer.fit_transform(df['cleaned_text']).toarray()
y = df['label'].values
from sklearn.svm import SVC

# --- Ø§Ù„Ù: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Û¶Û°:Û³Û°:Û±Û°) ---
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.4, random_state=42, stratify=y
)
X_test, X_val, y_test, y_val = train_test_split(
    X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
)

print(f"âœ… ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: Train: {len(X_train)}, Test: {len(X_test)}, Validation: {len(X_val)}")

# --- Ø¨: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM (ØªÙ†Ø¸ÛŒÙ… C Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Overfitting) ---
# C=1.0 ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø®ÙˆØ¨ Ø§Ø³Øª. Ø§Ú¯Ø± Overfitting Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø´Ø¯ØŒ C Ø±Ø§ Ú©Ù… Ú©Ù†ÛŒØ¯ (Ù…Ø«Ù„ 0.5)
svm_model = SVC(
    kernel='linear', 
    C=1.0, 
    random_state=42, 
    probability=True # Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC-ROC
)

print("\nğŸš€ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM Ø¢ØºØ§Ø² Ø´Ø¯...")
svm_model.fit(X_train, y_train)
print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ SVM Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

# --- Ø¬: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª ---
y_pred = svm_model.predict(X_test)
y_prob = svm_model.predict_proba(X_test)

print(f"âœ… Ù…Ù‡Ù†Ø¯Ø³ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒ (TF-IDF) Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯. Ø§Ø¨Ø¹Ø§Ø¯ Ù…Ø§ØªØ±ÛŒØ³ ÙˆÛŒÚ˜Ú¯ÛŒ: {X.shape}")
