import pandas as pd
import re
from google.colab import files
import io
from transformers import AutoModel, AutoTokenizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error
from sklearn.preprocessing import label_binarize
from sklearn.calibration import CalibrationDisplay
from scipy.stats import chi2_contingency
from imblearn.over_sampling import SMOTE
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

# زمان‌سنجی برای پردازش
start_time = time.time()

# تعریف واژگان دسته اول و دوم (برای ستون‌های اضافی خروجی)
category_1 = [
    "خودکشی", "آماده خودکشی", "نقشه خودکشی", "مرگ خودخواسته", "می‌خواهم خودم را بکشم",
    "تا ابد بخوابم", "هیچوقت بیدار نشم", "به زندگی‌ام پایان می‌دهم", "نمی‌تونم ادامه بدم",
    "نمی‌خوام زنده باشم", "خودسوزی", "زندگی برای من تمام شده", "خودم را خلاص کنم",
    "خودمو خلاص کنم", "خودم رو خلاص کنم", "چرا نباید خودم را بکشم؟", "#خودکشی"
]

category_2 = [
    "افسردگی", "دارم عذاب می‌کشم", "هیچ امیدی ندارم", "از خودم متنفرم", "از خودم بدم میاد",
    "چیزی برای از دست دادن ندارم", "از زندگی خسته‌ام", "تنها می‌میرم", "مرگ", "درد", "ناامیدی",
    "تنهایی", "غم", "بی‌ارزش", "خستگی", "فشار رومه", "استرس", "استرس دارم", "همه چیز بی‌فایده است",
    "هیچکس مرا نمی‌فهمد", "از همه چیز متنفرم", "#افسردگی", "#تنهایی", "#درد", "#غم", "#ناامیدی"
]

# کلمات زمینه برای تفکیک خودزنی واقعی و استعاری
serious_context = [
    "قرص", "خون", "آسیب", "گریه", "جیغ", "درد", "افسردگی", "خودکشی", "مرگ", "ناامیدی"
]
humorous_context = [
    "😂", "😁", "🤣", "سوتی", "کصخل", "خنده", "شوخی", "فان", "ههه", "هاها"
]

# تابع برای بازسازی ستون‌های اضافی (برای خروجی)
def reconstruct_metadata(tweet, label):
    tweet = str(tweet).lower()
    
    if "خودزنی" in tweet:
        if any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in serious_context):
            return "دسته اول", "خودزنی", 0.9
        elif any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in humorous_context) or "😂" in tweet or "😁" in tweet:
            return "هیچ‌کدام", "خودزنی", 0.3
        else:
            return "هیچ‌کدام", "خودزنی", 0.3
    
    for word in category_1:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "دسته اول", word, 0.9
    
    for word in category_2:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "دسته دوم", word, 0.65
    
    return "هیچ‌کدام", "هیچ‌کدام", 0.3

# تابع برای محاسبه Specificity
def specificity_score(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred, labels=classes)
    specificity_per_class = []
    for i in range(len(classes)):
        tn = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
        fp = np.sum(cm[:, i]) - cm[i, i]
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        specificity_per_class.append(specificity)
    return np.mean(specificity_per_class)

# تابع برای تولید امبدینگ با ParsBERT
def get_parsbert_embeddings(texts, model, tokenizer, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.to(device)
    model.eval()
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token
        embeddings.append(batch_embeddings)
    
    return np.vstack(embeddings)

# تعریف شبکه DQN
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# تعریف محیط تقویتی
class TweetEnvironment:
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels
        self.current_idx = 0
    
    def reset(self):
        self.current_idx = 0
        return self.embeddings[0]
    
    def step(self, action):
        reward = 1 if action == self.labels[self.current_idx] else -1
        self.current_idx += 1
        done = self.current_idx >= len(self.embeddings)
        next_state = self.embeddings[self.current_idx] if not done else None
        return next_state, reward, done

# کلاس عامل DQN (با اصلاح خطا)
class DQNAgent:
    def __init__(self, state_dim, action_dim, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        self.q_network = DQN(state_dim, action_dim).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        self.memory = deque(maxlen=10000)
        self.gamma = 0.99  # Discount factor
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.batch_size = 32
    
    def act(self, state):
        if random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state)
        return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # مدیریت حالات None در next_states
        non_none_indices = [i for i, ns in enumerate(next_states) if ns is not None]
        none_indices = [i for i, ns in enumerate(next_states) if ns is None]
        
        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        target_q = torch.zeros(self.batch_size).to(self.device)
        
        if non_none_indices:
            non_none_states = torch.FloatTensor(np.array([next_states[i] for i in non_none_indices])).to(self.device)
            next_q_values = self.q_network(non_none_states).max(1)[0].detach()
            for i, idx in enumerate(non_none_indices):
                target_q[idx] = rewards[idx] + (1 - dones[idx]) * self.gamma * next_q_values[i]
        
        for idx in none_indices:
            target_q[idx] = rewards[idx]  # برای حالات None، فقط پاداش فعلی
        
        loss = nn.MSELoss()(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

# نصب کتابخانه‌ها
!pip install transformers imbalanced-learn scikit-learn matplotlib seaborn torch

# آپلود فایل آموزشی
print("لطفاً فایل آموزشی 'labeled_tweets_3000.xlsx' را آپلود کنید:")
uploaded_train = files.upload()

if not uploaded_train:
    raise ValueError("فایل آموزشی آپلود نشد. لطفاً فایل 'labeled_tweets_3000.xlsx' را آپلود کنید.")

# خواندن فایل آموزشی
train_file = list(uploaded_train.keys())[0]
df_train = pd.read_excel(io.BytesIO(uploaded_train[train_file]))

# اطمینان از وجود ستون‌های مورد نیاز
required_columns = ['متن توییت', 'برچسب']
if not all(col in df_train.columns for col in required_columns):
    raise ValueError("ستون‌های 'متن توییت' یا 'برچسب' در فایل آموزشی یافت نشد.")

# آماده‌سازی داده‌های آموزشی
X_train_texts = df_train['متن توییت'].astype(str).tolist()
y_train = df_train['برچسب'].astype(int).values

# بارگذاری مدل و توکنایزر ParsBERT
model_name = "HooshvareLab/bert-fa-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# تبدیل متن‌های آموزشی به امبدینگ
print("در حال تولید امبدینگ‌های آموزشی با ParsBERT...")
X_train_embeddings = get_parsbert_embeddings(X_train_texts, model, tokenizer, batch_size=32)

# اعمال SMOTE برای رفع نامتوازنی
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_embeddings, y_train)

# آموزش مدل طبقه‌بند RandomForest (برای پیش‌بینی اولیه)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train_balanced, y_train_balanced)

# آموزش عامل DQN روی داده‌های آموزشی
env = TweetEnvironment(X_train_balanced, y_train_balanced)
agent = DQNAgent(state_dim=X_train_balanced.shape[1], action_dim=3)
episodes = 100
print("در حال آموزش عامل DQN روی داده‌های آموزشی...")
for episode in range(episodes):
    state = env.reset()
    done = False
    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)
        agent.remember(state, action, reward, next_state, done)
        agent.replay()
        state = next_state if not done else state
    print(f"اپیزود {episode + 1}/{episodes} تمام شد.")

# آپلود فایل جدید
print("لطفاً فایل جدید 'new_tweets.xlsx' را آپلود کنید:")
uploaded_new = files.upload()

if not uploaded_new:
    raise ValueError("فایل جدید آپلود نشد. لطفاً فایل 'new_tweets.xlsx' را آپلود کنید.")

# خواندن فایل جدید
new_file = list(uploaded_new.keys())[0]
df_new = pd.read_excel(io.BytesIO(uploaded_new[new_file]))

# اطمینان از وجود ستون‌های مورد نیاز
if 'متن توییت' not in df_new.columns or 'برچسب' not in df_new.columns:
    raise ValueError("ستون‌های 'متن توییت' یا 'برچسب' در فایل جدید یافت نشد.")

# تبدیل متن‌های جدید به امبدینگ
print("در حال تولید امبدینگ‌های داده‌های جدید با ParsBERT...")
X_new_texts = df_new['متن توییت'].astype(str).tolist()
X_new_embeddings = get_parsbert_embeddings(X_new_texts, model, tokenizer, batch_size=32)
y_true_new = df_new['برچسب'].astype(int).values

# پیش‌بینی لیبل‌ها با DQN
y_pred_new = []
for state in X_new_embeddings:
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
    q_values = agent.q_network(state_tensor)
    action = q_values.argmax().item()
    y_pred_new.append(action)
y_pred_new = np.array(y_pred_new)

# اصلاح برچسب‌ها برای خودزنی استعاری
for i, (tweet, label) in enumerate(zip(X_new_texts, y_pred_new)):
    tweet_lower = tweet.lower()
    if "خودزنی" in tweet_lower:
        if any(word in tweet_lower for word in humorous_context) or "😂" in tweet_lower or "😁" in tweet_lower:
            y_pred_new[i] = 0  # خودزنی استعاری: برچسب 0

# محاسبه معیارهای ارزیابی
accuracy = accuracy_score(y_true_new, y_pred_new)
precision_weighted = precision_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
recall_weighted = recall_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
f1_weighted = f1_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
precision_macro = precision_score(y_true_new, y_pred_new, average='macro', zero_division=0)
recall_macro = recall_score(y_true_new, y_pred_new, average='macro', zero_division=0)
f1_macro = f1_score(y_true_new, y_pred_new, average='macro', zero_division=0)
specificity_macro = specificity_score(y_true_new, y_pred_new, classes=[0, 1, 2])

# محاسبه AUC
y_true_bin = label_binarize(y_true_new, classes=[0, 1, 2])
y_pred_prob = np.zeros((len(y_pred_new), 3))
for i, state in enumerate(X_new_embeddings):
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
    q_values = agent.q_network(state_tensor).detach().cpu().numpy()
    y_pred_prob[i] = np.exp(q_values) / np.sum(np.exp(q_values))  # Softmax for probabilities
auc = roc_auc_score(y_true_bin, y_pred_prob, multi_class='ovr', average='weighted')

# محاسبه RMSE
rmse = np.sqrt(mean_squared_error(y_true_new, y_pred_new))

# محاسبه Confusion Matrix
cm = confusion_matrix(y_true_new, y_pred_new, labels=[0, 1, 2])

# رسم و ذخیره منحنی ROC
plt.figure(figsize=(8, 6))
for i in range(3):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])
    plt.plot(fpr, tpr, label=f'ROC منحنی کلاس {i} (AUC = {roc_auc_score(y_true_bin[:, i], y_pred_prob[:, i]):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('نرخ مثبت کاذب (False Positive Rate)')
plt.ylabel('نرخ مثبت واقعی (True Positive Rate)')
plt.title('منحنی ROC برای هر کلاس')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.show()
files.download('roc_curve.png')

# رسم و ذخیره ماتریس کالیبراسیون
plt.figure(figsize=(8, 6))
for i in range(3):
    disp = CalibrationDisplay.from_predictions(y_true_bin[:, i], y_pred_prob[:, i], n_bins=10, name=f'کلاس {i}')
    disp.plot(ax=plt.gca(), name=f'کلاس {i}')
plt.title('ماتریس کالیبراسیون برای هر کلاس')
plt.savefig('calibration_plot.png')
plt.show()
files.download('calibration_plot.png')

# رسم و ذخیره ماتریس درهم‌ریختگی
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])
plt.xlabel('برچسب پیش‌بینی‌شده')
plt.ylabel('برچسب واقعی')
plt.title('ماتریس درهم‌ریختگی')
plt.savefig('confusion_matrix.png')
plt.show()
files.download('confusion_matrix.png')

# آزمون آماری معناداری (Chi-Square)
cm_flat = cm.ravel()
if len(cm_flat) == 9:  # اطمینان از ماتریس 3x3
    chi2, p_value, _, _ = chi2_contingency(cm)
else:
    chi2, p_value = np.nan, np.nan

# چاپ معیارهای ارزیابی
print("معیارهای ارزیابی مدل روی داده‌های جدید (مقایسه با لیبل‌های واقعی):")
print(f"1. دقت (Accuracy): {accuracy:.2%}")
print(f"2. دقت مثبت (Precision, weighted): {precision_weighted:.2%}")
print(f"3. یادآوری (Recall, weighted): {recall_weighted:.2%}")
print(f"4. امتیاز F1 (F1-Score, weighted): {f1_weighted:.2%}")
print(f"5. دقت مثبت (Precision, macro): {precision_macro:.2%}")
print(f"6. یادآوری (Recall, macro): {recall_macro:.2%}")
print(f"7. امتیاز F1 (F1-Score, macro): {f1_macro:.2%}")
print(f"8. ویژگی (Specificity, macro): {specificity_macro:.2%}")
print(f"AUC (weighted): {auc:.2%}")
print(f"RMSE: {rmse:.4f}")
print("\nماتریس درهم‌ریختگی (Confusion Matrix):")
print(cm)
print(f"\nآزمون Chi-Square: مقدار chi2 = {chi2:.2f}, مقدار p-value = {p_value:.4f}")
print("\nگزارش کامل طبقه‌بندی:")
print(classification_report(y_true_new, y_pred_new, zero_division=0))

# ایجاد دیتافریم خروجی
df_output = df_new.copy()[['متن توییت']]
df_output['برچسب'] = y_pred_new
df_output['مجموعه واژگان'] = ''
df_output['واژه/عبارت مشابه'] = ''
df_output['آستانه شباهت'] = 0.0

# بازسازی ستون‌های اضافی
for index, row in df_output.iterrows():
    category, matched_word, similarity = reconstruct_metadata(row['متن توییت'], row['برچسب'])
    df_output.at[index, 'مجموعه واژگان'] = category
    df_output.at[index, 'واژه/عبارت مشابه'] = matched_word
    df_output.at[index, 'آستانه شباهت'] = similarity

# ذخیره فایل اکسل خروجی
output_excel = "new_labeled_tweets.xlsx"
df_output.to_excel(output_excel, index=False)

# ذخیره فایل متنی با جداکننده |
output_text = "new_labeled_tweets.txt"
df_output.to_csv(output_text, sep='|', index=False, encoding='utf-8')

print(f"فایل اکسل ذخیره شد: {output_excel}")
print(f"فایل متنی ذخیره شد: {output_text}")
print(f"زمان کل پردازش: {time.time() - start_time:.2f} ثانیه")

# دانلود خودکار فایل‌های خروجی
files.download(output_excel)
files.download(output_text)
