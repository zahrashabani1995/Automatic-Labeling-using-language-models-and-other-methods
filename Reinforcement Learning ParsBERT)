import pandas as pd
import re
from google.colab import files
import io
from transformers import AutoModel, AutoTokenizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error
from sklearn.preprocessing import label_binarize
from sklearn.calibration import CalibrationDisplay
from scipy.stats import chi2_contingency
from imblearn.over_sampling import SMOTE
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from collections import deque
import random

# Ø²Ù…Ø§Ù†â€ŒØ³Ù†Ø¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
start_time = time.time()

# ØªØ¹Ø±ÛŒÙ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ Ùˆ Ø¯ÙˆÙ… (Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø®Ø±ÙˆØ¬ÛŒ)
category_1 = [
    "Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ø¢Ù…Ø§Ø¯Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù†Ù‚Ø´Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯ Ø®ÙˆØ¯Ø®ÙˆØ§Ø³ØªÙ‡", "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…",
    "ØªØ§ Ø§Ø¨Ø¯ Ø¨Ø®ÙˆØ§Ø¨Ù…", "Ù‡ÛŒÚ†ÙˆÙ‚Øª Ø¨ÛŒØ¯Ø§Ø± Ù†Ø´Ù…", "Ø¨Ù‡ Ø²Ù†Ø¯Ú¯ÛŒâ€ŒØ§Ù… Ù¾Ø§ÛŒØ§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù…", "Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù…",
    "Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø²Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ù…", "Ø®ÙˆØ¯Ø³ÙˆØ²ÛŒ", "Ø²Ù†Ø¯Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù† ØªÙ…Ø§Ù… Ø´Ø¯Ù‡", "Ø®ÙˆØ¯Ù… Ø±Ø§ Ø®Ù„Ø§Øµ Ú©Ù†Ù…",
    "Ø®ÙˆØ¯Ù…Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ø®ÙˆØ¯Ù… Ø±Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ú†Ø±Ø§ Ù†Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…ØŸ", "#Ø®ÙˆØ¯Ú©Ø´ÛŒ"
]

category_2 = [
    "Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø¯Ø§Ø±Ù… Ø¹Ø°Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ø´Ù…", "Ù‡ÛŒÚ† Ø§Ù…ÛŒØ¯ÛŒ Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ù…ØªÙ†ÙØ±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ø¨Ø¯Ù… Ù…ÛŒØ§Ø¯",
    "Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ø®Ø³ØªÙ‡â€ŒØ§Ù…", "ØªÙ†Ù‡Ø§ Ù…ÛŒâ€ŒÙ…ÛŒØ±Ù…", "Ù…Ø±Ú¯", "Ø¯Ø±Ø¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ",
    "ØªÙ†Ù‡Ø§ÛŒÛŒ", "ØºÙ…", "Ø¨ÛŒâ€ŒØ§Ø±Ø²Ø´", "Ø®Ø³ØªÚ¯ÛŒ", "ÙØ´Ø§Ø± Ø±ÙˆÙ…Ù‡", "Ø§Ø³ØªØ±Ø³", "Ø§Ø³ØªØ±Ø³ Ø¯Ø§Ø±Ù…", "Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¨ÛŒâ€ŒÙØ§ÛŒØ¯Ù‡ Ø§Ø³Øª",
    "Ù‡ÛŒÚ†Ú©Ø³ Ù…Ø±Ø§ Ù†Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯", "Ø§Ø² Ù‡Ù…Ù‡ Ú†ÛŒØ² Ù…ØªÙ†ÙØ±Ù…", "#Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "#ØªÙ†Ù‡Ø§ÛŒÛŒ", "#Ø¯Ø±Ø¯", "#ØºÙ…", "#Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"
]

# Ú©Ù„Ù…Ø§Øª Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø®ÙˆØ¯Ø²Ù†ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ
serious_context = [
    "Ù‚Ø±Øµ", "Ø®ÙˆÙ†", "Ø¢Ø³ÛŒØ¨", "Ú¯Ø±ÛŒÙ‡", "Ø¬ÛŒØº", "Ø¯Ø±Ø¯", "Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"
]
humorous_context = [
    "ðŸ˜‚", "ðŸ˜", "ðŸ¤£", "Ø³ÙˆØªÛŒ", "Ú©ØµØ®Ù„", "Ø®Ù†Ø¯Ù‡", "Ø´ÙˆØ®ÛŒ", "ÙØ§Ù†", "Ù‡Ù‡Ù‡", "Ù‡Ø§Ù‡Ø§"
]

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ (Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ)
def reconstruct_metadata(tweet, label):
    tweet = str(tweet).lower()
    
    if "Ø®ÙˆØ¯Ø²Ù†ÛŒ" in tweet:
        if any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in serious_context):
            return "Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.9
        elif any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in humorous_context) or "ðŸ˜‚" in tweet or "ðŸ˜" in tweet:
            return "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.3
        else:
            return "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.3
    
    for word in category_1:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„", word, 0.9
    
    for word in category_2:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "Ø¯Ø³ØªÙ‡ Ø¯ÙˆÙ…", word, 0.65
    
    return "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", 0.3

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Specificity
def specificity_score(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred, labels=classes)
    specificity_per_class = []
    for i in range(len(classes)):
        tn = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
        fp = np.sum(cm[:, i]) - cm[i, i]
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        specificity_per_class.append(specificity)
    return np.mean(specificity_per_class)

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø¨Ø§ ParsBERT
def get_parsbert_embeddings(texts, model, tokenizer, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.to(device)
    model.eval()
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token
        embeddings.append(batch_embeddings)
    
    return np.vstack(embeddings)

# ØªØ¹Ø±ÛŒÙ Ø´Ø¨Ú©Ù‡ DQN
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# ØªØ¹Ø±ÛŒÙ Ù…Ø­ÛŒØ· ØªÙ‚ÙˆÛŒØªÛŒ
class TweetEnvironment:
    def __init__(self, embeddings, labels):
        self.embeddings = embeddings
        self.labels = labels
        self.current_idx = 0
    
    def reset(self):
        self.current_idx = 0
        return self.embeddings[0]
    
    def step(self, action):
        reward = 1 if action == self.labels[self.current_idx] else -1
        self.current_idx += 1
        done = self.current_idx >= len(self.embeddings)
        next_state = self.embeddings[self.current_idx] if not done else None
        return next_state, reward, done

# Ú©Ù„Ø§Ø³ Ø¹Ø§Ù…Ù„ DQN (Ø¨Ø§ Ø§ØµÙ„Ø§Ø­ Ø®Ø·Ø§)
class DQNAgent:
    def __init__(self, state_dim, action_dim, device='cuda' if torch.cuda.is_available() else 'cpu'):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.device = device
        self.q_network = DQN(state_dim, action_dim).to(device)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)
        self.memory = deque(maxlen=10000)
        self.gamma = 0.99  # Discount factor
        self.epsilon = 1.0  # Exploration rate
        self.epsilon_min = 0.01
        self.epsilon_decay = 0.995
        self.batch_size = 32
    
    def act(self, state):
        if random.random() < self.epsilon:
            return random.randrange(self.action_dim)
        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)
        q_values = self.q_network(state)
        return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        if len(self.memory) < self.batch_size:
            return
        batch = random.sample(self.memory, self.batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # Ù…Ø¯ÛŒØ±ÛŒØª Ø­Ø§Ù„Ø§Øª None Ø¯Ø± next_states
        non_none_indices = [i for i, ns in enumerate(next_states) if ns is not None]
        none_indices = [i for i, ns in enumerate(next_states) if ns is None]
        
        q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)
        target_q = torch.zeros(self.batch_size).to(self.device)
        
        if non_none_indices:
            non_none_states = torch.FloatTensor(np.array([next_states[i] for i in non_none_indices])).to(self.device)
            next_q_values = self.q_network(non_none_states).max(1)[0].detach()
            for i, idx in enumerate(non_none_indices):
                target_q[idx] = rewards[idx] + (1 - dones[idx]) * self.gamma * next_q_values[i]
        
        for idx in none_indices:
            target_q[idx] = rewards[idx]  # Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Ø§Øª NoneØŒ ÙÙ‚Ø· Ù¾Ø§Ø¯Ø§Ø´ ÙØ¹Ù„ÛŒ
        
        loss = nn.MSELoss()(q_values, target_q)
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§
!pip install transformers imbalanced-learn scikit-learn matplotlib seaborn torch

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ 'labeled_tweets_3000.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded_train = files.upload()

if not uploaded_train:
    raise ValueError("ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ 'labeled_tweets_3000.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ
train_file = list(uploaded_train.keys())[0]
df_train = pd.read_excel(io.BytesIO(uploaded_train[train_file]))

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
required_columns = ['Ù…ØªÙ† ØªÙˆÛŒÛŒØª', 'Ø¨Ø±Ú†Ø³Ø¨']
if not all(col in df_train.columns for col in required_columns):
    raise ValueError("Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' ÛŒØ§ 'Ø¨Ø±Ú†Ø³Ø¨' Ø¯Ø± ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
X_train_texts = df_train['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'].astype(str).tolist()
y_train = df_train['Ø¨Ø±Ú†Ø³Ø¨'].astype(int).values

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ParsBERT
model_name = "HooshvareLab/bert-fa-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø§ ParsBERT...")
X_train_embeddings = get_parsbert_embeddings(X_train_texts, model, tokenizer, batch_size=32)

# Ø§Ø¹Ù…Ø§Ù„ SMOTE Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù†Ø§Ù…ØªÙˆØ§Ø²Ù†ÛŒ
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_embeddings, y_train)

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ RandomForest (Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø§ÙˆÙ„ÛŒÙ‡)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train_balanced, y_train_balanced)

# Ø¢Ù…ÙˆØ²Ø´ Ø¹Ø§Ù…Ù„ DQN Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
env = TweetEnvironment(X_train_balanced, y_train_balanced)
agent = DQNAgent(state_dim=X_train_balanced.shape[1], action_dim=3)
episodes = 100
print("Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ø¹Ø§Ù…Ù„ DQN Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ...")
for episode in range(episodes):
    state = env.reset()
    done = False
    while not done:
        action = agent.act(state)
        next_state, reward, done = env.step(action)
        agent.remember(state, action, reward, next_state, done)
        agent.replay()
        state = next_state if not done else state
    print(f"Ø§Ù¾ÛŒØ²ÙˆØ¯ {episode + 1}/{episodes} ØªÙ…Ø§Ù… Ø´Ø¯.")

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ 'new_tweets.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded_new = files.upload()

if not uploaded_new:
    raise ValueError("ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ 'new_tweets.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
new_file = list(uploaded_new.keys())[0]
df_new = pd.read_excel(io.BytesIO(uploaded_new[new_file]))

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
if 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' not in df_new.columns or 'Ø¨Ø±Ú†Ø³Ø¨' not in df_new.columns:
    raise ValueError("Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' ÛŒØ§ 'Ø¨Ø±Ú†Ø³Ø¨' Ø¯Ø± ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ ParsBERT...")
X_new_texts = df_new['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'].astype(str).tolist()
X_new_embeddings = get_parsbert_embeddings(X_new_texts, model, tokenizer, batch_size=32)
y_true_new = df_new['Ø¨Ø±Ú†Ø³Ø¨'].astype(int).values

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ø§ DQN
y_pred_new = []
for state in X_new_embeddings:
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
    q_values = agent.q_network(state_tensor)
    action = q_values.argmax().item()
    y_pred_new.append(action)
y_pred_new = np.array(y_pred_new)

# Ø§ØµÙ„Ø§Ø­ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯Ø²Ù†ÛŒ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ
for i, (tweet, label) in enumerate(zip(X_new_texts, y_pred_new)):
    tweet_lower = tweet.lower()
    if "Ø®ÙˆØ¯Ø²Ù†ÛŒ" in tweet_lower:
        if any(word in tweet_lower for word in humorous_context) or "ðŸ˜‚" in tweet_lower or "ðŸ˜" in tweet_lower:
            y_pred_new[i] = 0  # Ø®ÙˆØ¯Ø²Ù†ÛŒ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ: Ø¨Ø±Ú†Ø³Ø¨ 0

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
accuracy = accuracy_score(y_true_new, y_pred_new)
precision_weighted = precision_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
recall_weighted = recall_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
f1_weighted = f1_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
precision_macro = precision_score(y_true_new, y_pred_new, average='macro', zero_division=0)
recall_macro = recall_score(y_true_new, y_pred_new, average='macro', zero_division=0)
f1_macro = f1_score(y_true_new, y_pred_new, average='macro', zero_division=0)
specificity_macro = specificity_score(y_true_new, y_pred_new, classes=[0, 1, 2])

# Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC
y_true_bin = label_binarize(y_true_new, classes=[0, 1, 2])
y_pred_prob = np.zeros((len(y_pred_new), 3))
for i, state in enumerate(X_new_embeddings):
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
    q_values = agent.q_network(state_tensor).detach().cpu().numpy()
    y_pred_prob[i] = np.exp(q_values) / np.sum(np.exp(q_values))  # Softmax for probabilities
auc = roc_auc_score(y_true_bin, y_pred_prob, multi_class='ovr', average='weighted')

# Ù…Ø­Ø§Ø³Ø¨Ù‡ RMSE
rmse = np.sqrt(mean_squared_error(y_true_new, y_pred_new))

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Confusion Matrix
cm = confusion_matrix(y_true_new, y_pred_new, labels=[0, 1, 2])

# Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ù†Ø­Ù†ÛŒ ROC
plt.figure(figsize=(8, 6))
for i in range(3):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])
    plt.plot(fpr, tpr, label=f'ROC Ù…Ù†Ø­Ù†ÛŒ Ú©Ù„Ø§Ø³ {i} (AUC = {roc_auc_score(y_true_bin[:, i], y_pred_prob[:, i]):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Ù†Ø±Ø® Ù…Ø«Ø¨Øª Ú©Ø§Ø°Ø¨ (False Positive Rate)')
plt.ylabel('Ù†Ø±Ø® Ù…Ø«Ø¨Øª ÙˆØ§Ù‚Ø¹ÛŒ (True Positive Rate)')
plt.title('Ù…Ù†Ø­Ù†ÛŒ ROC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.show()
files.download('roc_curve.png')

# Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ†
plt.figure(figsize=(8, 6))
for i in range(3):
    disp = CalibrationDisplay.from_predictions(y_true_bin[:, i], y_pred_prob[:, i], n_bins=10, name=f'Ú©Ù„Ø§Ø³ {i}')
    disp.plot(ax=plt.gca(), name=f'Ú©Ù„Ø§Ø³ {i}')
plt.title('Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³')
plt.savefig('calibration_plot.png')
plt.show()
files.download('calibration_plot.png')

# Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])
plt.xlabel('Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡')
plt.ylabel('Ø¨Ø±Ú†Ø³Ø¨ ÙˆØ§Ù‚Ø¹ÛŒ')
plt.title('Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ')
plt.savefig('confusion_matrix.png')
plt.show()
files.download('confusion_matrix.png')

# Ø¢Ø²Ù…ÙˆÙ† Ø¢Ù…Ø§Ø±ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ (Chi-Square)
cm_flat = cm.ravel()
if len(cm_flat) == 9:  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…Ø§ØªØ±ÛŒØ³ 3x3
    chi2, p_value, _, _ = chi2_contingency(cm)
else:
    chi2, p_value = np.nan, np.nan

# Ú†Ø§Ù¾ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
print("Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ (Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ):")
print(f"1. Ø¯Ù‚Øª (Accuracy): {accuracy:.2%}")
print(f"2. Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision, weighted): {precision_weighted:.2%}")
print(f"3. ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall, weighted): {recall_weighted:.2%}")
print(f"4. Ø§Ù…ØªÛŒØ§Ø² F1 (F1-Score, weighted): {f1_weighted:.2%}")
print(f"5. Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision, macro): {precision_macro:.2%}")
print(f"6. ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall, macro): {recall_macro:.2%}")
print(f"7. Ø§Ù…ØªÛŒØ§Ø² F1 (F1-Score, macro): {f1_macro:.2%}")
print(f"8. ÙˆÛŒÚ˜Ú¯ÛŒ (Specificity, macro): {specificity_macro:.2%}")
print(f"AUC (weighted): {auc:.2%}")
print(f"RMSE: {rmse:.4f}")
print("\nÙ…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix):")
print(cm)
print(f"\nØ¢Ø²Ù…ÙˆÙ† Chi-Square: Ù…Ù‚Ø¯Ø§Ø± chi2 = {chi2:.2f}, Ù…Ù‚Ø¯Ø§Ø± p-value = {p_value:.4f}")
print("\nÚ¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print(classification_report(y_true_new, y_pred_new, zero_division=0))

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø±ÙˆØ¬ÛŒ
df_output = df_new.copy()[['Ù…ØªÙ† ØªÙˆÛŒÛŒØª']]
df_output['Ø¨Ø±Ú†Ø³Ø¨'] = y_pred_new
df_output['Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†'] = ''
df_output['ÙˆØ§Ú˜Ù‡/Ø¹Ø¨Ø§Ø±Øª Ù…Ø´Ø§Ø¨Ù‡'] = ''
df_output['Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª'] = 0.0

# Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
for index, row in df_output.iterrows():
    category, matched_word, similarity = reconstruct_metadata(row['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'], row['Ø¨Ø±Ú†Ø³Ø¨'])
    df_output.at[index, 'Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†'] = category
    df_output.at[index, 'ÙˆØ§Ú˜Ù‡/Ø¹Ø¨Ø§Ø±Øª Ù…Ø´Ø§Ø¨Ù‡'] = matched_word
    df_output.at[index, 'Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª'] = similarity

# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø®Ø±ÙˆØ¬ÛŒ
output_excel = "new_labeled_tweets.xlsx"
df_output.to_excel(output_excel, index=False)

# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¨Ø§ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ |
output_text = "new_labeled_tweets.txt"
df_output.to_csv(output_text, sep='|', index=False, encoding='utf-8')

print(f"ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_excel}")
print(f"ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_text}")
print(f"Ø²Ù…Ø§Ù† Ú©Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {time.time() - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
files.download(output_excel)
files.download(output_text)
