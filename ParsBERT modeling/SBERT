# =================================================================
# Cell 0: Install Required Libraries and Define Global Configuration (SBERT)
# =================================================================
%%time
# 🚨 ENSURE GPU is enabled in Runtime -> Change runtime type 🚨

print("🚀 Installing required libraries...")
!pip install transformers datasets torch scikit-learn seaborn matplotlib pandas openpyxl accelerate -U --quiet
!pip install --upgrade accelerate -q

import os
import torch
import numpy as np

# --- GLOBAL MODEL CONFIGURATION (SBERT/XLM-R Base) ---
# ما از XLM-RoBERTa-base به عنوان پایه برای معماری SBERT-style استفاده می‌کنیم.
MODEL_NAME = 'xlm-roberta-base' 
REPORT_NAME = 'SBERT-like-XLM-R' # نامی که در گزارش‌ها استفاده می‌شود
NUM_LABELS = 3
KEYWORD_DIM = 2 

HYPERPARAMS = {
    'model_name': MODEL_NAME, 
    'report_name': REPORT_NAME,
    'num_labels': NUM_LABELS,
    'batch_size': 16,
    'learning_rate': 2e-5,
    'num_epochs': 5,
    'weight_decay': 0.01,
    'warmup_steps': 200,
    'max_length': 128,
    'random_state': 42,
    'keyword_dim': KEYWORD_DIM
}

# --- GLOBAL COLUMN NAMES ---
TEXT_COL = 'text'   
LABEL_COL = 'label' 

print(f"\n✅ Installation complete. Global Hyperparameters set for {REPORT_NAME}.")
# =================================================================
# Cell 1: Upload, Clean, and Data Split (60% Train, 30% Test, 10% Val)
# =================================================================
%%time
import pandas as pd
from google.colab import files
import io
from sklearn.model_selection import train_test_split
import re

# --- ۱. آپلود و لود داده ---
print("🚨 Please upload your Excel file (First column: text, Second column: label 0/1/2)")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
df = pd.read_excel(io.BytesIO(uploaded[file_name]))

# فرض: ستون اول متن و ستون دوم لیبل است
df.columns = [TEXT_COL, LABEL_COL] + list(df.columns[2:]) 

df = df[[TEXT_COL, LABEL_COL]].dropna()
df[LABEL_COL] = df[LABEL_COL].astype(int)

# --- ۲. تمیز کردن متن ---
def clean_text(text):
    text = str(text)
    # حذف URL, هشتگ و منشن (همانند کد ParsBERT)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'#\w+|@\w+', '', text)
    return text.strip()

df[TEXT_COL] = df[TEXT_COL].apply(clean_text)

# --- ۳. تقسیم داده: 60% train, 30% test, 10% val ---
train_df, temp_df = train_test_split(
    df, test_size=0.4, stratify=df[LABEL_COL], random_state=HYPERPARAMS['random_state']
)
test_df, val_df = train_test_split(
    temp_df, test_size=0.25, stratify=temp_df[LABEL_COL], random_state=HYPERPARAMS['random_state']
) 

# --- ۴. بررسی و آماده‌سازی متغیرهای گلوبال ---
global keywords 

keywords = {
    2: ["خودکشی", "آماده خودکشی", "نقشه خودکشی", "مرگ خودخواسته", "می‌خواهم خودم را بکشم",
        "تا ابد بخوابم", "هیچوقت بیدار نشم", "به زندگی‌ام پایان می‌دهم", "نمی‌تونم ادامه بدم",
        "نمی‌خوام زنده باشم", "خودسوزی", "زندگی برای من تمام شده", "خودم را خلاص کنم",
        "خودمو خلاص کنم", "خودم رو خلاص کنم", "چرا نباید خودم را بکشم؟", "#خودکشی"],
    1: ["افسردگی", "دارم عذاب می‌کشم", "هیچ امیدی ندارم", "از خودم متنفرم", "از خودم بدم میاد",
        "چیزی برای از دست دادن ندارم", "از زندگی خسته‌ام", "تنها می‌میرم", "مرگ", "درد", "ناامیدی",
        "تنهایی", "غم", "بی‌ارزش", "خستگی", "فشار رومه", "استرس", "استرس دارم", "همه چیز بی‌فایده است",
        "هیچکس مرا نمی‌فهمد", "از همه چیز متنفرم", "#افسردگی", "#تنهایی", "#درد", "#غم", "#ناامیدی"],
    0: []
}

total = len(df)
print(f"\n✅ Data loaded and split:")
print(f"    - Total: {total}")
print(f"    - Train (60%): {len(train_df)} → {len(train_df)/total:.1%}")
print(f"    - Test  (30%): {len(test_df)} → {len(test_df)/total:.1%}")
print(f"    - Val   (10%): {len(val_df)} → {len(val_df)/total:.1%}")
print("    - Label Distribution:\n", df[LABEL_COL].value_counts().sort_index())
# =================================================================
# Cell 2: Tokenizer, Custom Dataset, and Keyword Features Logic (XLM-R)
# =================================================================
%%time

from transformers import AutoTokenizer
from torch.utils.data import Dataset
import torch
from datasets import Dataset as HFDataset 

# --- ۱. توکنایزر ---
try:
    # 🚨 XLM-R از AutoTokenizer استفاده می‌کند.
    tokenizer = AutoTokenizer.from_pretrained(HYPERPARAMS['model_name'])
except Exception as e:
    print(f"❌ Error loading tokenizer: {e}")
    tokenizer = AutoTokenizer.from_pretrained(HYPERPARAMS['model_name'], local_files_only=True)


# --- ۲. تابع استخراج ویژگی کلیدواژه (بدون تغییر) ---
def get_keyword_features(text, keywords):
    """
    Extracts two features:
    1. Presence of Indirect keywords (label 1)
    2. Presence of Direct keywords (label 2)
    """
    feat = []
    
    # Feature 1: Indirect Keywords (Label 1)
    present_1 = any(kw.lower() in text.lower() for kw in keywords.get(1, []))
    feat.append(1.0 if present_1 else 0.0)
    
    # Feature 2: Direct Keywords (Label 2)
    present_2 = any(kw.lower() in text.lower() for kw in keywords.get(2, []))
    feat.append(1.0 if present_2 else 0.0)
    
    return torch.tensor(feat, dtype=torch.float)

# --- ۳. دیتاست سفارشی (بدون تغییر) ---
class SuicideKeywordDataset(Dataset):
    def __init__(self, df, tokenizer, max_length, keywords):
        self.texts = df[TEXT_COL].values
        self.labels = df[LABEL_COL].values
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.keywords = keywords

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        # استخراج ویژگی‌های کلیدواژه
        kw_feat = get_keyword_features(text, self.keywords)
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'keyword_features': kw_feat, 
            'labels': torch.tensor(label, dtype=torch.long)
        }

# --- ۴. ساخت دیتاست‌ها ---
train_dataset = SuicideKeywordDataset(train_df, tokenizer, HYPERPARAMS['max_length'], keywords)
val_dataset = SuicideKeywordDataset(val_df, tokenizer, HYPERPARAMS['max_length'], keywords)
test_dataset = SuicideKeywordDataset(test_df, tokenizer, HYPERPARAMS['max_length'], keywords)


print(f"✅ Custom Datasets ready: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}")
# =================================================================
# Cell 3: Custom Model, Training Arguments, and Fine-Tuning (SBERT-like XLM-R)
# =================================================================
%%time

# 🚨 Imports لازم
from typing import Dict, Any, Union, Optional, List, Tuple 

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import torch.nn as nn
import time


# --- ۱. لود مدل پایه ---
print(f"⚙️ Loading model from: {HYPERPARAMS['model_name']}")
try:
    model = AutoModelForSequenceClassification.from_pretrained(
        HYPERPARAMS['model_name'],
        num_labels=HYPERPARAMS['num_labels']
    )
except Exception as e:
    print(f"❌ Error loading model: {e}")
    model = AutoModelForSequenceClassification.from_pretrained(
        HYPERPARAMS['model_name'],
        num_labels=HYPERPARAMS['num_labels'],
        local_files_only=True
    )

# --- ۲. پیاده‌سازی Keyword-Aware Classifier (اصلاح لایه نهایی) ---
hidden_size = model.config.hidden_size
keyword_dim = HYPERPARAMS['keyword_dim'] 

# XLM-R از out_proj استفاده می‌کند. ما باید out_proj را اصلاح کنیم.
old_out_proj_weight = model.classifier.out_proj.weight.data.clone()
old_out_proj_bias = model.classifier.out_proj.bias.data.clone()
input_dim_new = hidden_size + keyword_dim 

new_out_proj = nn.Linear(input_dim_new, HYPERPARAMS['num_labels'])

# کپی وزن‌ها 
with torch.no_grad():
    new_out_proj.weight.data[:, :hidden_size] = old_out_proj_weight
    new_out_proj.weight.data[:, hidden_size:] = 0
    new_out_proj.bias.data[:] = old_out_proj_bias

model.classifier.out_proj = new_out_proj # جایگزینی لایه نهایی

# --- ۳. Monkey Patching متد forward (تزریق در لایه نهایی) ---
def custom_forward(
    self,
    input_ids=None,
    attention_mask=None,
    keyword_features=None,
    labels=None,
    **kwargs 
) -> Dict[str, Any]:
    
    # 🚨 فیلتر آرگومان‌های ناخواسته (مثل num_items_in_batch)
    if 'num_items_in_batch' in kwargs:
        kwargs.pop('num_items_in_batch')
    
    # اجرای مدل RoBERTa
    outputs = self.roberta(
        input_ids=input_ids,
        attention_mask=attention_mask,
        output_hidden_states=True,
        **kwargs 
    )
    
    sequence_output = outputs.last_hidden_state
    cls_embedding = sequence_output[:, 0, :]
    
    # اجرای لایه Classification Dense (768x768)
    logits = self.classifier.dense(cls_embedding) 
    logits = torch.tanh(logits)                  
    logits = self.classifier.dropout(logits)     
    
    # 🚨 تزریق کلیدواژه درست قبل از لایه out_proj
    if keyword_features is not None:
        logits = torch.cat((logits, keyword_features.to(logits.device)), dim=-1)

    # اجرای لایه نهایی اصلاح شده (770 -> 3)
    logits = self.classifier.out_proj(logits)    

    loss = None
    if labels is not None:
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))

    return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}

model.forward = custom_forward.__get__(model, model.__class__)

print(f"✅ Custom **{HYPERPARAMS['report_name']}** Model ready! Input size: {hidden_size} + {keyword_dim} → {HYPERPARAMS['num_labels']}")

# --- ۴. تابع Collate سفارشی ---
def collate_fn(batch):
    """Handles stacking and moving all necessary tensors to device."""
    return {
        'input_ids': torch.stack([x['input_ids'] for x in batch]),
        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),
        'keyword_features': torch.stack([x['keyword_features'] for x in batch]),
        'labels': torch.stack([x['labels'] for x in batch])
    }

# --- ۵. Custom Trainer برای فیلتر کردن آرگومان اضافی ---
class InputFilterTrainer(Trainer):
    def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:
        inputs.pop('num_items_in_batch', None) 
        return super().prediction_step(model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys)

# --- ۶. تنظیم آرگومان‌های آموزش و اجرای Fine-Tuning ---
output_dir_name = './results_sbert_kw' # نام پوشه تغییر کرد
training_args = TrainingArguments(
    output_dir=output_dir_name,
    num_train_epochs=HYPERPARAMS['num_epochs'],
    per_device_train_batch_size=HYPERPARAMS['batch_size'],
    per_device_eval_batch_size=HYPERPARAMS['batch_size'],
    learning_rate=HYPERPARAMS['learning_rate'],
    weight_decay=HYPERPARAMS['weight_decay'],
    warmup_steps=HYPERPARAMS['warmup_steps'],
    eval_strategy='epoch',
    save_strategy='epoch',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss', 
    greater_is_better=False,
    seed=HYPERPARAMS['random_state'],
    fp16=torch.cuda.is_available(),
)

# استفاده از Custom Trainer
trainer = InputFilterTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collate_fn,
)

# --- ۷. آموزش ---
print(f"\n🚀 Starting Fine-Tuning ({HYPERPARAMS['report_name']} Keyword-Aware)...")
start_time = time.time()
trainer.train()
TRAINING_TIME = time.time() - start_time
trainer.save_model('./best_sbert_kw_model') # نام فایل مدل تغییر کرد
print(f"✅ Training completed in {TRAINING_TIME:.2f} seconds")

global TRAINING_TIME_SEC
TRAINING_TIME_SEC = TRAINING_TIME
# =================================================================
# Cell 4: Full Evaluation, Plots, and Excel Export (SBERT-like)
# =================================================================
%%time

import numpy as np
from sklearn.metrics import *
from sklearn.preprocessing import label_binarize
import pandas as pd
from google.colab import files
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import softmax
import time 

# 🚨 فرض می‌کنیم TRAINING_TIME_SEC از سلول ۳ موجود است.
try:
    TRAINING_TIME_SEC
except NameError:
    TRAINING_TIME_SEC = 0.0 

# --- ۱. پیش‌بینی روی TEST (30%) ---
predictions = trainer.predict(test_dataset)
logits = predictions.predictions
pred_labels = np.argmax(logits, axis=1)
pred_proba = softmax(logits, axis=1)
true_labels = predictions.label_ids

CLASS_NAMES_LIST = ['Non-related (0)', 'Indirect Signs (1)', 'Direct Signs (2)']

# --- ۲. محاسبه معیارهای کلی ---
accuracy = accuracy_score(true_labels, pred_labels)
f1_macro = f1_score(true_labels, pred_labels, average='macro')
recall_macro = recall_score(true_labels, pred_labels, average='macro')
precision_macro = precision_score(true_labels, pred_labels, average='macro')
kappa = cohen_kappa_score(true_labels, pred_labels)
mcc = matthews_corrcoef(true_labels, pred_labels)
y_bin = label_binarize(true_labels, classes=[0,1,2])
auc_ovr = roc_auc_score(y_bin, pred_proba, multi_class='ovr', average='macro')
auc_ovo = roc_auc_score(y_bin, pred_proba, multi_class='ovo', average='macro')
logloss = log_loss(true_labels, pred_proba)
mae = mean_absolute_error(true_labels, pred_labels)

# --- ۳. معیارهای هر کلاس ---
report_dict = classification_report(true_labels, pred_labels, target_names=CLASS_NAMES_LIST, output_dict=True)
precision_per = [report_dict[c]['precision'] for c in CLASS_NAMES_LIST]
recall_per = [report_dict[c]['recall'] for c in CLASS_NAMES_LIST]
f1_per = [report_dict[c]['f1-score'] for c in CLASS_NAMES_LIST]
support_per = [report_dict[c]['support'] for c in CLASS_NAMES_LIST]


# ==================================
# بخش تولید اکسل (سه شیت)
# ==================================

# --- شیت ۱: Overall Metrics ---
overall_df = pd.DataFrame({
    'Metric': ['Accuracy', 'F1 Macro', 'Recall Macro', 'Precision Macro', 'Kappa', 'MCC',
               'AUC (ROC-OVR)', 'AUC (ROC-OVO)', 'Log Loss', 'MAE', 'Training Time (s)'],
    'Value': [accuracy, f1_macro, recall_macro, precision_macro, kappa, mcc,
              auc_ovr, auc_ovo, logloss, mae, TRAINING_TIME_SEC]
})
overall_df['Value'] = overall_df['Value'].apply(lambda x: f"{x:.4f}" if isinstance(x, (float, np.float32, np.float64)) else x)


# --- شیت ۲: Per-Class Metrics ---
per_class_df = pd.DataFrame({
    'Class': CLASS_NAMES_LIST,
    'Precision': precision_per,
    'Recall': recall_per,
    'F1-Score': f1_per,
    'Support': support_per
})


# --- شیت ۳: Important Phrases (Keyword Weights) ---
print("\n⚙️ Calculating Keyword Importance Weights (Sheet 3)...")

def get_keyword_weights_proxy(df, keywords_dict, logits, true_labels, keyword_dim):
    """
    Approximates keyword importance by looking at the input keyword feature values
    and the resulting logits on the test set.
    """
    kw_list = []
    
    for index, row in df.iterrows():
        label = row[LABEL_COL]
        text = row[TEXT_COL]
        
        kw_feat = get_keyword_features(text, keywords_dict).numpy()
        
        if np.any(kw_feat == 1.0):
            weight_proxy = logits[index, pred_labels[index]]
            
            for i, kw_dim_idx in enumerate([1, 2]):
                if kw_feat[i] == 1.0:
                    
                    for kw in keywords_dict[kw_dim_idx]:
                        if kw.lower() in text.lower():
                            kw_list.append({
                                'Keyword': kw, 
                                'Label': kw_dim_idx, 
                                'Weight (Logit Proxy)': weight_proxy
                            })
                            
    if not kw_list:
        return pd.DataFrame({'Important Phrases/Tokens (Model Interpretation)': ['N/A'], 'Importance Weight': [0]})

    kw_df_raw = pd.DataFrame(kw_list)
    kw_df_grouped = kw_df_raw.groupby(['Keyword', 'Label'])['Weight (Logit Proxy)'].mean().reset_index()
    kw_df_grouped.rename(
        columns={'Keyword': 'Important Phrases/Tokens (Model Interpretation)', 'Weight (Logit Proxy)': 'Importance Weight'}, 
        inplace=True
    )
    return kw_df_grouped.sort_values(by='Importance Weight', ascending=False).head(50)

kw_df = get_keyword_weights_proxy(test_df.reset_index(drop=True), keywords, logits, true_labels, KEYWORD_DIM)


# --- ۴. ذخیره در فایل اکسل ---
output_file = 'SBERT_Keyword_Prediction_Report.xlsx' # 🚨 نام فایل تغییر کرد
with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
    overall_df.to_excel(writer, sheet_name='Summary Metrics', index=False)
    per_class_df.to_excel(writer, sheet_name='Per-Class Metrics', index=False)
    kw_df.to_excel(writer, sheet_name='Important Phrases', index=False)

print(f"\n✅ Excel file saved: '{output_file}' (3 sheets: Summary, Per-Class, Important Phrases).")
files.download(output_file)


# ==================================
# بخش نمودارهای گرافیکی
# ==================================
plt.rcParams.update({'font.size': 12, 'font.family': 'sans-serif'}) 

# --- Confusion Matrix ---
plt.figure(figsize=(8,6))
cm = confusion_matrix(true_labels, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=CLASS_NAMES_LIST,
            yticklabels=CLASS_NAMES_LIST)
plt.title('Confusion Matrix: Actual vs. Predicted Labels', fontsize=14)
plt.ylabel('Actual Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()


# --- ROC Curve (One-vs-Rest) ---
plt.figure(figsize=(10, 8))
for i in range(NUM_LABELS):
    fpr, tpr, _ = roc_curve(y_bin[:, i], pred_proba[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC curve of class {CLASS_NAMES_LIST[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - OVR', fontsize=14)
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()


# --- Bar Chart: Metrics Comparison ---
metrics_plot_df = pd.DataFrame({
    'Class': CLASS_NAMES_LIST,
    'Precision': precision_per,
    'Recall': recall_per,
    'F1-Score': f1_per
})
metrics_plot_df.set_index('Class').plot(kind='bar', figsize=(10,6), color=['#1f77b4', '#ff7f0e', '#2ca02c'])
plt.title('Metrics Comparison Across Classes', fontsize=14)
plt.ylabel('Score Value')
plt.xlabel('Class Label')
plt.xticks(rotation=15)
plt.legend(title='Metric')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()


print("\n✅ All required plots are generated and saved.")
