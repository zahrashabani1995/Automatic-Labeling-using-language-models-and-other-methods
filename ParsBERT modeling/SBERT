# =================================================================
# Cell 0: Install Required Libraries and Define Global Configuration (SBERT)
# =================================================================
%%time
# ğŸš¨ ENSURE GPU is enabled in Runtime -> Change runtime type ğŸš¨

print("ğŸš€ Installing required libraries...")
!pip install transformers datasets torch scikit-learn seaborn matplotlib pandas openpyxl accelerate -U --quiet
!pip install --upgrade accelerate -q

import os
import torch
import numpy as np

# --- GLOBAL MODEL CONFIGURATION (SBERT/XLM-R Base) ---
# Ù…Ø§ Ø§Ø² XLM-RoBERTa-base Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ù…Ø¹Ù…Ø§Ø±ÛŒ SBERT-style Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
MODEL_NAME = 'xlm-roberta-base' 
REPORT_NAME = 'SBERT-like-XLM-R' # Ù†Ø§Ù…ÛŒ Ú©Ù‡ Ø¯Ø± Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
NUM_LABELS = 3
KEYWORD_DIM = 2 

HYPERPARAMS = {
    'model_name': MODEL_NAME, 
    'report_name': REPORT_NAME,
    'num_labels': NUM_LABELS,
    'batch_size': 16,
    'learning_rate': 2e-5,
    'num_epochs': 5,
    'weight_decay': 0.01,
    'warmup_steps': 200,
    'max_length': 128,
    'random_state': 42,
    'keyword_dim': KEYWORD_DIM
}

# --- GLOBAL COLUMN NAMES ---
TEXT_COL = 'text'   
LABEL_COL = 'label' 

print(f"\nâœ… Installation complete. Global Hyperparameters set for {REPORT_NAME}.")
# =================================================================
# Cell 1: Upload, Clean, and Data Split (60% Train, 30% Test, 10% Val)
# =================================================================
%%time
import pandas as pd
from google.colab import files
import io
from sklearn.model_selection import train_test_split
import re

# --- Û±. Ø¢Ù¾Ù„ÙˆØ¯ Ùˆ Ù„ÙˆØ¯ Ø¯Ø§Ø¯Ù‡ ---
print("ğŸš¨ Please upload your Excel file (First column: text, Second column: label 0/1/2)")
uploaded = files.upload()
file_name = list(uploaded.keys())[0]
df = pd.read_excel(io.BytesIO(uploaded[file_name]))

# ÙØ±Ø¶: Ø³ØªÙˆÙ† Ø§ÙˆÙ„ Ù…ØªÙ† Ùˆ Ø³ØªÙˆÙ† Ø¯ÙˆÙ… Ù„ÛŒØ¨Ù„ Ø§Ø³Øª
df.columns = [TEXT_COL, LABEL_COL] + list(df.columns[2:]) 

df = df[[TEXT_COL, LABEL_COL]].dropna()
df[LABEL_COL] = df[LABEL_COL].astype(int)

# --- Û². ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù† Ù…ØªÙ† ---
def clean_text(text):
    text = str(text)
    # Ø­Ø°Ù URL, Ù‡Ø´ØªÚ¯ Ùˆ Ù…Ù†Ø´Ù† (Ù‡Ù…Ø§Ù†Ù†Ø¯ Ú©Ø¯ ParsBERT)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'#\w+|@\w+', '', text)
    return text.strip()

df[TEXT_COL] = df[TEXT_COL].apply(clean_text)

# --- Û³. ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡: 60% train, 30% test, 10% val ---
train_df, temp_df = train_test_split(
    df, test_size=0.4, stratify=df[LABEL_COL], random_state=HYPERPARAMS['random_state']
)
test_df, val_df = train_test_split(
    temp_df, test_size=0.25, stratify=temp_df[LABEL_COL], random_state=HYPERPARAMS['random_state']
) 

# --- Û´. Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ ---
global keywords 

keywords = {
    2: ["Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ø¢Ù…Ø§Ø¯Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù†Ù‚Ø´Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯ Ø®ÙˆØ¯Ø®ÙˆØ§Ø³ØªÙ‡", "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…",
        "ØªØ§ Ø§Ø¨Ø¯ Ø¨Ø®ÙˆØ§Ø¨Ù…", "Ù‡ÛŒÚ†ÙˆÙ‚Øª Ø¨ÛŒØ¯Ø§Ø± Ù†Ø´Ù…", "Ø¨Ù‡ Ø²Ù†Ø¯Ú¯ÛŒâ€ŒØ§Ù… Ù¾Ø§ÛŒØ§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù…", "Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù…",
        "Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø²Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ù…", "Ø®ÙˆØ¯Ø³ÙˆØ²ÛŒ", "Ø²Ù†Ø¯Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù† ØªÙ…Ø§Ù… Ø´Ø¯Ù‡", "Ø®ÙˆØ¯Ù… Ø±Ø§ Ø®Ù„Ø§Øµ Ú©Ù†Ù…",
        "Ø®ÙˆØ¯Ù…Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ø®ÙˆØ¯Ù… Ø±Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ú†Ø±Ø§ Ù†Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…ØŸ", "#Ø®ÙˆØ¯Ú©Ø´ÛŒ"],
    1: ["Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø¯Ø§Ø±Ù… Ø¹Ø°Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ø´Ù…", "Ù‡ÛŒÚ† Ø§Ù…ÛŒØ¯ÛŒ Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ù…ØªÙ†ÙØ±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ø¨Ø¯Ù… Ù…ÛŒØ§Ø¯",
        "Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ø®Ø³ØªÙ‡â€ŒØ§Ù…", "ØªÙ†Ù‡Ø§ Ù…ÛŒâ€ŒÙ…ÛŒØ±Ù…", "Ù…Ø±Ú¯", "Ø¯Ø±Ø¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ",
        "ØªÙ†Ù‡Ø§ÛŒÛŒ", "ØºÙ…", "Ø¨ÛŒâ€ŒØ§Ø±Ø²Ø´", "Ø®Ø³ØªÚ¯ÛŒ", "ÙØ´Ø§Ø± Ø±ÙˆÙ…Ù‡", "Ø§Ø³ØªØ±Ø³", "Ø§Ø³ØªØ±Ø³ Ø¯Ø§Ø±Ù…", "Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¨ÛŒâ€ŒÙØ§ÛŒØ¯Ù‡ Ø§Ø³Øª",
        "Ù‡ÛŒÚ†Ú©Ø³ Ù…Ø±Ø§ Ù†Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯", "Ø§Ø² Ù‡Ù…Ù‡ Ú†ÛŒØ² Ù…ØªÙ†ÙØ±Ù…", "#Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "#ØªÙ†Ù‡Ø§ÛŒÛŒ", "#Ø¯Ø±Ø¯", "#ØºÙ…", "#Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"],
    0: []
}

total = len(df)
print(f"\nâœ… Data loaded and split:")
print(f"    - Total: {total}")
print(f"    - Train (60%): {len(train_df)} â†’ {len(train_df)/total:.1%}")
print(f"    - Test  (30%): {len(test_df)} â†’ {len(test_df)/total:.1%}")
print(f"    - Val   (10%): {len(val_df)} â†’ {len(val_df)/total:.1%}")
print("    - Label Distribution:\n", df[LABEL_COL].value_counts().sort_index())
# =================================================================
# Cell 2: Tokenizer, Custom Dataset, and Keyword Features Logic (XLM-R)
# =================================================================
%%time

from transformers import AutoTokenizer
from torch.utils.data import Dataset
import torch
from datasets import Dataset as HFDataset 

# --- Û±. ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ---
try:
    # ğŸš¨ XLM-R Ø§Ø² AutoTokenizer Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    tokenizer = AutoTokenizer.from_pretrained(HYPERPARAMS['model_name'])
except Exception as e:
    print(f"âŒ Error loading tokenizer: {e}")
    tokenizer = AutoTokenizer.from_pretrained(HYPERPARAMS['model_name'], local_files_only=True)


# --- Û². ØªØ§Ø¨Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ---
def get_keyword_features(text, keywords):
    """
    Extracts two features:
    1. Presence of Indirect keywords (label 1)
    2. Presence of Direct keywords (label 2)
    """
    feat = []
    
    # Feature 1: Indirect Keywords (Label 1)
    present_1 = any(kw.lower() in text.lower() for kw in keywords.get(1, []))
    feat.append(1.0 if present_1 else 0.0)
    
    # Feature 2: Direct Keywords (Label 2)
    present_2 = any(kw.lower() in text.lower() for kw in keywords.get(2, []))
    feat.append(1.0 if present_2 else 0.0)
    
    return torch.tensor(feat, dtype=torch.float)

# --- Û³. Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) ---
class SuicideKeywordDataset(Dataset):
    def __init__(self, df, tokenizer, max_length, keywords):
        self.texts = df[TEXT_COL].values
        self.labels = df[LABEL_COL].values
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.keywords = keywords

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]
        
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡
        kw_feat = get_keyword_features(text, self.keywords)
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'keyword_features': kw_feat, 
            'labels': torch.tensor(label, dtype=torch.long)
        }

# --- Û´. Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§Ø³Øªâ€ŒÙ‡Ø§ ---
train_dataset = SuicideKeywordDataset(train_df, tokenizer, HYPERPARAMS['max_length'], keywords)
val_dataset = SuicideKeywordDataset(val_df, tokenizer, HYPERPARAMS['max_length'], keywords)
test_dataset = SuicideKeywordDataset(test_df, tokenizer, HYPERPARAMS['max_length'], keywords)


print(f"âœ… Custom Datasets ready: Train={len(train_dataset)}, Val={len(val_dataset)}, Test={len(test_dataset)}")
# =================================================================
# Cell 3: Custom Model, Training Arguments, and Fine-Tuning (SBERT-like XLM-R)
# =================================================================
%%time

# ğŸš¨ Imports Ù„Ø§Ø²Ù…
from typing import Dict, Any, Union, Optional, List, Tuple 

from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments
import torch
import torch.nn as nn
import time


# --- Û±. Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ù¾Ø§ÛŒÙ‡ ---
print(f"âš™ï¸ Loading model from: {HYPERPARAMS['model_name']}")
try:
    model = AutoModelForSequenceClassification.from_pretrained(
        HYPERPARAMS['model_name'],
        num_labels=HYPERPARAMS['num_labels']
    )
except Exception as e:
    print(f"âŒ Error loading model: {e}")
    model = AutoModelForSequenceClassification.from_pretrained(
        HYPERPARAMS['model_name'],
        num_labels=HYPERPARAMS['num_labels'],
        local_files_only=True
    )

# --- Û². Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Keyword-Aware Classifier (Ø§ØµÙ„Ø§Ø­ Ù„Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ) ---
hidden_size = model.config.hidden_size
keyword_dim = HYPERPARAMS['keyword_dim'] 

# XLM-R Ø§Ø² out_proj Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù…Ø§ Ø¨Ø§ÛŒØ¯ out_proj Ø±Ø§ Ø§ØµÙ„Ø§Ø­ Ú©Ù†ÛŒÙ….
old_out_proj_weight = model.classifier.out_proj.weight.data.clone()
old_out_proj_bias = model.classifier.out_proj.bias.data.clone()
input_dim_new = hidden_size + keyword_dim 

new_out_proj = nn.Linear(input_dim_new, HYPERPARAMS['num_labels'])

# Ú©Ù¾ÛŒ ÙˆØ²Ù†â€ŒÙ‡Ø§ 
with torch.no_grad():
    new_out_proj.weight.data[:, :hidden_size] = old_out_proj_weight
    new_out_proj.weight.data[:, hidden_size:] = 0
    new_out_proj.bias.data[:] = old_out_proj_bias

model.classifier.out_proj = new_out_proj # Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù„Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ

# --- Û³. Monkey Patching Ù…ØªØ¯ forward (ØªØ²Ø±ÛŒÙ‚ Ø¯Ø± Ù„Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ) ---
def custom_forward(
    self,
    input_ids=None,
    attention_mask=None,
    keyword_features=None,
    labels=None,
    **kwargs 
) -> Dict[str, Any]:
    
    # ğŸš¨ ÙÛŒÙ„ØªØ± Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡ (Ù…Ø«Ù„ num_items_in_batch)
    if 'num_items_in_batch' in kwargs:
        kwargs.pop('num_items_in_batch')
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¯Ù„ RoBERTa
    outputs = self.roberta(
        input_ids=input_ids,
        attention_mask=attention_mask,
        output_hidden_states=True,
        **kwargs 
    )
    
    sequence_output = outputs.last_hidden_state
    cls_embedding = sequence_output[:, 0, :]
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Classification Dense (768x768)
    logits = self.classifier.dense(cls_embedding) 
    logits = torch.tanh(logits)                  
    logits = self.classifier.dropout(logits)     
    
    # ğŸš¨ ØªØ²Ø±ÛŒÙ‚ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø¯Ø±Ø³Øª Ù‚Ø¨Ù„ Ø§Ø² Ù„Ø§ÛŒÙ‡ out_proj
    if keyword_features is not None:
        logits = torch.cat((logits, keyword_features.to(logits.device)), dim=-1)

    # Ø§Ø¬Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Ù†Ù‡Ø§ÛŒÛŒ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ (770 -> 3)
    logits = self.classifier.out_proj(logits)    

    loss = None
    if labels is not None:
        loss_fct = nn.CrossEntropyLoss()
        loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))

    return {'loss': loss, 'logits': logits} if loss is not None else {'logits': logits}

model.forward = custom_forward.__get__(model, model.__class__)

print(f"âœ… Custom **{HYPERPARAMS['report_name']}** Model ready! Input size: {hidden_size} + {keyword_dim} â†’ {HYPERPARAMS['num_labels']}")

# --- Û´. ØªØ§Ø¨Ø¹ Collate Ø³ÙØ§Ø±Ø´ÛŒ ---
def collate_fn(batch):
    """Handles stacking and moving all necessary tensors to device."""
    return {
        'input_ids': torch.stack([x['input_ids'] for x in batch]),
        'attention_mask': torch.stack([x['attention_mask'] for x in batch]),
        'keyword_features': torch.stack([x['keyword_features'] for x in batch]),
        'labels': torch.stack([x['labels'] for x in batch])
    }

# --- Ûµ. Custom Trainer Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù† Ø§Ø¶Ø§ÙÛŒ ---
class InputFilterTrainer(Trainer):
    def prediction_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:
        inputs.pop('num_items_in_batch', None) 
        return super().prediction_step(model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys)

# --- Û¶. ØªÙ†Ø¸ÛŒÙ… Ø¢Ø±Ú¯ÙˆÙ…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ùˆ Ø§Ø¬Ø±Ø§ÛŒ Fine-Tuning ---
output_dir_name = './results_sbert_kw' # Ù†Ø§Ù… Ù¾ÙˆØ´Ù‡ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯
training_args = TrainingArguments(
    output_dir=output_dir_name,
    num_train_epochs=HYPERPARAMS['num_epochs'],
    per_device_train_batch_size=HYPERPARAMS['batch_size'],
    per_device_eval_batch_size=HYPERPARAMS['batch_size'],
    learning_rate=HYPERPARAMS['learning_rate'],
    weight_decay=HYPERPARAMS['weight_decay'],
    warmup_steps=HYPERPARAMS['warmup_steps'],
    eval_strategy='epoch',
    save_strategy='epoch',
    logging_steps=10,
    load_best_model_at_end=True,
    metric_for_best_model='eval_loss', 
    greater_is_better=False,
    seed=HYPERPARAMS['random_state'],
    fp16=torch.cuda.is_available(),
)

# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Custom Trainer
trainer = InputFilterTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    data_collator=collate_fn,
)

# --- Û·. Ø¢Ù…ÙˆØ²Ø´ ---
print(f"\nğŸš€ Starting Fine-Tuning ({HYPERPARAMS['report_name']} Keyword-Aware)...")
start_time = time.time()
trainer.train()
TRAINING_TIME = time.time() - start_time
trainer.save_model('./best_sbert_kw_model') # Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ù…Ø¯Ù„ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯
print(f"âœ… Training completed in {TRAINING_TIME:.2f} seconds")

global TRAINING_TIME_SEC
TRAINING_TIME_SEC = TRAINING_TIME
# =================================================================
# Cell 4: Full Evaluation, Plots, and Excel Export (SBERT-like)
# =================================================================
%%time

import numpy as np
from sklearn.metrics import *
from sklearn.preprocessing import label_binarize
import pandas as pd
from google.colab import files
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.special import softmax
import time 

# ğŸš¨ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… TRAINING_TIME_SEC Ø§Ø² Ø³Ù„ÙˆÙ„ Û³ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³Øª.
try:
    TRAINING_TIME_SEC
except NameError:
    TRAINING_TIME_SEC = 0.0 

# --- Û±. Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ TEST (30%) ---
predictions = trainer.predict(test_dataset)
logits = predictions.predictions
pred_labels = np.argmax(logits, axis=1)
pred_proba = softmax(logits, axis=1)
true_labels = predictions.label_ids

CLASS_NAMES_LIST = ['Non-related (0)', 'Indirect Signs (1)', 'Direct Signs (2)']

# --- Û². Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ ---
accuracy = accuracy_score(true_labels, pred_labels)
f1_macro = f1_score(true_labels, pred_labels, average='macro')
recall_macro = recall_score(true_labels, pred_labels, average='macro')
precision_macro = precision_score(true_labels, pred_labels, average='macro')
kappa = cohen_kappa_score(true_labels, pred_labels)
mcc = matthews_corrcoef(true_labels, pred_labels)
y_bin = label_binarize(true_labels, classes=[0,1,2])
auc_ovr = roc_auc_score(y_bin, pred_proba, multi_class='ovr', average='macro')
auc_ovo = roc_auc_score(y_bin, pred_proba, multi_class='ovo', average='macro')
logloss = log_loss(true_labels, pred_proba)
mae = mean_absolute_error(true_labels, pred_labels)

# --- Û³. Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ ---
report_dict = classification_report(true_labels, pred_labels, target_names=CLASS_NAMES_LIST, output_dict=True)
precision_per = [report_dict[c]['precision'] for c in CLASS_NAMES_LIST]
recall_per = [report_dict[c]['recall'] for c in CLASS_NAMES_LIST]
f1_per = [report_dict[c]['f1-score'] for c in CLASS_NAMES_LIST]
support_per = [report_dict[c]['support'] for c in CLASS_NAMES_LIST]


# ==================================
# Ø¨Ø®Ø´ ØªÙˆÙ„ÛŒØ¯ Ø§Ú©Ø³Ù„ (Ø³Ù‡ Ø´ÛŒØª)
# ==================================

# --- Ø´ÛŒØª Û±: Overall Metrics ---
overall_df = pd.DataFrame({
    'Metric': ['Accuracy', 'F1 Macro', 'Recall Macro', 'Precision Macro', 'Kappa', 'MCC',
               'AUC (ROC-OVR)', 'AUC (ROC-OVO)', 'Log Loss', 'MAE', 'Training Time (s)'],
    'Value': [accuracy, f1_macro, recall_macro, precision_macro, kappa, mcc,
              auc_ovr, auc_ovo, logloss, mae, TRAINING_TIME_SEC]
})
overall_df['Value'] = overall_df['Value'].apply(lambda x: f"{x:.4f}" if isinstance(x, (float, np.float32, np.float64)) else x)


# --- Ø´ÛŒØª Û²: Per-Class Metrics ---
per_class_df = pd.DataFrame({
    'Class': CLASS_NAMES_LIST,
    'Precision': precision_per,
    'Recall': recall_per,
    'F1-Score': f1_per,
    'Support': support_per
})


# --- Ø´ÛŒØª Û³: Important Phrases (Keyword Weights) ---
print("\nâš™ï¸ Calculating Keyword Importance Weights (Sheet 3)...")

def get_keyword_weights_proxy(df, keywords_dict, logits, true_labels, keyword_dim):
    """
    Approximates keyword importance by looking at the input keyword feature values
    and the resulting logits on the test set.
    """
    kw_list = []
    
    for index, row in df.iterrows():
        label = row[LABEL_COL]
        text = row[TEXT_COL]
        
        kw_feat = get_keyword_features(text, keywords_dict).numpy()
        
        if np.any(kw_feat == 1.0):
            weight_proxy = logits[index, pred_labels[index]]
            
            for i, kw_dim_idx in enumerate([1, 2]):
                if kw_feat[i] == 1.0:
                    
                    for kw in keywords_dict[kw_dim_idx]:
                        if kw.lower() in text.lower():
                            kw_list.append({
                                'Keyword': kw, 
                                'Label': kw_dim_idx, 
                                'Weight (Logit Proxy)': weight_proxy
                            })
                            
    if not kw_list:
        return pd.DataFrame({'Important Phrases/Tokens (Model Interpretation)': ['N/A'], 'Importance Weight': [0]})

    kw_df_raw = pd.DataFrame(kw_list)
    kw_df_grouped = kw_df_raw.groupby(['Keyword', 'Label'])['Weight (Logit Proxy)'].mean().reset_index()
    kw_df_grouped.rename(
        columns={'Keyword': 'Important Phrases/Tokens (Model Interpretation)', 'Weight (Logit Proxy)': 'Importance Weight'}, 
        inplace=True
    )
    return kw_df_grouped.sort_values(by='Importance Weight', ascending=False).head(50)

kw_df = get_keyword_weights_proxy(test_df.reset_index(drop=True), keywords, logits, true_labels, KEYWORD_DIM)


# --- Û´. Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ ---
output_file = 'SBERT_Keyword_Prediction_Report.xlsx' # ğŸš¨ Ù†Ø§Ù… ÙØ§ÛŒÙ„ ØªØºÛŒÛŒØ± Ú©Ø±Ø¯
with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
    overall_df.to_excel(writer, sheet_name='Summary Metrics', index=False)
    per_class_df.to_excel(writer, sheet_name='Per-Class Metrics', index=False)
    kw_df.to_excel(writer, sheet_name='Important Phrases', index=False)

print(f"\nâœ… Excel file saved: '{output_file}' (3 sheets: Summary, Per-Class, Important Phrases).")
files.download(output_file)


# ==================================
# Ø¨Ø®Ø´ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ
# ==================================
plt.rcParams.update({'font.size': 12, 'font.family': 'sans-serif'}) 

# --- Confusion Matrix ---
plt.figure(figsize=(8,6))
cm = confusion_matrix(true_labels, pred_labels)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=CLASS_NAMES_LIST,
            yticklabels=CLASS_NAMES_LIST)
plt.title('Confusion Matrix: Actual vs. Predicted Labels', fontsize=14)
plt.ylabel('Actual Label', fontsize=12)
plt.xlabel('Predicted Label', fontsize=12)
plt.show()


# --- ROC Curve (One-vs-Rest) ---
plt.figure(figsize=(10, 8))
for i in range(NUM_LABELS):
    fpr, tpr, _ = roc_curve(y_bin[:, i], pred_proba[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, label=f'ROC curve of class {CLASS_NAMES_LIST[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], 'k--', label='Chance (AUC = 0.50)')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve - OVR', fontsize=14)
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()


# --- Bar Chart: Metrics Comparison ---
metrics_plot_df = pd.DataFrame({
    'Class': CLASS_NAMES_LIST,
    'Precision': precision_per,
    'Recall': recall_per,
    'F1-Score': f1_per
})
metrics_plot_df.set_index('Class').plot(kind='bar', figsize=(10,6), color=['#1f77b4', '#ff7f0e', '#2ca02c'])
plt.title('Metrics Comparison Across Classes', fontsize=14)
plt.ylabel('Score Value')
plt.xlabel('Class Label')
plt.xticks(rotation=15)
plt.legend(title='Metric')
plt.grid(axis='y', alpha=0.3)
plt.tight_layout()
plt.show()


print("\nâœ… All required plots are generated and saved.")
