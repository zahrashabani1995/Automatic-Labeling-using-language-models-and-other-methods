print("متغیرهای موجود:")
print("X_train_balanced:", 'X_train_balanced' in globals())
print("y_train_balanced:", 'y_train_balanced' in globals())
print("classifier:", 'classifier' in globals())
print("agent:", 'agent' in globals())
import pandas as pd
import re
from google.colab import files
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error
from sklearn.preprocessing import label_binarize
from sklearn.calibration import CalibrationDisplay
from scipy.stats import chi2_contingency
import torch
import time
from sentence_transformers import SentenceTransformer

# زمان‌سنجی برای پردازش
start_time = time.time()

# تعریف واژگان برای بازسازی ستون‌های اضافی
category_1 = [
    "خودکشی", "آماده خودکشی", "نقشه خودکشی", "مرگ خودخواسته", "می‌خواهم خودم را بکشم",
    "تا ابد بخوابم", "هیچوقت بیدار نشم", "به زندگی‌ام پایان می‌دهم", "نمی‌تونم ادامه بدم",
    "نمی‌خوام زنده باشم", "خودسوزی", "زندگی برای من تمام شده", "خودم را خلاص کنم",
    "خودمو خلاص کنم", "خودم رو خلاص کنم", "چرا نباید خودم را بکشم؟", "#خودکشی"
]

category_2 = [
    "افسردگی", "دارم عذاب می‌کشم", "هیچ امیدی ندارم", "از خودم متنفرم", "از خودم بدم میاد",
    "چیزی برای از دست دادن ندارم", "از زندگی خسته‌ام", "تنها می‌میرم", "مرگ", "درد", "ناامیدی",
    "تنهایی", "غم", "بی‌ارزش", "خستگی", "فشار رومه", "استرس", "استرس دارم", "همه چیز بی‌فایده است",
    "هیچکس مرا نمی‌فهمد", "از همه چیز متنفرم", "#افسردگی", "#تنهایی", "#درد", "#غم", "#ناامیدی"
]

serious_context = [
    "قرص", "خون", "آسیب", "گریه", "جیغ", "درد", "افسردگی", "خودکشی", "مرگ", "ناامیدی"
]
humorous_context = [
    "😂", "😁", "🤣", "سوتی", "کصخل", "خنده", "شوخی", "فان", "ههه", "هاها"
]

# تابع برای بازسازی ستون‌های اضافی
def reconstruct_metadata(tweet, label):
    tweet = str(tweet).lower()
    
    if "خودزنی" in tweet:
        if any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in serious_context):
            return "دسته اول", "خودزنی", 0.9
        elif any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in humorous_context) or "😂" in tweet or "😁" in tweet:
            return "هیچ‌کدام", "خودزنی", 0.3
        else:
            return "هیچ‌کدام", "خودزنی", 0.3
    
    for word in category_1:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "دسته اول", word, 0.9
    
    for word in category_2:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "دسته دوم", word, 0.65
    
    return "هیچ‌کدام", "هیچ‌کدام", 0.3

# تابع برای محاسبه Specificity
def specificity_score(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred, labels=classes)
    specificity_per_class = []
    for i in range(len(classes)):
        tn = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
        fp = np.sum(cm[:, i]) - cm[i, i]
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        specificity_per_class.append(specificity)
    return np.mean(specificity_per_class)

# تابع برای تولید امبدینگ با SBERT
def get_sbert_embeddings(texts, model, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    embeddings = model.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True, device=device)
    return embeddings

# نصب کتابخانه sentence-transformers
!pip install sentence-transformers

# بارگذاری مدل SBERT
model_name = "Sahajtomar/sbert-fa"
sbert_model = SentenceTransformer(model_name)

# آپلود فایل جدید
print("لطفاً فایل جدید 'new_tweets.xlsx' را آپلود کنید:")
uploaded_new = files.upload()

if not uploaded_new:
    raise ValueError("فایل جدید آپلود نشد. لطفاً فایل 'new_tweets.xlsx' را آپلود کنید.")

# خواندن فایل جدید
new_file = list(uploaded_new.keys())[0]
df_new = pd.read_excel(io.BytesIO(uploaded_new[new_file]))

# اطمینان از وجود ستون‌های مورد نیاز
if 'متن توییت' not in df_new.columns or 'برچسب' not in df_new.columns:
    raise ValueError("ستون‌های 'متن توییت' یا 'برچسب' در فایل جدید یافت نشد.")

# تبدیل متن‌های جدید به امبدینگ
print("در حال تولید امبدینگ‌های داده‌های جدید با SBERT...")
X_new_texts = df_new['متن توییت'].astype(str).tolist()
X_new_embeddings = get_sbert_embeddings(X_new_texts, sbert_model, batch_size=32)
y_true_new = df_new['برچسب'].astype(int).values

# بررسی NaN در امبدینگ‌ها
if np.any(np.isnan(X_new_embeddings)):
    print("هشدار: امبدینگ‌ها شامل NaN هستند. جایگزینی با صفر...")
    X_new_embeddings = np.nan_to_num(X_new_embeddings, nan=0.0)

# پیش‌بینی لیبل‌ها با DQN
y_pred_new = []
y_pred_prob = np.zeros((len(X_new_texts), 3))
for i, state in enumerate(X_new_embeddings):
    state_tensor = torch.FloatTensor(state).unsqueeze(0).to(agent.device)
    q_values = agent.q_network(state_tensor).detach().cpu().numpy()
    
    # بررسی و اصلاح NaN یا بی‌نهایت در q_values
    if np.any(np.isnan(q_values)) or np.any(np.isinf(q_values)):
        print(f"هشدار: q_values در ایندکس {i} شامل NaN یا بی‌نهایت است. جایگزینی با صفر...")
        q_values = np.nan_to_num(q_values, nan=0.0, posinf=0.0, neginf=0.0)
    
    # محاسبه softmax
    exp_q = np.exp(q_values - np.max(q_values))  # جلوگیری از سرریز
    prob = exp_q / np.sum(exp_q)
    y_pred_prob[i] = prob
    action = np.argmax(prob)
    y_pred_new.append(action)
y_pred_new = np.array(y_pred_new)

# اصلاح برچسب‌ها برای خودزنی استعاری
for i, (tweet, label) in enumerate(zip(X_new_texts, y_pred_new)):
    tweet_lower = tweet.lower()
    if "خودزنی" in tweet_lower:
        if any(word in tweet_lower for word in humorous_context) or "😂" in tweet_lower or "😁" in tweet_lower:
            y_pred_new[i] = 0  # خودزنی استعاری: برچسب 0

# بررسی NaN در y_pred_prob
if np.any(np.isnan(y_pred_prob)):
    print("هشدار: y_pred_prob شامل NaN است. جایگزینی با مقادیر پیش‌فرض...")
    y_pred_prob = np.nan_to_num(y_pred_prob, nan=0.0)

# محاسبه معیارهای ارزیابی
accuracy = accuracy_score(y_true_new, y_pred_new)
precision_weighted = precision_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
recall_weighted = recall_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
f1_weighted = f1_score(y_true_new, y_pred_new, average='weighted', zero_division=0)
precision_macro = precision_score(y_true_new, y_pred_new, average='macro', zero_division=0)
recall_macro = recall_score(y_true_new, y_pred_new, average='macro', zero_division=0)
f1_macro = f1_score(y_true_new, y_pred_new, average='macro', zero_division=0)
specificity_macro = specificity_score(y_true_new, y_pred_new, classes=[0, 1, 2])

# محاسبه AUC
y_true_bin = label_binarize(y_true_new, classes=[0, 1, 2])
auc = roc_auc_score(y_true_bin, y_pred_prob, multi_class='ovr', average='weighted')

# محاسبه RMSE
rmse = np.sqrt(mean_squared_error(y_true_new, y_pred_new))

# محاسبه Confusion Matrix
cm = confusion_matrix(y_true_new, y_pred_new, labels=[0, 1, 2])

# رسم و ذخیره منحنی ROC
plt.figure(figsize=(8, 6))
for i in range(3):
    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_pred_prob[:, i])
    plt.plot(fpr, tpr, label=f'ROC منحنی کلاس {i} (AUC = {roc_auc_score(y_true_bin[:, i], y_pred_prob[:, i]):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('نرخ مثبت کاذب (False Positive Rate)')
plt.ylabel('نرخ مثبت واقعی (True Positive Rate)')
plt.title('منحنی ROC برای هر کلاس')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.show()
files.download('roc_curve.png')

# رسم و ذخیره ماتریس کالیبراسیون
plt.figure(figsize=(8, 6))
for i in range(3):
    disp = CalibrationDisplay.from_predictions(y_true_bin[:, i], y_pred_prob[:, i], n_bins=10, name=f'کلاس {i}')
    disp.plot(ax=plt.gca(), name=f'کلاس {i}')
plt.title('ماتریس کالیبراسیون برای هر کلاس')
plt.savefig('calibration_plot.png')
plt.show()
files.download('calibration_plot.png')

# رسم و ذخیره ماتریس درهم‌ریختگی
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])
plt.xlabel('برچسب پیش‌بینی‌شده')
plt.ylabel('برچسب واقعی')
plt.title('ماتریس درهم‌ریختگی')
plt.savefig('confusion_matrix.png')
plt.show()
files.download('confusion_matrix.png')

# آزمون آماری معناداری (Chi-Square)
cm_flat = cm.ravel()
if len(cm_flat) == 9:  # اطمینان از ماتریس 3x3
    chi2, p_value, _, _ = chi2_contingency(cm)
else:
    chi2, p_value = np.nan, np.nan

# چاپ معیارهای ارزیابی
print("معیارهای ارزیابی مدل روی داده‌های جدید (مقایسه با لیبل‌های واقعی):")
print(f"1. دقت (Accuracy): {accuracy:.2%}")
print(f"2. دقت مثبت (Precision, weighted): {precision_weighted:.2%}")
print(f"3. یادآوری (Recall, weighted): {recall_weighted:.2%}")
print(f"4. امتیاز F1 (F1-Score, weighted): {f1_weighted:.2%}")
print(f"5. دقت مثبت (Precision, macro): {precision_macro:.2%}")
print(f"6. یادآوری (Recall, macro): {recall_macro:.2%}")
print(f"7. امتیاز F1 (F1-Score, macro): {f1_macro:.2%}")
print(f"8. ویژگی (Specificity, macro): {specificity_macro:.2%}")
print(f"AUC (weighted): {auc:.2%}")
print(f"RMSE: {rmse:.4f}")
print("\nماتریس درهم‌ریختگی (Confusion Matrix):")
print(cm)
print(f"\nآزمون Chi-Square: مقدار chi2 = {chi2:.2f}, مقدار p-value = {p_value:.4f}")
print("\nگزارش کامل طبقه‌بندی:")
print(classification_report(y_true_new, y_pred_new, zero_division=0))

# ایجاد دیتافریم خروجی
df_output = df_new.copy()[['متن توییت']]
df_output['برچسب'] = y_pred_new
df_output['مجموعه واژگان'] = ''
df_output['واژه/عبارت مشابه'] = ''
df_output['آستانه شباهت'] = 0.0

# بازسازی ستون‌های اضافی
for index, row in df_output.iterrows():
    category, matched_word, similarity = reconstruct_metadata(row['متن توییت'], row['برچسب'])
    df_output.at[index, 'مجموعه واژگان'] = category
    df_output.at[index, 'واژه/عبارت مشابه'] = matched_word
    df_output.at[index, 'آستانه شباهت'] = similarity

# ذخیره فایل اکسل خروجی
output_excel = "new_labeled_tweets.xlsx"
df_output.to_excel(output_excel, index=False)

# ذخیره فایل متنی با جداکننده |
output_text = "new_labeled_tweets.txt"
df_output.to_csv(output_text, sep='|', index=False, encoding='utf-8')

print(f"فایل اکسل ذخیره شد: {output_excel}")
print(f"فایل متنی ذخیره شد: {output_text}")
print(f"زمان کل پردازش: {time.time() - start_time:.2f} ثانیه")

# دانلود خودکار فایل‌های خروجی
files.download(output_excel)
files.download(output_text)
