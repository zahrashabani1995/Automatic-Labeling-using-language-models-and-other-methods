# =================================================================
# Ø³Ù„ÙˆÙ„ Û±: Ù†ØµØ¨ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒØŒ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ Ø§ÛŒÙ…Ù¾ÙˆØ±Øªâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ (Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ø¨Ø§ SMOTE)
# =================================================================
%%time

# --- Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø¬Ø¯Ø¯ Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ø®Ø·Ø§) ---
!pip install pandas numpy scikit-learn parsivar matplotlib seaborn imbalanced-learn openpyxl --quiet

# --- Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ---
import pandas as pd
import numpy as np
import re
import io
import time
import math
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from parsivar import Normalizer, Tokenizer 
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, 
    cohen_kappa_score, recall_score, make_scorer, roc_auc_score, roc_curve, 
    precision_recall_fscore_support, precision_score, log_loss, mean_absolute_error,
    brier_score_loss
)
from sklearn.preprocessing import LabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.calibration import calibration_curve
from imblearn.pipeline import Pipeline as ImbPipeline
from imblearn.over_sampling import SMOTE
from sklearn.feature_extraction.text import TfidfVectorizer # Placeholder for defining features

# --- ØªÙˆØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§Ø±Ø³ÛŒ ---
normalizer = Normalizer()
tokenizer = Tokenizer()

def preprocess_text(text):
    if pd.isna(text):
        return ""
    text = normalizer.normalize(text)
    text = re.sub(r'[Û°-Û¹]', '', text) 
    text = re.sub(r'[0-9]', '', text) 
    text = re.sub(r'[^\w\s]', '', text) 
    text = re.sub(r'_', '', text) 
    tokens = [token for token in tokenizer.tokenize_words(text) if token.strip()]
    return " ".join(tokens)


# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ùˆ Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ---
print("Ù„Ø·ÙØ§ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ (Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'text' Ùˆ 'label') Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")
try:
    # âš ï¸ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ† Ø®Ø·ÙˆØ· Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
    uploaded = files.upload()
    file_name = next(iter(uploaded))
    df = pd.read_excel(io.BytesIO(uploaded[file_name]))
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
    df.columns = df.columns.str.lower()
    if 'text' not in df.columns or 'label' not in df.columns:
         raise ValueError("ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'text' Ùˆ 'label' Ø¨Ø§Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† Ø­Ø³Ø§Ø³ÛŒØª Ø¨Ù‡ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯/Ú©ÙˆÚ†Ú©).")

    # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ NaN Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    df.dropna(subset=['text', 'label'], inplace=True)
    
    df['cleaned_text'] = df['text'].apply(preprocess_text)
    y_labels = df['label'].astype(int).values 
    
    LABELS = np.unique(y_labels).tolist() 
    
    # ğŸ’¡ ØªØ¹Ø±ÛŒÙ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ (Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø´Ù…Ø§ Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
    CLASS_NAMES = {
        0: 'Non-related', 
        1: 'Indirect Suicide Signs', 
        2: 'Direct Suicide Signs'
    }
    
    # --- ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (60:30:10) ---
    X = df['cleaned_text']
    y = y_labels

    # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Training (60%) Ùˆ Temp (40%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=42, stratify=y
    )

    # ØªÙ‚Ø³ÛŒÙ… Temp Ø¨Ù‡ Validation (30%) Ùˆ Test (10%)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )

    print(f"âœ… ÙØ§ÛŒÙ„ Ø¨Ø§ {len(df)} Ø³Ø·Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯.")
    print(f"\nğŸ“Š ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
    print(f"Ø¢Ù…ÙˆØ²Ø´ (Train): {len(X_train)} ({len(X_train) / len(df) * 100:.1f}%)")
    print(f"Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (Validation): {len(X_val)} ({len(X_val) / len(df) * 100:.1f}%)")
    print(f"ØªØ³Øª (Test): {len(X_test)} ({len(X_test) / len(df) * 100:.1f}%)")

except Exception as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {e}")
    raise 
# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ SVM Ø¨Ø§ Word Embedding Ø¨Ø¯ÙˆÙ† SMOTE
# =================================================================
%%time

MODEL_NAME = "SVM_Word2Vec_NoSMOTE" # ğŸ’¡ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ SMOTE

# --- Û±. ØªØ¹Ø±ÛŒÙ Word Embedding (Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† TF-IDF) ---
# ğŸš¨ ØªÙˆØ¬Ù‡: Ø§ÛŒÙ† Ú©Ù„Ø§Ø³ Placeholder Ù‡Ù…Ø§Ù†Ù†Ø¯ Ù‚Ø¨Ù„ Ø§Ø³Øª Ùˆ Ø§Ø² TF-IDF Ø¯Ø§Ø®Ù„ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
class Word2VecVectorizer:
    def __init__(self, vector_size=100):
        self.vector_size = vector_size
        self.is_fitted = False

    def fit(self, X, y=None):
        self.is_fitted = True
        return self

    def transform(self, X):
        if self.is_fitted:
            print("âš ï¸ Ù‡Ø´Ø¯Ø§Ø±: Word2VecVectorizer ÙØ¹Ù„Ø§Ù‹ Ø§Ø² TF-IDF Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. Ù„Ø·ÙØ§Ù‹ Ø¢Ù† Ø±Ø§ Ø¨Ø§ Ø¨Ø±Ø¯Ø§Ø±Ø³Ø§Ø²ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ù†ÛŒØ¯.")
            # ğŸ’¡ Ø¯Ø± Ù…Ø¯Ù„ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ø±Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…ØªÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†ÛŒØ¯.
            vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_features=10000)
            # Ø§Ø² fit_transform Ø¯Ø± Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´ Ùˆ transform Ø¯Ø± Ø²Ù…Ø§Ù† ØªØ³Øª/Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯
            # Ø§Ù…Ø§ Ø¯Ø± PipelineØŒ fit/transform Ø¨Ø± Ø§Ø³Ø§Ø³ Ú¯Ø§Ù…â€ŒÙ‡Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒØ´ÙˆØ¯.
            # Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ú©Ø±Ø¯ ØµØ­ÛŒØ­ Ø¨Ø§ GridSearchCVØŒ Ø¨Ø§ÛŒØ¯ Ø§Ø² ÛŒÚ© Vectorizer ÙˆØ§Ù‚Ø¹ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯.
            return TfidfVectorizer(ngram_range=(1, 3), max_features=10000).fit_transform(X)
        else:
             return X

# Word Embedding Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø´Ù…Ø§
word_embedding = TfidfVectorizer(ngram_range=(1, 3), max_features=10000) # ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² TF-IDF Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒ

# Û². ØªØ¹Ø±ÛŒÙ SVM
svm_classifier = SVC(random_state=42, probability=True, decision_function_shape='ovr') 

# ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² sklearn.pipeline.Pipeline Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ (Ø­Ø°Ù ImbPipeline Ùˆ SMOTE)
pipeline = Pipeline([
    ('embedding', word_embedding),
    ('clf', svm_classifier) 
])

# Û³. ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (Grid Search)
param_grid = {
    'clf__C': [0.1, 1, 10], 
    'clf__kernel': ['linear'] # ØªØ¶Ù…ÛŒÙ† Ú©Ø±Ù†Ù„ Ø®Ø·ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†
}

f1_macro_scorer = make_scorer(f1_score, average='macro')

print("ğŸš€ Ø´Ø±ÙˆØ¹ Grid Search Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
grid_search_start = time.time()

grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring=f1_macro_scorer,
    cv=3, 
    verbose=1,
    n_jobs=-1
)

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
grid_search.fit(X_train, y_train)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {best_params}")

# Û´. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
# =================================================================
# Ø³Ù„ÙˆÙ„ Û³: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ú©Ø§Ù…Ù„)
# =================================================================
%%time

from sklearn.metrics import log_loss, mean_absolute_error, roc_auc_score, precision_recall_fscore_support, confusion_matrix

# --- Ø§Ù„Ù: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û±) ---
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')
recall_macro = recall_score(y_test, y_pred, average='macro')
precision_macro = precision_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
logloss = log_loss(y_test, y_prob)
mae = mean_absolute_error(y_test, y_pred)

lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC OVR Ùˆ OVO
try:
    if len(LABELS) > 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob, multi_class='ovr')
        auc_roc_ovo = roc_auc_score(y_test, y_prob, multi_class='ovo')
    elif len(LABELS) == 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob[:, 1])
        auc_roc_ovo = "N/A (Binary)"
    else:
        auc_roc_ovr = "N/A"
        auc_roc_ovo = "N/A"
except Exception: 
    auc_roc_ovr = "Error"
    auc_roc_ovo = "Error"

results_overall = {
    'Model': [MODEL_NAME], 'Embedding': ['TF-IDF (No SMOTE)'], # ğŸ’¡ Ù†Ø§Ù… Embedding Ø§ØµÙ„Ø§Ø­ Ø´Ø¯
    'Accuracy': [accuracy], 'F1-Macro': [f1_macro], 'Recall-Macro': [recall_macro],
    'Precision-Macro': [precision_macro], 'Kappa': [kappa], 'MCC': [mcc],
    'AUC-ROC (OVR)': [auc_roc_ovr], 'AUC-ROC (OVO)': [auc_roc_ovo],
    'Log Loss': [logloss], 'MAE': [mae],
    'Training Time (s)': [training_time]
}
df_overall = pd.DataFrame(results_overall)


# --- Ø¨: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û²) ---
precision_c, recall_c, f1_c, support_c = precision_recall_fscore_support(
    y_test, y_pred, labels=LABELS, average=None
)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ SpecificityØŒ NPV Ùˆ AUC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
specificity_c = []
npv_c = []
auc_c = []

for i, label in enumerate(LABELS):
    cm_binary = confusion_matrix(y_test == label, y_pred == label, labels=[True, False])
    
    if cm_binary.size == 4:
        tn, fp, fn, tp = cm_binary.ravel()
    else:
        tn, fp, fn, tp = 0, 0, 0, 0

    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 
    specificity_c.append(specificity)

    npv = tn / (tn + fn) if (tn + fn) > 0 else 0 
    npv_c.append(npv)
    
    try:
        auc_class = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])
        auc_c.append(auc_class)
    except Exception:
        auc_c.append(np.nan)


df_class_metrics = pd.DataFrame({
    'Class': [CLASS_NAMES.get(i, f'Class {i}') for i in LABELS],
    'Precision': precision_c,
    'Recall (Sensitivity)': recall_c, 
    'Specificity': specificity_c,
    'F1-Score': f1_c,
    'NPV (Negative Predictive Value)': npv_c,
    'AUC (OVR)': auc_c,
    'Support': support_c
})

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø¬Ø¯ÙˆÙ„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
df_class_metrics.loc[len(df_class_metrics)] = {
    'Class': 'Macro Average',
    'Precision': precision_macro, 
    'Recall (Sensitivity)': recall_macro, 
    'F1-Score': f1_macro, 
    'Specificity': np.mean(specificity_c), 
    'NPV (Negative Predictive Value)': np.mean(npv_c),
    'AUC (OVR)': auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan,
    'Support': np.sum(support_c)
}


# --- Ø¬: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û³) ---
key_words_df = pd.DataFrame() 

if hasattr(best_model.named_steps['clf'], 'coef_'):
    
    tfidf_final = best_model.named_steps['embedding']
    svm_final = best_model.named_steps['clf']
    feature_names = tfidf_final.get_feature_names_out()
    
    # ğŸš¨ Ø±ÙØ¹ Ù‚Ø·Ø¹ÛŒ Ø®Ø·Ø§: ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ Ù…ØªØ±Ø§Ú©Ù…
    coefs_array = svm_final.coef_.toarray() if hasattr(svm_final.coef_, 'toarray') else svm_final.coef_
    num_coef_sets = coefs_array.shape[0] 
    
    # === Ù…Ù†Ø·Ù‚ Ø³Ø§Ø®Øª DataFrame ØªØ¶Ù…ÛŒÙ†ÛŒ Ø¨Ø§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ===
    if num_coef_sets == 1:
         coef_data = coefs_array.flatten() 
         coef_columns = [f'Weight (Primary Separation | {len(LABELS)} Classes)']
         data_dict = {'Feature': feature_names, coef_columns[0]: coef_data}
         coefs_df = pd.DataFrame(data_dict)
         key_words_df = coefs_df.sort_values(by=coef_columns[0], ascending=False).head(100)
         
    elif num_coef_sets == len(LABELS):
         coef_columns = [f'Weight for Class {c} ({CLASS_NAMES.get(c, c)})' for c in LABELS]
         data_dict = {'Feature': feature_names}
         for i, col_name in enumerate(coef_columns):
             data_dict[col_name] = coefs_array[i]
         coefs_df = pd.DataFrame(data_dict)
         
         positive_class_cols = [col for col in coefs_df.columns if 'Class 1' in col or 'Class 2' in col]
         if positive_class_cols:
             coefs_df['Abs_Sum_Weight'] = coefs_df[positive_class_cols].abs().sum(axis=1)
             coefs_df = coefs_df.sort_values(by='Abs_Sum_Weight', ascending=False)
             key_words_df = coefs_df[['Feature'] + [col for col in coefs_df.columns if 'Weight' in col and col != 'Abs_Sum_Weight']].head(100)
         else:
             key_words_df = coefs_df.sort_values(by=coef_columns[0], ascending=False).head(100)

    else:
         print(f"âš ï¸ Ø§Ø®Ø·Ø§Ø±: ØªØ¹Ø¯Ø§Ø¯ Ø¶Ø±Ø§ÛŒØ¨ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯Ù‡ ({num_coef_sets}) ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø§Ø³Øª. ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯.")
         
else:
    print("âš ï¸ Ø§Ø®Ø·Ø§Ø±: Ù…Ø¯Ù„ SVM Ø§Ø² Ú©Ø±Ù†Ù„ Ø®Ø·ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯Ù‡ ÛŒØ§ Embedding Ø´Ù…Ø§ Feature Names Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ù†Ø¯Ø§Ø±Ø¯.")


# --- Ø¯: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ ---
output_file = f'{MODEL_NAME}_Evaluation_Reports.xlsx'

with pd.ExcelWriter(output_file) as writer:
    df_overall.to_excel(writer, sheet_name='1_Overall_Metrics', index=False)
    df_class_metrics.to_excel(writer, sheet_name='2_Class_Metrics', index=False)
    key_words_df.to_excel(writer, sheet_name='3_Key_Words_Analysis', index=False)

files.download(output_file)
print(f"\nâœ… Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û³ (Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ Ø¢Ù…Ø§Ø±ÛŒ):")
# =================================================================
# Ø³Ù„ÙˆÙ„ Û´: ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¨ØµØ±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ)
# =================================================================
%%time

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ ---
plt.style.use('seaborn-v0_8-whitegrid')
CLASS_LABELS_EN = list(CLASS_NAMES.values())
COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c'] 
# AUC OVR Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Û³ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
auc_roc_ovr_plot = auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan

# Û±. Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ (Metrics Bar Chart)
plt.figure(figsize=(14, 7))
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† NPV Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ±Ø³ÛŒÙ…ÛŒ
metrics_to_plot = ['Precision', 'Recall (Sensitivity)', 'F1-Score', 'Specificity', 'NPV (Negative Predictive Value)'] 
df_plot = df_class_metrics[df_class_metrics['Class'] != 'Macro Average'].reset_index(drop=True)
macro_avg_row = df_class_metrics[df_class_metrics['Class'] == 'Macro Average']
width = 0.15 

x = np.arange(len(metrics_to_plot))
# ØªØ±Ø³ÛŒÙ… Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
for i, class_name in enumerate(df_plot['Class']):
    plt.bar(x + i * width, df_plot.iloc[i][metrics_to_plot], width=width, label=class_name, color=COLORS[i])

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§Ú©Ø±Ùˆ (Macro Average) Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ù‚Ø·Ù‡
for i, metric in enumerate(metrics_to_plot):
    plt.plot(x[i] + width * (len(LABELS) - 1) / 2, # Ù…Ø±Ú©Ø² Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ù‚Ø·Ù‡
             macro_avg_row[metric].iloc[0], 
             'o', color='black', markersize=8, zorder=10, 
             label='Macro Avg' if i == 0 else "")

plt.xticks(x + width * (len(LABELS) - 1) / 2, metrics_to_plot, rotation=15)
plt.ylabel('Score Value', fontsize=12)
plt.title(f'Comparative Metrics per Class and Macro Average ({MODEL_NAME})', fontsize=14)
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()


# Û². Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix)
cm = confusion_matrix(y_test, y_pred, labels=LABELS)
cm_df = pd.DataFrame(cm, index=CLASS_LABELS_EN, columns=CLASS_LABELS_EN)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='black', 
            annot_kws={"size": 14})
plt.title('Confusion Matrix (Test Data)', fontsize=16)
plt.ylabel('True Label', fontsize=14)
plt.xlabel('Predicted Label', fontsize=14)
plt.show()

# Û³. Ù†Ù…ÙˆØ¯Ø§Ø± ROC (Receiver Operating Characteristic)
plt.figure(figsize=(10, 7))
auc_scores = []

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ ØªØ±Ø³ÛŒÙ… ROC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ (One-vs-Rest)
for i in range(len(LABELS)):
    try:
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
        auc_score = df_class_metrics.iloc[i]['AUC (OVR)'] 
        plt.plot(fpr, tpr, color=COLORS[i], label=f'{CLASS_LABELS_EN[i]} (AUC = {auc_score:.2f})')
    except Exception as e:
        print(f"âš ï¸ Ø§Ø®Ø·Ø§Ø±: Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªØ±Ø³ÛŒÙ… ROC Ú©Ù„Ø§Ø³ {CLASS_LABELS_EN[i]} Ø±Ø® Ø¯Ø§Ø¯: {e}")

# ØªØ±Ø³ÛŒÙ… Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§Ú©Ø±Ùˆ (ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
if not np.isnan(auc_roc_ovr_plot):
    mean_fpr = np.linspace(0.0, 1.0, 100) # Ø§ØµÙ„Ø§Ø­ Ø±Ù†Ø¬
    mean_tpr = np.mean([np.interp(mean_fpr, *roc_curve(y_test_binarized[:, i], y_prob[:, i])[:2]) for i in range(len(LABELS))], axis=0)
    mean_tpr[0] = 0.0
    plt.plot(mean_fpr, mean_tpr, color='black', linestyle='--', label=f'Macro Average (AUC = {auc_roc_ovr_plot:.2f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve per Class (One-vs-Rest Strategy)', fontsize=16)
plt.legend(loc="lower right")
plt.show()


# Û´. Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† (Calibration Curve)
plt.figure(figsize=(10, 7))
plt.plot([0, 1], [0, 1], "r--", label="Perfectly Calibrated")
brier_scores = []

for i in range(len(LABELS)):
    y_true_class = (y_test == LABELS[i])
    y_prob_class = y_prob[:, i]
    
    brier = brier_score_loss(y_true_class, y_prob_class)
    brier_scores.append(brier)
    
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_true_class, y_prob_class, n_bins=10
    )
    
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", 
             label=f'{CLASS_LABELS_EN[i]} (Brier: {brier:.2f})', color=COLORS[i])

mean_brier = np.mean(brier_scores)
plt.text(0.1, 0.8, f'Mean Brier Score: {mean_brier:.2f}', fontsize=12, color='black', transform=plt.gca().transAxes)
plt.text(0.1, 0.75, f'Overall Log Loss: {logloss:.4f}', fontsize=12, color='black', transform=plt.gca().transAxes)


plt.ylabel("Fraction of Positives", fontsize=12)
plt.xlabel("Mean Predicted Probability", fontsize=12)
plt.title("Calibration Curve (Predicted vs. True Probability)", fontsize=16)
plt.legend(loc="lower right")
plt.show()

print("\nâœ… ØªÙ…Ø§Ù…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û´ (ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§):")

print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û± (Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡):")
