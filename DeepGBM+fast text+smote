# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ XGBoost Ø¨Ø§ FastText Ùˆ SMOTE
# =================================================================
%%time

MODEL_NAME = "XGBoost_FastText_SMOTE" 

# --- Û±. ØªØ¹Ø±ÛŒÙ FastText Embedding (Ù…Ø¬Ø¯Ø¯) ---

class FastTextVectorizer(BaseEstimator, TransformerMixin):
    """ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ù†Ø¯Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gensim FastText."""
    
    def __init__(self, vector_size=100, min_count=1, window=5, workers=4, min_n=3, max_n=6):
        self.vector_size = vector_size
        self.min_count = min_count
        self.window = window
        self.workers = workers
        self.min_n = min_n 
        self.max_n = max_n 
        self.fasttext_model = None

    def fit(self, X, y=None):
        from gensim.models.fasttext import FastText as FastTextModel 
        
        tokenized_sentences = [text.split() for text in X]
        
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ FastText Ø¨Ø§ vector_size={self.vector_size}, min_n={self.min_n}...")
        
        self.fasttext_model = FastTextModel(
            sentences=tokenized_sentences,
            vector_size=self.vector_size,
            min_count=self.min_count,
            window=self.window,
            workers=self.workers,
            min_n=self.min_n, 
            max_n=self.max_n, 
            seed=42
        )
        self.fasttext_model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)
        print("âœ… Ø¢Ù…ÙˆØ²Ø´ FastText Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
        return self

    def transform(self, X):
        tokenized_sentences = [text.split() for text in X]
        
        def get_mean_vector(tokens):
            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¯Ø± Ù…Ø¯Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†
            vectors = [self.fasttext_model.wv[word] for word in tokens if word in self.fasttext_model.wv]
            if vectors:
                return np.mean(vectors, axis=0)
            else:
                return np.zeros(self.vector_size)

        return np.array([get_mean_vector(tokens) for tokens in tokenized_sentences])


# --- Û². ØªØ¹Ø±ÛŒÙ Ø§Ø¬Ø²Ø§ÛŒ Pipeline (Ø¨Ø§ SMOTE) ---
from imblearn.pipeline import Pipeline as ImbPipeline 
from imblearn.over_sampling import SMOTE 
from xgboost import XGBClassifier
from sklearn.metrics import make_scorer, f1_score
from sklearn.model_selection import GridSearchCV
from sklearn.base import BaseEstimator, TransformerMixin 
import numpy as np 
import time

fasttext_vectorizer = FastTextVectorizer(vector_size=200) 
# ğŸ‘ˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SMOTE
smote_sampler = SMOTE(random_state=42) 

xgb_classifier = XGBClassifier(
    objective='multi:softprob', 
    eval_metric='mlogloss', 
    use_label_encoder=False, 
    random_state=42
) 

# ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ImbPipeline Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ SMOTE
pipeline = ImbPipeline([
    ('embedding', fasttext_vectorizer),
    ('smote', smote_sampler), # ğŸ‘ˆ Ù…Ø±Ø­Ù„Ù‡ SMOTE
    ('clf', xgb_classifier) 
])

# --- Û³. ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (Grid Search) ---
param_grid = {
    # Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ FastText
    'embedding__vector_size': [100, 200], 
    # ğŸ’¡ Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ SMOTE
    'smote__k_neighbors': [3, 5], 
    # Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ XGBoost
    'clf__n_estimators': [100, 200], 
    'clf__max_depth': [5, 7], 
    'clf__learning_rate': [0.1, 0.2] 
}

f1_macro_scorer = make_scorer(f1_score, average='macro')

print("ğŸš€ Ø´Ø±ÙˆØ¹ Grid Search Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
grid_search_start = time.time()

# Ø§Ø¬Ø±Ø§ÛŒ Grid Search
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring=f1_macro_scorer,
    cv=3, 
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {best_params}")

# Û´. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
