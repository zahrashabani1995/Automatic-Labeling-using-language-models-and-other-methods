# =================================================================
# Ø³Ù„ÙˆÙ„ Û±: Ù†ØµØ¨ØŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
# =================================================================
%%time

# --- Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ---
!pip install pandas numpy scikit-learn parsivar matplotlib seaborn imbalanced-learn openpyxl --quiet

# --- Ø§ÛŒÙ…Ù¾ÙˆØ±Øª Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ---
import pandas as pd
import numpy as np
import re
import io
import time
import math
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files
from parsivar import Normalizer, Tokenizer # ğŸš¨ PersianTagger Ø­Ø°Ù Ø´Ø¯
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import (
    accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, 
    cohen_kappa_score, recall_score, make_scorer, roc_auc_score, roc_curve, 
    precision_recall_fscore_support, precision_score
)
from sklearn.preprocessing import LabelBinarizer
from sklearn.pipeline import Pipeline
from sklearn.calibration import calibration_curve
from sklearn.metrics import brier_score_loss


# --- ØªÙˆØ§Ø¨Ø¹ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§Ø±Ø³ÛŒ ---
normalizer = Normalizer()
tokenizer = Tokenizer()

def preprocess_text(text):
    if pd.isna(text):
        return ""
    # Û±. Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ: Ø§ØµÙ„Ø§Ø­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ØŒ ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ Ùˆ Ù†ÛŒÙ…â€ŒÙØ§ØµÙ„Ù‡
    text = normalizer.normalize(text)
    # Û². Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ (Ø§Ø¹Ø¯Ø§Ø¯ØŒ Ø¹Ù„Ø§Ø¦Ù… Ù†Ú¯Ø§Ø±Ø´ÛŒ Ùˆ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ØºÛŒØ±Ø­Ø±ÙÛŒ)
    text = re.sub(r'[Û°-Û¹]', '', text) 
    text = re.sub(r'[0-9]', '', text) 
    text = re.sub(r'[^\w\s]', '', text) 
    text = re.sub(r'_', '', text) 
    # Û³. ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ØªÙˆÚ©Ù† Ùˆ Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ
    tokens = [token for token in tokenizer.tokenize_words(text) if token.strip()]
    return " ".join(tokens)


# --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ùˆ Ø§Ø¹Ù…Ø§Ù„ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ ---
print("Ù„Ø·ÙØ§ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ (Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'text' Ùˆ 'label') Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")
try:
    # âš ï¸ Ø¨Ø±Ø§ÛŒ Ø§Ø¬Ø±Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒØŒ Ø§ÛŒÙ† Ø®Ø·ÙˆØ· Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
    uploaded = files.upload()
    file_name = next(iter(uploaded))
    df = pd.read_excel(io.BytesIO(uploaded[file_name]))
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§
    df.columns = df.columns.str.lower()
    if 'text' not in df.columns or 'label' not in df.columns:
         raise ValueError("ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'text' Ùˆ 'label' Ø¨Ø§Ø´Ø¯ (Ø¨Ø¯ÙˆÙ† Ø­Ø³Ø§Ø³ÛŒØª Ø¨Ù‡ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯/Ú©ÙˆÚ†Ú©).")

    # Ø­Ø°Ù Ø³Ø·Ø±Ù‡Ø§ÛŒ Ø¯Ø§Ø±Ø§ÛŒ NaN Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
    df.dropna(subset=['text', 'label'], inplace=True)
    
    df['cleaned_text'] = df['text'].apply(preprocess_text)
    y_labels = df['label'].astype(int).values 
    
    LABELS = np.unique(y_labels).tolist() 
    
    # ğŸ’¡ ØªØ¹Ø±ÛŒÙ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ù„Ø§Ø³ (Ø¨Ø§ÛŒØ¯ Ø¨Ø§ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ Ø´Ù…Ø§ Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯)
    CLASS_NAMES = {
        0: 'Non-related', 
        1: 'Indirect Suicide Signs', 
        2: 'Direct Suicide Signs'
    }
    
    # --- ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (60:30:10) ---
    X = df['cleaned_text']
    y = y_labels

    # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Training (60%) Ùˆ Temp (40%)
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=42, stratify=y
    )

    # ØªÙ‚Ø³ÛŒÙ… Temp Ø¨Ù‡ Validation (30%) Ùˆ Test (10%)
    X_val, X_test, y_val, y_test = train_test_split(
        X_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp
    )

    print(f"âœ… ÙØ§ÛŒÙ„ Ø¨Ø§ {len(df)} Ø³Ø·Ø± Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯ Ùˆ Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙ‚Ø³ÛŒÙ… Ø´Ø¯.")
    print(f"\nğŸ“Š ØªÙˆØ²ÛŒØ¹ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§:")
    print(f"Ø¢Ù…ÙˆØ²Ø´ (Train): {len(X_train)} ({len(X_train) / len(df) * 100:.1f}%)")
    print(f"Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (Validation): {len(X_val)} ({len(X_val) / len(df) * 100:.1f}%)")
    print(f"ØªØ³Øª (Test): {len(X_test)} ({len(X_test) / len(df) * 100:.1f}%)")

except Exception as e:
    print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„: {e}")
    raise 
# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ SVM Ø¨Ø§ N-gram (TF-IDF) Ùˆ Grid Search Ø¨Ø¯ÙˆÙ† SMOTE
# =================================================================
%%time

MODEL_NAME = "SVM_TFIDF_NoSMOTE"

# Û±. ØªØ¹Ø±ÛŒÙ TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(
    ngram_range=(1, 3), 
    max_features=10000, 
    analyzer='word'
)

# Û². ØªØ¹Ø±ÛŒÙ Ù…Ø¯Ù„ Ùˆ Pipeline
svm_classifier = SVC(random_state=42, probability=True, decision_function_shape='ovr') 

pipeline = Pipeline([
    ('tfidf', tfidf_vectorizer),
    ('clf', svm_classifier) 
])

# Û³. ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (Grid Search)
param_grid = {
    'clf__C': [0.1, 1, 10], 
    'clf__kernel': ['linear'] # ğŸš¨ ØªØ¶Ù…ÛŒÙ† Ú©Ø±Ù†Ù„ Ø®Ø·ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù†
}

f1_macro_scorer = make_scorer(f1_score, average='macro')

print("ğŸš€ Ø´Ø±ÙˆØ¹ Grid Search (ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§) Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
grid_search_start = time.time()

grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring=f1_macro_scorer,
    cv=3, 
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {best_params}")

# Û´. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
# =================================================================
# Ø³Ù„ÙˆÙ„ Û³: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ú©Ø§Ù…Ù„)
# =================================================================
%%time

from sklearn.metrics import log_loss, mean_absolute_error, roc_auc_score, precision_recall_fscore_support, confusion_matrix

# --- Ø§Ù„Ù: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û±) ---
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')
recall_macro = recall_score(y_test, y_pred, average='macro')
precision_macro = precision_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
logloss = log_loss(y_test, y_prob)
mae = mean_absolute_error(y_test, y_pred)

lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC OVR
try:
    if len(LABELS) > 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob, multi_class='ovr')
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC OVO (One-vs-One)
        auc_roc_ovo = roc_auc_score(y_test, y_prob, multi_class='ovo')
    elif len(LABELS) == 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob[:, 1])
        auc_roc_ovo = "N/A (Binary)"
    else:
        auc_roc_ovr = "N/A"
        auc_roc_ovo = "N/A"
except Exception: # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§Ù‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡
    auc_roc_ovr = "Error"
    auc_roc_ovo = "Error"

results_overall = {
    'Model': [MODEL_NAME], 'Embedding': ['N-gram (TF-IDF)'],
    'Accuracy': [accuracy], 'F1-Macro': [f1_macro], 'Recall-Macro': [recall_macro],
    'Precision-Macro': [precision_macro], 'Kappa': [kappa], 'MCC': [mcc],
    'AUC-ROC (OVR)': [auc_roc_ovr], 'AUC-ROC (OVO)': [auc_roc_ovo],
    'Log Loss': [logloss], 'MAE': [mae],
    'Training Time (s)': [training_time]
}
df_overall = pd.DataFrame(results_overall)


# --- Ø¨: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û²) ---
precision_c, recall_c, f1_c, support_c = precision_recall_fscore_support(
    y_test, y_pred, labels=LABELS, average=None
)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ SpecificityØŒ NPV Ùˆ AUC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
specificity_c = []
npv_c = []
auc_c = []

for i, label in enumerate(LABELS):
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ TN, FP, FN, TP Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ (True vs. Not-True)
    # confusion_matrix(y_true == label, y_pred == label, labels=[True, False])
    cm_binary = confusion_matrix(y_test == label, y_pred == label, labels=[True, False])
    
    # Ø§Ú¯Ø± Ú©Ù„Ø§Ø³ Ø¯Ø± Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ ØªØ³Øª ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² ØµÙØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if cm_binary.size == 4:
        tn, fp, fn, tp = cm_binary.ravel()
    else:
        # Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ú©Ù„Ø§Ø³ Ø®Ø§Øµ Ø¯Ø± ØªØ³Øª (Ù‡Ø±Ú†Ù†Ø¯ Ù†Ø§Ø¯Ø± Ø§Ø³Øª)
        tn, fp, fn, tp = 0, 0, 0, 0

    # Specificity = TN / (TN + FP) (True Negative Rate)
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 
    specificity_c.append(specificity)

    # NPV = TN / (TN + FN) (Negative Predictive Value)
    npv = tn / (tn + fn) if (tn + fn) > 0 else 0 
    npv_c.append(npv)
    
    # Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
    try:
        auc_class = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])
        auc_c.append(auc_class)
    except Exception:
        auc_c.append(np.nan)


df_class_metrics = pd.DataFrame({
    'Class': [CLASS_NAMES.get(i, f'Class {i}') for i in LABELS],
    'Precision': precision_c,
    'Recall (Sensitivity)': recall_c, # Recall Ù‡Ù…Ø§Ù† Sensitivity Ø§Ø³Øª
    'Specificity': specificity_c,
    'F1-Score': f1_c,
    'NPV (Negative Predictive Value)': npv_c,
    'AUC (OVR)': auc_c,
    'Support': support_c
})

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø¬Ø¯ÙˆÙ„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
df_class_metrics.loc[len(df_class_metrics)] = {
    'Class': 'Macro Average',
    'Precision': precision_macro, 
    'Recall (Sensitivity)': recall_macro, 
    'F1-Score': f1_macro, 
    'Specificity': np.mean(specificity_c), 
    'NPV (Negative Predictive Value)': np.mean(npv_c),
    'AUC (OVR)': auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan,
    'Support': np.sum(support_c)
}


# --- Ø¬: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù…Ø«Ø¨Øª (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û³) ---
key_words_df = pd.DataFrame() 

if hasattr(best_model.named_steps['clf'], 'coef_'):
    
    tfidf_final = best_model.named_steps['tfidf']
    svm_final = best_model.named_steps['clf']
    feature_names = tfidf_final.get_feature_names_out()
    
    # ğŸš¨ Ø±ÙØ¹ Ù‚Ø·Ø¹ÛŒ Ø®Ø·Ø§: ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø¢Ø±Ø§ÛŒÙ‡ Ù…ØªØ±Ø§Ú©Ù… Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ Scipy Sparse
    coefs_array = svm_final.coef_.toarray() if hasattr(svm_final.coef_, 'toarray') else svm_final.coef_
    num_coef_sets = coefs_array.shape[0] 
    
    # === Ù…Ù†Ø·Ù‚ Ø³Ø§Ø®Øª DataFrame ØªØ¶Ù…ÛŒÙ†ÛŒ Ø¨Ø§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ ===
    if num_coef_sets == 1:
         # Ø­Ø§Ù„Øª Ù…Ø´Ú©Ù„â€ŒØ³Ø§Ø² Ø´Ù…Ø§ (1 Ø¶Ø±ÛŒØ¨): Ø³Ø§Ø®Øª Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
         coef_data = coefs_array.flatten() 
         coef_columns = [f'Weight (Primary Separation | {len(LABELS)} Classes)']
         
         # Ø³Ø§Ø®Øª DataFrame Ø¨Ø§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
         data_dict = {'Feature': feature_names, coef_columns[0]: coef_data}
         coefs_df = pd.DataFrame(data_dict)
         
         # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªÙ†Ù‡Ø§ Ø³ØªÙˆÙ† Ù…ÙˆØ¬ÙˆØ¯
         key_words_df = coefs_df.sort_values(by=coef_columns[0], ascending=False).head(100)
         
    elif num_coef_sets == len(LABELS):
         # Ø­Ø§Ù„Øª Ø§ÛŒØ¯Ù‡ Ø¢Ù„ OVR (3 Ø¶Ø±ÛŒØ¨): Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯ T Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ (N, 3)
         coef_columns = [f'Weight for Class {c} ({CLASS_NAMES.get(c, c)})' for c in LABELS]
         
         # Ø³Ø§Ø®Øª DataFrame Ø¨Ø§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øª Ú†Ù†Ø¯ Ø¶Ø±ÛŒØ¨ÛŒ
         data_dict = {'Feature': feature_names}
         for i, col_name in enumerate(coef_columns):
             data_dict[col_name] = coefs_array[i]
         coefs_df = pd.DataFrame(data_dict)
         
         # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¬Ù…ÙˆØ¹ Ù‚Ø¯Ø± Ù…Ø·Ù„Ù‚ ÙˆØ²Ù†â€ŒÙ‡Ø§
         positive_class_cols = [col for col in coefs_df.columns if 'Class 1' in col or 'Class 2' in col]
         if positive_class_cols:
             coefs_df['Abs_Sum_Weight'] = coefs_df[positive_class_cols].abs().sum(axis=1)
             coefs_df = coefs_df.sort_values(by='Abs_Sum_Weight', ascending=False)
             key_words_df = coefs_df[['Feature'] + [col for col in coefs_df.columns if 'Weight' in col and col != 'Abs_Sum_Weight']].head(100)
         else:
             # Ø§Ú¯Ø± ÙÙ‚Ø· 1 Ø¶Ø±ÛŒØ¨ Ø¨Ø§Ø´Ø¯ Ø§Ù…Ø§ num_coef_sets = 3 Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ (ØºÛŒØ±Ù…Ø­ØªÙ…Ù„)
             key_words_df = coefs_df.sort_values(by=coef_columns[0], ascending=False).head(100)

    else:
         print(f"âš ï¸ Ø§Ø®Ø·Ø§Ø±: ØªØ¹Ø¯Ø§Ø¯ Ø¶Ø±Ø§ÛŒØ¨ Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù‡ Ø´Ø¯Ù‡ ({num_coef_sets}) ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø§Ø³Øª. ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯.")
         

else:
    print("âš ï¸ Ø§Ø®Ø·Ø§Ø±: Ù…Ø¯Ù„ SVM Ø§Ø² Ú©Ø±Ù†Ù„ Ø®Ø·ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù†Ú©Ø±Ø¯Ù‡ ÛŒØ§ coef_ Ù†Ø¯Ø§Ø±Ø¯. ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯.")


# --- Ø¯: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ ---
output_file = f'{MODEL_NAME}_Evaluation_Reports.xlsx'

with pd.ExcelWriter(output_file) as writer:
    df_overall.to_excel(writer, sheet_name='1_Overall_Metrics', index=False)
    df_class_metrics.to_excel(writer, sheet_name='2_Class_Metrics', index=False)
    key_words_df.to_excel(writer, sheet_name='3_Key_Words_Analysis', index=False)

files.download(output_file)
print(f"\nâœ… Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û³ (Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ Ø¢Ù…Ø§Ø±ÛŒ):")
# =================================================================
# Ø³Ù„ÙˆÙ„ Û´: ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¨ØµØ±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ)
# =================================================================
%%time

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ ---
plt.style.use('seaborn-v0_8-whitegrid')
CLASS_LABELS_EN = list(CLASS_NAMES.values())
COLORS = ['#1f77b4', '#ff7f0e', '#2ca02c'] 
# AUC OVR Ú©Ù‡ Ø¯Ø± Ø³Ù„ÙˆÙ„ Û³ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
auc_roc_ovr_plot = auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan

# Û±. Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡â€ŒØ§ÛŒ (Metrics Bar Chart)
plt.figure(figsize=(14, 7))
# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† NPV Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªØ±Ø³ÛŒÙ…ÛŒ
metrics_to_plot = ['Precision', 'Recall (Sensitivity)', 'F1-Score', 'Specificity', 'NPV (Negative Predictive Value)'] 
df_plot = df_class_metrics[df_class_metrics['Class'] != 'Macro Average'].reset_index(drop=True)
macro_avg_row = df_class_metrics[df_class_metrics['Class'] == 'Macro Average']
width = 0.15 

x = np.arange(len(metrics_to_plot))
# ØªØ±Ø³ÛŒÙ… Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
for i, class_name in enumerate(df_plot['Class']):
    plt.bar(x + i * width, df_plot.iloc[i][metrics_to_plot], width=width, label=class_name, color=COLORS[i])

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§Ú©Ø±Ùˆ (Macro Average) Ø¨Ù‡ ØµÙˆØ±Øª Ù†Ù‚Ø·Ù‡
for i, metric in enumerate(metrics_to_plot):
    plt.plot(x[i] + width * (len(LABELS) - 1) / 2, # Ù…Ø±Ú©Ø² Ù‚Ø±Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ù‚Ø·Ù‡
             macro_avg_row[metric].iloc[0], 
             'o', color='black', markersize=8, zorder=10, 
             label='Macro Avg' if i == 0 else "")

plt.xticks(x + width * (len(LABELS) - 1) / 2, metrics_to_plot, rotation=15)
plt.ylabel('Score Value', fontsize=12)
plt.title(f'Comparative Metrics per Class and Macro Average ({MODEL_NAME})', fontsize=14)
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()


# Û². Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix)
cm = confusion_matrix(y_test, y_pred, labels=LABELS)
cm_df = pd.DataFrame(cm, index=CLASS_LABELS_EN, columns=CLASS_LABELS_EN)

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='black', 
            annot_kws={"size": 14})
plt.title('Confusion Matrix (Test Data)', fontsize=16)
plt.ylabel('True Label', fontsize=14)
plt.xlabel('Predicted Label', fontsize=14)
plt.show()

# Û³. Ù†Ù…ÙˆØ¯Ø§Ø± ROC (Receiver Operating Characteristic)
plt.figure(figsize=(10, 7))
auc_scores = []

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ ØªØ±Ø³ÛŒÙ… ROC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³ (One-vs-Rest)
for i in range(len(LABELS)):
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ROC Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´ÙˆÙ†Ø¯ (Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ Ø¯Ø± Ø³Ù„ÙˆÙ„ Û³)
    try:
        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])
        # AUC Ø§Ø² Ø¬Ø¯ÙˆÙ„ metrics Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯
        auc_score = df_class_metrics.iloc[i]['AUC (OVR)'] 
        plt.plot(fpr, tpr, color=COLORS[i], label=f'{CLASS_LABELS_EN[i]} (AUC = {auc_score:.2f})')
    except Exception as e:
        print(f"âš ï¸ Ø§Ø®Ø·Ø§Ø±: Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± ØªØ±Ø³ÛŒÙ… ROC Ú©Ù„Ø§Ø³ {CLASS_LABELS_EN[i]} Ø±Ø® Ø¯Ø§Ø¯: {e}")

# ØªØ±Ø³ÛŒÙ… Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù…Ø§Ú©Ø±Ùˆ (ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯)
if not np.isnan(auc_roc_ovr_plot):
    mean_fpr = np.linspace(0, 1, 100)
    # Ø§Ø² interpolation Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† TPR Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    mean_tpr = np.mean([np.interp(mean_fpr, *roc_curve(y_test_binarized[:, i], y_prob[:, i])[:2]) for i in range(len(LABELS))], axis=0)
    mean_tpr[0] = 0.0
    plt.plot(mean_fpr, mean_tpr, color='black', linestyle='--', label=f'Macro Average (AUC = {auc_roc_ovr_plot:.2f})', linewidth=2)

plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate', fontsize=12)
plt.ylabel('True Positive Rate', fontsize=12)
plt.title('ROC Curve per Class (One-vs-Rest Strategy)', fontsize=16)
plt.legend(loc="lower right")
plt.show()


# Û´. Ù†Ù…ÙˆØ¯Ø§Ø± Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† (Calibration Curve)
plt.figure(figsize=(10, 7))
plt.plot([0, 1], [0, 1], "r--", label="Perfectly Calibrated")
brier_scores = []

for i in range(len(LABELS)):
    y_true_class = (y_test == LABELS[i])
    y_prob_class = y_prob[:, i]
    
    # ğŸš¨ Log Loss Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯
    brier = brier_score_loss(y_true_class, y_prob_class)
    brier_scores.append(brier)
    
    fraction_of_positives, mean_predicted_value = calibration_curve(
        y_true_class, y_prob_class, n_bins=10
    )
    
    plt.plot(mean_predicted_value, fraction_of_positives, "s-", 
             label=f'{CLASS_LABELS_EN[i]} (Brier: {brier:.2f})', color=COLORS[i])

mean_brier = np.mean(brier_scores)
plt.text(0.1, 0.8, f'Mean Brier Score: {mean_brier:.2f}', fontsize=12, color='black', transform=plt.gca().transAxes)
plt.text(0.1, 0.75, f'Overall Log Loss: {logloss:.4f}', fontsize=12, color='black', transform=plt.gca().transAxes)


plt.ylabel("Fraction of Positives", fontsize=12)
plt.xlabel("Mean Predicted Probability", fontsize=12)
plt.title("Calibration Curve (Predicted vs. True Probability)", fontsize=16)
plt.legend(loc="lower right")
plt.show()

print("\nâœ… ØªÙ…Ø§Ù…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û´ (ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§):")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û± (Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡):")
