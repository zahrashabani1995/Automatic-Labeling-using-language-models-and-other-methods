# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ LSTM Ø¨Ø§ FastText (Embedding Ø«Ø§Ø¨Øª)
# =================================================================
%%time

MODEL_NAME = "LSTM_FastText_FixedEmb" 

# ğŸš¨ ØªØ¹Ø±ÛŒÙ Ù…Ø¬Ø¯Ø¯ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ (Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¯Ø³ØªØ±Ø³ÛŒ) ğŸš¨
try:
    if 'y_labels' in globals():
        LABELS = np.unique(y_labels).tolist() 
        NUM_LABELS = len(LABELS)
    else:
        raise NameError("Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ (Ù…Ø§Ù†Ù†Ø¯ y_labels) Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªÙ†Ø¯. Ù„Ø·ÙØ§ Ø³Ù„ÙˆÙ„ Û± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
    
    CLASS_NAMES = {
        0: 'Non-related', 
        1: 'Indirect Suicide Signs', 
        2: 'Direct Suicide Signs'
    }
except NameError as e:
    print(f"âŒ Ø®Ø·Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ: {e}")
    raise 

# --- Û±. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ù…ÙˆØ²Ø´ FastText ---
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from gensim.models.fasttext import FastText as FastTextModel 
import numpy as np
import time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


MAX_SEQUENCE_LENGTH = 100 
MAX_WORDS = 10000 
EMBEDDING_DIM = 200 # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§Ø¨Ø¹Ø§Ø¯ Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø§ÛŒ FastText

# ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ (Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Keras)
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
tokenizer.fit_on_texts(X_train)

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Padding
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

X_train_seq = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)
X_test_seq = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)

# ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª One-Hot
y_train_one_hot = to_categorical(y_train, num_classes=NUM_LABELS)
y_test_one_hot = to_categorical(y_test, num_classes=NUM_LABELS)

# --- Û². Ø³Ø§Ø®Øª Ù…Ø§ØªØ±ÛŒØ³ Embedding Ø§Ø² FastText ---

print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ FastText Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ (Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ LSTM)...")

tokenized_sentences = [text.split() for text in X_train]

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ FastText
fasttext_model = FastTextModel(
    sentences=tokenized_sentences,
    vector_size=EMBEDDING_DIM,
    min_count=1,
    window=5,
    workers=4,
    min_n=3, max_n=6, # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø®Ø§Øµ FastText
    seed=42
)
fasttext_model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)
print("âœ… Ø¢Ù…ÙˆØ²Ø´ FastText Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

# Ø³Ø§Ø®Øª Ù…Ø§ØªØ±ÛŒØ³ Embedding Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Keras
VOCAB_SIZE = len(tokenizer.word_index) + 1 
embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))

for word, i in tokenizer.word_index.items():
    if i < VOCAB_SIZE:
        if word in fasttext_model.wv:
            embedding_matrix[i] = fasttext_model.wv[word]

# --- Û³. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LSTM Ø¨Ø§ Ù„Ø§ÛŒÙ‡ Embedding Ø§Ø² Ù¾ÛŒØ´ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ ---

model = Sequential()

# Ù„Ø§ÛŒÙ‡ Embedding: Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ FastText
model.add(Embedding(input_dim=VOCAB_SIZE, 
                    output_dim=EMBEDDING_DIM, 
                    weights=[embedding_matrix], # ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø§ØªØ±ÛŒØ³ FastText
                    input_length=MAX_SEQUENCE_LENGTH, 
                    trainable=False)) # ğŸš¨ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ø«Ø§Ø¨Øª Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯ (Fixed)

# ğŸš¨ Ù„Ø§ÛŒÙ‡ LSTM
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))

# Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú†Ú¯Ø§Ù„
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(NUM_LABELS, activation='softmax'))

model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

print("ğŸš€ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„ LSTM:")
model.summary()

# --- Û´. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---
grid_search_start = time.time() 

callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]

history = model.fit(
    X_train_seq, y_train_one_hot,
    epochs=15, 
    batch_size=32, 
    validation_split=0.1, 
    callbacks=callbacks,
    verbose=1
)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

# --- Ûµ. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test ---
print("\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª...")
y_prob_one_hot = model.predict(X_test_seq)
y_prob = y_prob_one_hot 
y_pred = np.argmax(y_prob_one_hot, axis=1)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
