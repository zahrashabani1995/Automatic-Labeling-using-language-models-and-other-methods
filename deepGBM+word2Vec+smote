#ØªØºÛŒÛŒØ± ÙÙ‚Ø· Ø¯Ø± Ø³Ù„ÙˆÙ„ 2
  # =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ XGBoost Ø¨Ø§ Word2Vec Ùˆ SMOTE
# =================================================================
%%time

MODEL_NAME = "XGBoost_Word2Vec_SMOTE" 

# --- Û±. ØªØ¹Ø±ÛŒÙ Word2Vec Embedding ---

class Word2VecVectorizer(BaseEstimator, TransformerMixin):
    """ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ù†Ø¯Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ú©Ù„Ù…Ø§Øª Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gensim Word2Vec."""
    
    def __init__(self, vector_size=100, min_count=1, window=5, workers=4):
        self.vector_size = vector_size
        self.min_count = min_count
        self.window = window
        self.workers = workers
        self.word2vec_model = None

    def fit(self, X, y=None):
        # Word2Vec Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù„ÛŒØ³Øª ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ Ø¯Ø§Ø±Ø¯
        tokenized_sentences = [text.split() for text in X]
        
        print(f"ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Word2Vec Ø¨Ø§ vector_size={self.vector_size}...")
        
        # ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…ØªØ¯ Word2Vec
        self.word2vec_model = gensim.models.Word2Vec(
            sentences=tokenized_sentences,
            vector_size=self.vector_size,
            min_count=self.min_count,
            window=self.window,
            workers=self.workers,
            seed=42
        )
        # Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„
        self.word2vec_model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)
        print("âœ… Ø¢Ù…ÙˆØ²Ø´ Word2Vec Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")
        return self

    def transform(self, X):
        tokenized_sentences = [text.split() for text in X]
        
        def get_mean_vector(tokens):
            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ú©Ù„Ù…Ø§ØªÛŒ Ú©Ù‡ Ø¯Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ†Ø§Ù…Ù‡ Ù…Ø¯Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯
            vectors = [self.word2vec_model.wv[word] for word in tokens if word in self.word2vec_model.wv]
            if vectors:
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Ú©Ù„Ù…Ø§Øª
                return np.mean(vectors, axis=0)
            else:
                # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± ØµÙØ± Ø¯Ø± ØµÙˆØ±Øª Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯Ù† Ø³Ù†Ø¯ ÛŒØ§ OOV Ø¨ÙˆØ¯Ù† Ù‡Ù…Ù‡ Ú©Ù„Ù…Ø§Øª
                return np.zeros(self.vector_size)

        return np.array([get_mean_vector(tokens) for tokens in tokenized_sentences])


# --- Û². ØªØ¹Ø±ÛŒÙ Ø§Ø¬Ø²Ø§ÛŒ Pipeline (Ø¨Ø§ SMOTE) ---
word2vec_vectorizer = Word2VecVectorizer(vector_size=100) 
# ğŸ‘ˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SMOTE
smote_sampler = SMOTE(random_state=42) 

xgb_classifier = XGBClassifier(
    objective='multi:softprob', 
    eval_metric='mlogloss', 
    use_label_encoder=False, 
    random_state=42
) 

# ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ImbPipeline Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ù…Ø§Ù„ SMOTE
pipeline = ImbPipeline([
    ('embedding', word2vec_vectorizer),
    ('smote', smote_sampler), # ğŸ‘ˆ Ù…Ø±Ø­Ù„Ù‡ SMOTE
    ('clf', xgb_classifier) 
])

# --- Û³. ØªÙ†Ø¸ÛŒÙ… Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ (Grid Search) ---
param_grid = {
    # Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Word2Vec
    'embedding__vector_size': [100, 200], 
    # ğŸ’¡ Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ SMOTE
    'smote__k_neighbors': [3, 5], 
    # Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ XGBoost
    'clf__n_estimators': [100, 200], 
    'clf__max_depth': [5, 7], 
    'clf__learning_rate': [0.1, 0.2] 
}

f1_macro_scorer = make_scorer(f1_score, average='macro')

print("ğŸš€ Ø´Ø±ÙˆØ¹ Grid Search Ùˆ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„...")
grid_search_start = time.time()

# Ø§Ø¬Ø±Ø§ÛŒ Grid Search
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    scoring=f1_macro_scorer,
    cv=3, 
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train, y_train)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

best_model = grid_search.best_estimator_
best_params = grid_search.best_params_

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
print(f"âœ… Ø¨Ù‡ØªØ±ÛŒÙ† Ù‡Ø§ÛŒÙ¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÛŒØ§ÙØª Ø´Ø¯Ù‡: {best_params}")

# Û´. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
