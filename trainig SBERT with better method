import pandas as pd
import re
from google.colab import files
import io
from sentence_transformers import SentenceTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import label_binarize
from imblearn.over_sampling import SMOTE
import time
import numpy as np

# Ø²Ù…Ø§Ù†â€ŒØ³Ù†Ø¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
start_time = time.time()

# ØªØ¹Ø±ÛŒÙ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ Ùˆ Ø¯ÙˆÙ…
category_1 = [
    "Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ø¢Ù…Ø§Ø¯Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù†Ù‚Ø´Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯ Ø®ÙˆØ¯Ø®ÙˆØ§Ø³ØªÙ‡", "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…",
    "ØªØ§ Ø§Ø¨Ø¯ Ø¨Ø®ÙˆØ§Ø¨Ù…", "Ù‡ÛŒÚ†ÙˆÙ‚Øª Ø¨ÛŒØ¯Ø§Ø± Ù†Ø´Ù…", "Ø¨Ù‡ Ø²Ù†Ø¯Ú¯ÛŒâ€ŒØ§Ù… Ù¾Ø§ÛŒØ§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù…", "Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù…",
    "Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø²Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ù…", "Ø®ÙˆØ¯Ø³ÙˆØ²ÛŒ", "Ø²Ù†Ø¯Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù† ØªÙ…Ø§Ù… Ø´Ø¯Ù‡", "Ø®ÙˆØ¯Ù… Ø±Ø§ Ø®Ù„Ø§Øµ Ú©Ù†Ù…",
    "Ø®ÙˆØ¯Ù…Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ø®ÙˆØ¯Ù… Ø±Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ú†Ø±Ø§ Ù†Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…ØŸ", "#Ø®ÙˆØ¯Ú©Ø´ÛŒ"
]

category_2 = [
    "Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø¯Ø§Ø±Ù… Ø¹Ø°Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ø´Ù…", "Ù‡ÛŒÚ† Ø§Ù…ÛŒØ¯ÛŒ Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ù…ØªÙ†ÙØ±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ø¨Ø¯Ù… Ù…ÛŒØ§Ø¯",
    "Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ø®Ø³ØªÙ‡â€ŒØ§Ù…", "ØªÙ†Ù‡Ø§ Ù…ÛŒâ€ŒÙ…ÛŒØ±Ù…", "Ù…Ø±Ú¯", "Ø¯Ø±Ø¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ",
    "ØªÙ†Ù‡Ø§ÛŒÛŒ", "ØºÙ…", "Ø¨ÛŒâ€ŒØ§Ø±Ø²Ø´", "Ø®Ø³ØªÚ¯ÛŒ", "ÙØ´Ø§Ø± Ø±ÙˆÙ…Ù‡", "Ø§Ø³ØªØ±Ø³", "Ø§Ø³ØªØ±Ø³ Ø¯Ø§Ø±Ù…", "Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¨ÛŒâ€ŒÙØ§ÛŒØ¯Ù‡ Ø§Ø³Øª",
    "Ù‡ÛŒÚ†Ú©Ø³ Ù…Ø±Ø§ Ù†Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯", "Ø§Ø² Ù‡Ù…Ù‡ Ú†ÛŒØ² Ù…ØªÙ†ÙØ±Ù…", "#Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "#ØªÙ†Ù‡Ø§ÛŒÛŒ", "#Ø¯Ø±Ø¯", "#ØºÙ…", "#Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"
]

# Ú©Ù„Ù…Ø§Øª Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø®ÙˆØ¯Ø²Ù†ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ
serious_context = [
    "Ù‚Ø±Øµ", "Ø®ÙˆÙ†", "Ø¢Ø³ÛŒØ¨", "Ú¯Ø±ÛŒÙ‡", "Ø¬ÛŒØº", "Ø¯Ø±Ø¯", "Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"
]
humorous_context = [
    "ğŸ˜‚", "ğŸ˜", "ğŸ¤£", "Ø³ÙˆØªÛŒ", "Ú©ØµØ®Ù„", "Ø®Ù†Ø¯Ù‡", "Ø´ÙˆØ®ÛŒ", "ÙØ§Ù†", "Ù‡Ù‡Ù‡", "Ù‡Ø§Ù‡Ø§"
]

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
def reconstruct_metadata(tweet, label):
    tweet = str(tweet).lower()
    
    if "Ø®ÙˆØ¯Ø²Ù†ÛŒ" in tweet:
        if any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in serious_context):
            return "Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.9
        elif any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in humorous_context) or "ğŸ˜‚" in tweet or "ğŸ˜" in tweet:
            return "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.3
        else:
            return "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.3
    
    for word in category_1:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„", word, 0.9
    
    for word in category_2:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return "Ø¯Ø³ØªÙ‡ Ø¯ÙˆÙ…", word, 0.65
    
    return "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", 0.3

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Specificity
def specificity_score(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred, labels=classes)
    specificity_per_class = []
    for i in range(len(classes)):
        tn = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
        fp = np.sum(cm[:, i]) - cm[i, i]
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        specificity_per_class.append(specificity)
    return np.mean(specificity_per_class)

# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§
!pip install sentence-transformers imbalanced-learn

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ 'labeled_tweets_3000.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded_train = files.upload()

if not uploaded_train:
    raise ValueError("ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ 'labeled_tweets_3000.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ
train_file = list(uploaded_train.keys())[0]
df_train = pd.read_excel(io.BytesIO(uploaded_train[train_file]))

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
required_columns = ['Ù…ØªÙ† ØªÙˆÛŒÛŒØª', 'Ø¨Ø±Ú†Ø³Ø¨']
if not all(col in df_train.columns for col in required_columns):
    raise ValueError("Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' ÛŒØ§ 'Ø¨Ø±Ú†Ø³Ø¨' Ø¯Ø± ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
X_train_texts = df_train['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'].astype(str)
y_train = df_train['Ø¨Ø±Ú†Ø³Ø¨'].astype(int)

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ SBERT Ù‚ÙˆÛŒâ€ŒØªØ±
model_sbert = SentenceTransformer('xlm-r-bert-base-nli-stsb-mean-tokens')

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ...")
X_train_embeddings = model_sbert.encode(X_train_texts.tolist(), show_progress_bar=True, batch_size=32)

# Ø§Ø¹Ù…Ø§Ù„ SMOTE Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù†Ø§Ù…ØªÙˆØ§Ø²Ù†ÛŒ
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_embeddings, y_train)

# ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ùˆ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ
X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(
    X_train_balanced, y_train_balanced, test_size=0.2, random_state=42
)

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ RandomForest
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train_split, y_train_split)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Cross-Validation
cv_scores = cross_val_score(classifier, X_train_balanced, y_train_balanced, cv=5, scoring='accuracy')
print(f"Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø¯Ù‚Øª Cross-Validation (5-fold): {cv_scores.mean():.2%} (Â±{cv_scores.std():.2%})")

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ
y_pred_test = classifier.predict(X_test_split)
accuracy = accuracy_score(y_test_split, y_pred_test)
precision_weighted = precision_score(y_test_split, y_pred_test, average='weighted', zero_division=0)
recall_weighted = recall_score(y_test_split, y_pred_test, average='weighted', zero_division=0)
f1_weighted = f1_score(y_test_split, y_pred_test, average='weighted', zero_division=0)
precision_macro = precision_score(y_test_split, y_pred_test, average='macro', zero_division=0)
recall_macro = recall_score(y_test_split, y_pred_test, average='macro', zero_division=0)
f1_macro = f1_score(y_test_split, y_pred_test, average='macro', zero_division=0)
specificity_macro = specificity_score(y_test_split, y_pred_test, classes=[0, 1, 2])

# Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC
y_test_bin = label_binarize(y_test_split, classes=[0, 1, 2])
y_pred_prob = classifier.predict_proba(X_test_split)
auc = roc_auc_score(y_test_bin, y_pred_prob, multi_class='ovr', average='weighted')

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Confusion Matrix
cm = confusion_matrix(y_test_split, y_pred_test, labels=[0, 1, 2])

# Ú†Ø§Ù¾ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
print("Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´ÛŒ:")
print(f"1. Ø¯Ù‚Øª (Accuracy): {accuracy:.2%}")
print(f"2. Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision, weighted): {precision_weighted:.2%}")
print(f"3. ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall, weighted): {recall_weighted:.2%}")
print(f"4. Ø§Ù…ØªÛŒØ§Ø² F1 (F1-Score, weighted): {f1_weighted:.2%}")
print(f"5. Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision, macro): {precision_macro:.2%}")
print(f"6. ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall, macro): {recall_macro:.2%}")
print(f"7. Ø§Ù…ØªÛŒØ§Ø² F1 (F1-Score, macro): {f1_macro:.2%}")
print(f"8. ÙˆÛŒÚ˜Ú¯ÛŒ (Specificity, macro): {specificity_macro:.2%}")
print(f"AUC (weighted): {auc:.2%}")
print("\nÙ…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix):")
print(cm)
print("\nÚ¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print(classification_report(y_test_split, y_pred_test, zero_division=0))

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ 'new_tweets.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded_new = files.upload()

if not uploaded_new:
    raise ValueError("ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ 'new_tweets.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
new_file = list(uploaded_new.keys())[0]
df_new = pd.read_excel(io.BytesIO(uploaded_new[new_file]))

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† "Ù…ØªÙ† ØªÙˆÛŒÛŒØª"
if 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' not in df_new.columns:
    raise ValueError("Ø³ØªÙˆÙ† 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' Ø¯Ø± ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯...")
X_new_texts = df_new['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'].astype(str)
X_new_embeddings = model_sbert.encode(X_new_texts.tolist(), show_progress_bar=True, batch_size=32)

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§
y_pred_new = classifier.predict(X_new_embeddings)

# Ø§ØµÙ„Ø§Ø­ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯Ø²Ù†ÛŒ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ
for i, (tweet, label) in enumerate(zip(X_new_texts, y_pred_new)):
    tweet_lower = tweet.lower()
    if "Ø®ÙˆØ¯Ø²Ù†ÛŒ" in tweet_lower:
        if any(word in tweet_lower for word in humorous_context) or "ğŸ˜‚" in tweet_lower or "ğŸ˜" in tweet_lower:
            y_pred_new[i] = 0  # Ø®ÙˆØ¯Ø²Ù†ÛŒ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ: Ø¨Ø±Ú†Ø³Ø¨ 0

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø±ÙˆØ¬ÛŒ
df_output = df_new.copy()[['Ù…ØªÙ† ØªÙˆÛŒÛŒØª']]
df_output['Ø¨Ø±Ú†Ø³Ø¨'] = y_pred_new
df_output['Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†'] = ''
df_output['ÙˆØ§Ú˜Ù‡/Ø¹Ø¨Ø§Ø±Øª Ù…Ø´Ø§Ø¨Ù‡'] = ''
df_output['Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª'] = 0.0

# Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
for index, row in df_output.iterrows():
    category, matched_word, similarity = reconstruct_metadata(row['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'], row['Ø¨Ø±Ú†Ø³Ø¨'])
    df_output.at[index, 'Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†'] = category
    df_output.at[index, 'ÙˆØ§Ú˜Ù‡/Ø¹Ø¨Ø§Ø±Øª Ù…Ø´Ø§Ø¨Ù‡'] = matched_word
    df_output.at[index, 'Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª'] = similarity

# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø®Ø±ÙˆØ¬ÛŒ
output_excel = "new_labeled_tweets.xlsx"
df_output.to_excel(output_excel, index=False)

# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¨Ø§ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ |
output_text = "new_labeled_tweets.txt"
df_output.to_csv(output_text, sep='|', index=False, encoding='utf-8')

print(f"ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_excel}")
print(f"ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_text}")
print(f"Ø²Ù…Ø§Ù† Ú©Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {time.time() - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
files.download(output_excel)
files.download(output_text)
