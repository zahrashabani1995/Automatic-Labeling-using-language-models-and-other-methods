# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ LSTM Ø¨Ø§ Word2Vec (Embedding Ø«Ø§Ø¨Øª)
# =================================================================
%%time

MODEL_NAME = "LSTM_Word2Vec_FixedEmb" 

# ğŸš¨ ØªØ¹Ø±ÛŒÙ Ù…Ø¬Ø¯Ø¯ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ (Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¯Ø³ØªØ±Ø³ÛŒ) ğŸš¨
try:
    if 'y_labels' in globals():
        LABELS = np.unique(y_labels).tolist() 
        NUM_LABELS = len(LABELS)
    else:
        raise NameError("Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ (Ù…Ø§Ù†Ù†Ø¯ y_labels) Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªÙ†Ø¯. Ù„Ø·ÙØ§ Ø³Ù„ÙˆÙ„ Û± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
    
    CLASS_NAMES = {
        0: 'Non-related', 
        1: 'Indirect Suicide Signs', 
        2: 'Direct Suicide Signs'
    }
except NameError as e:
    print(f"âŒ Ø®Ø·Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ: {e}")
    raise 

# --- Û±. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ùˆ Ø¢Ù…ÙˆØ²Ø´ Word2Vec ---
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
from gensim.models import Word2Vec 
import numpy as np
import time
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


MAX_SEQUENCE_LENGTH = 100 
MAX_WORDS = 10000 
EMBEDDING_DIM = 100 # Ø§Ø¨Ø¹Ø§Ø¯ Word2Vec

# ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ (Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Keras)
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
tokenizer.fit_on_texts(X_train)

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Padding
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

X_train_seq = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)
X_test_seq = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)

# ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª One-Hot
y_train_one_hot = to_categorical(y_train, num_classes=NUM_LABELS)
y_test_one_hot = to_categorical(y_test, num_classes=NUM_LABELS)

# --- Û². Ø³Ø§Ø®Øª Ù…Ø§ØªØ±ÛŒØ³ Embedding Ø§Ø² Word2Vec ---

print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø¢Ù…ÙˆØ²Ø´ Word2Vec Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ù…Ø§ (Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ LSTM)...")

tokenized_sentences = [text.split() for text in X_train]

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Word2Vec
word2vec_model = Word2Vec(
    sentences=tokenized_sentences,
    vector_size=EMBEDDING_DIM,
    min_count=1,
    window=5,
    workers=4,
    seed=42
)
word2vec_model.train(tokenized_sentences, total_examples=len(tokenized_sentences), epochs=10)
print("âœ… Ø¢Ù…ÙˆØ²Ø´ Word2Vec Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

# Ø³Ø§Ø®Øª Ù…Ø§ØªØ±ÛŒØ³ Embedding Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡ Keras
VOCAB_SIZE = len(tokenizer.word_index) + 1 
embedding_matrix = np.zeros((VOCAB_SIZE, EMBEDDING_DIM))

for word, i in tokenizer.word_index.items():
    if i < VOCAB_SIZE:
        if word in word2vec_model.wv:
            embedding_matrix[i] = word2vec_model.wv[word]

# --- Û³. Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LSTM Ø¨Ø§ Ù„Ø§ÛŒÙ‡ Embedding Ø§Ø² Ù¾ÛŒØ´ ØªØ¹ÛŒÛŒÙ† Ø´Ø¯Ù‡ ---

model = Sequential()

# Ù„Ø§ÛŒÙ‡ Embedding: Ù…Ù‚Ø¯Ø§Ø±Ø¯Ù‡ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ÛŒ Word2Vec
model.add(Embedding(input_dim=VOCAB_SIZE, 
                    output_dim=EMBEDDING_DIM, 
                    weights=[embedding_matrix], # ğŸš¨ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø§ØªØ±ÛŒØ³ Word2Vec
                    input_length=MAX_SEQUENCE_LENGTH, 
                    trainable=False)) # ğŸš¨ Ø§ÛŒÙ† Ù…Ù‡Ù… Ø§Ø³Øª: Ø¨Ø±Ø¯Ø§Ø±Ù‡Ø§ Ø«Ø§Ø¨Øª Ù…ÛŒâ€ŒÙ…Ø§Ù†Ù†Ø¯ (Fixed)

# ğŸš¨ Ù„Ø§ÛŒÙ‡ LSTM
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2))

# Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú†Ú¯Ø§Ù„
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(NUM_LABELS, activation='softmax'))

model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

print("ğŸš€ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„ LSTM:")
model.summary()

# --- Û´. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---
grid_search_start = time.time() 

callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]

history = model.fit(
    X_train_seq, y_train_one_hot,
    epochs=15, 
    batch_size=32, 
    validation_split=0.1, 
    callbacks=callbacks,
    verbose=1
)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

# --- Ûµ. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test ---
print("\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª...")
y_prob_one_hot = model.predict(X_test_seq)
y_prob = y_prob_one_hot 
y_pred = np.argmax(y_prob_one_hot, axis=1)
# =================================================================
# Ø³Ù„ÙˆÙ„ Û³: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ (Ù†Ø³Ø®Ù‡ Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯ Ø´Ø¯Ù‡)
# =================================================================
%%time

from sklearn.metrics import (
    accuracy_score, f1_score, recall_score, precision_score, 
    confusion_matrix, log_loss, cohen_kappa_score, matthews_corrcoef, 
    roc_auc_score, mean_absolute_error, precision_recall_fscore_support
)
from sklearn.preprocessing import LabelBinarizer
import pandas as pd
import numpy as np
from google.colab import files 
import time

# ğŸš¨ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ MODEL_NAME Ùˆ training_time Ø§Ø² Ø³Ù„ÙˆÙ„ Û² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.
# ğŸš¨ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… LABELS Ùˆ CLASS_NAMES Ø§Ø² Ø³Ù„ÙˆÙ„ Û±/Û² Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù‡Ø³ØªÙ†Ø¯.
# ğŸš¨ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ y_testØŒ y_pred Ùˆ y_prob Ø§Ø² Ø³Ù„ÙˆÙ„ Û² Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù‡Ø³ØªÙ†Ø¯.


# --- Ø§Ù„Ù: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ú©Ù„ÛŒ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û±) ---
accuracy = accuracy_score(y_test, y_pred)
f1_macro = f1_score(y_test, y_pred, average='macro')
recall_macro = recall_score(y_test, y_pred, average='macro')
precision_macro = precision_score(y_test, y_pred, average='macro')
kappa = cohen_kappa_score(y_test, y_pred)
mcc = matthews_corrcoef(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Log Loss Ùˆ AUC (ÙÙ‚Ø· Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ Ù¾Ø±ÙˆØ¨Ø§Ù„ÛŒØªÛŒ)
try:
    logloss = log_loss(y_test, y_prob)
except ValueError:
    logloss = "N/A (Probabilities not available or incompatible)"

lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test)

try:
    if len(LABELS) > 2:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob, multi_class='ovr')
        auc_roc_ovo = roc_auc_score(y_test, y_prob, multi_class='ovo')
    else:
        auc_roc_ovr = roc_auc_score(y_test_binarized, y_prob[:, 1])
        auc_roc_ovo = "N/A (Binary)"
except Exception: 
    auc_roc_ovr = "Error/N/A"
    auc_roc_ovo = "Error/N/A"
    
# ØªØ¹ÛŒÛŒÙ† Ù†ÙˆØ¹ Embedding Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ
if 'TFIDF' in MODEL_NAME:
    EMBEDDING_TYPE = 'TF-IDF'
elif 'Word2Vec' in MODEL_NAME:
    EMBEDDING_TYPE = 'Word2Vec'
elif 'FastText' in MODEL_NAME:
    EMBEDDING_TYPE = 'FastText'
elif 'Keras' in MODEL_NAME:
    EMBEDDING_TYPE = 'Keras Trainable/Fixed'
else:
    EMBEDDING_TYPE = 'N/A'


results_overall = {
    'Model': [MODEL_NAME], 'Embedding': [EMBEDDING_TYPE],
    'Accuracy': [accuracy], 'F1-Macro': [f1_macro], 'Recall-Macro': [recall_macro],
    'Precision-Macro': [precision_macro], 'Kappa': [kappa], 'MCC': [mcc],
    'AUC-ROC (OVR)': [auc_roc_ovr], 'AUC-ROC (OVO)': [auc_roc_ovo],
    'Log Loss': [logloss], 'MAE': [mae],
    'Training Time (s)': [training_time] 
}
df_overall = pd.DataFrame(results_overall)


# --- Ø¨: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú©Ù„Ø§Ø³ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û²) ---
precision_c, recall_c, f1_c, support_c = precision_recall_fscore_support(
    y_test, y_pred, labels=LABELS, average=None
)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ SpecificityØŒ NPV Ùˆ AUC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
specificity_c = []
npv_c = []
auc_c = []

for i, label in enumerate(LABELS):
    cm_binary = confusion_matrix(y_test == label, y_pred == label, labels=[True, False])
    
    if cm_binary.size == 4:
        tn, fp, fn, tp = cm_binary.ravel()
    else:
        tn, fp, fn, tp = 0, 0, 0, 0

    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 
    specificity_c.append(specificity)

    npv = tn / (tn + fn) if (tn + fn) > 0 else 0 
    npv_c.append(npv)
    
    try:
        # AUC OVR Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³
        auc_class = roc_auc_score(y_test_binarized[:, i], y_prob[:, i])
        auc_c.append(auc_class)
    except Exception:
        auc_c.append(np.nan)


df_class_metrics = pd.DataFrame({
    'Class': [CLASS_NAMES.get(i, f'Class {i}') for i in LABELS],
    'Precision': precision_c,
    'Recall (Sensitivity)': recall_c, 
    'Specificity': specificity_c,
    'F1-Score': f1_c,
    'NPV (Negative Predictive Value)': npv_c,
    'AUC (OVR)': auc_c,
    'Support': support_c
})

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù¾Ø§ÛŒÛŒÙ† Ø¬Ø¯ÙˆÙ„ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§
df_class_metrics.loc[len(df_class_metrics)] = {
    'Class': 'Macro Average',
    'Precision': precision_macro, 
    'Recall (Sensitivity)': recall_macro, 
    'F1-Score': f1_macro, 
    'Specificity': np.mean(specificity_c), 
    'NPV (Negative Predictive Value)': np.mean(npv_c),
    'AUC (OVR)': auc_roc_ovr if isinstance(auc_roc_ovr, float) else np.nan,
    'Support': np.sum(support_c)
}


# --- Ø¬: Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ (ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Û³ - Placeholder Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ØºÛŒØ± TF-IDF) ---
# Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ KerasØŒ Ø§ÛŒÙ† ØªØ­Ù„ÛŒÙ„ Ø§Ø¹Ù…Ø§Ù„ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.
key_words_df = pd.DataFrame({
    'Feature': ['N/A (Key word analysis only for TF-IDF/CountVec models)'],
    'Weight': ['N/A']
}) 
print("âš ï¸ Ø§Ø®Ø·Ø§Ø±: ØªØ­Ù„ÛŒÙ„ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù†ÙˆØ¹ Ù…Ø¯Ù„ (Keras) Ø§Ø¹Ù…Ø§Ù„ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")


# --- Ø¯: Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ú©Ø³Ù„ ---
output_file = f'{MODEL_NAME}_Evaluation_Reports.xlsx'

with pd.ExcelWriter(output_file) as writer:
    df_overall.to_excel(writer, sheet_name='1_Overall_Metrics', index=False)
    df_class_metrics.to_excel(writer, sheet_name='2_Class_Metrics', index=False)
    key_words_df.to_excel(writer, sheet_name='3_Key_Words_Analysis', index=False)

files.download(output_file)
print(f"\nâœ… Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø±ÛŒ Ø¬Ø§Ù…Ø¹ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print("Ø²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û³ (Ú¯Ø²Ø§Ø±Ø´â€ŒØ¯Ù‡ÛŒ Ø¢Ù…Ø§Ø±ÛŒ):")

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
