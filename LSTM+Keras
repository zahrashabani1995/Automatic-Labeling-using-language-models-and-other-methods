# =================================================================
# Ø³Ù„ÙˆÙ„ Û²: Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ LSTM Ø¨Ø§ Keras (Embedding Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´)
# =================================================================
%%time

MODEL_NAME = "LSTM_Keras_TrainableEmb" 

# ğŸš¨ ØªØ¹Ø±ÛŒÙ Ù…Ø¬Ø¯Ø¯ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ (Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø¯Ø³ØªØ±Ø³ÛŒ) ğŸš¨
try:
    if 'y_labels' in globals():
        LABELS = np.unique(y_labels).tolist() 
        NUM_LABELS = len(LABELS)
    else:
        raise NameError("Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ú¯Ù„ÙˆØ¨Ø§Ù„ (Ù…Ø§Ù†Ù†Ø¯ y_labels) Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³ØªÙ†Ø¯. Ù„Ø·ÙØ§ Ø³Ù„ÙˆÙ„ Û± Ø±Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
    
    CLASS_NAMES = {
        0: 'Non-related', 
        1: 'Indirect Suicide Signs', 
        2: 'Direct Suicide Signs'
    }
except NameError as e:
    print(f"âŒ Ø®Ø·Ø§ÛŒ Ø­ÛŒØ§ØªÛŒ: {e}")
    raise 

# --- Û±. Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Keras ---
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import time
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping


# ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø¨Ø¹Ø§Ø¯ Ùˆ Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡
MAX_SEQUENCE_LENGTH = 100 
MAX_WORDS = 10000 
EMBEDDING_DIM = 128 # Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø¨Ø¹Ø§Ø¯ Embedding Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ LSTM

# ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ (Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Keras)
tokenizer = Tokenizer(num_words=MAX_WORDS, oov_token="<unk>")
tokenizer.fit_on_texts(X_train)

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ† Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„Ù‡ Ø§Ø¹Ø¯Ø§Ø¯ Ùˆ Padding
sequences_train = tokenizer.texts_to_sequences(X_train)
sequences_test = tokenizer.texts_to_sequences(X_test)

X_train_seq = pad_sequences(sequences_train, maxlen=MAX_SEQUENCE_LENGTH)
X_test_seq = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)

# ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ù‡ ÙØ±Ù…Øª One-Hot
y_train_one_hot = to_categorical(y_train, num_classes=NUM_LABELS)
y_test_one_hot = to_categorical(y_test, num_classes=NUM_LABELS)

# --- Û². Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LSTM ---

VOCAB_SIZE = len(tokenizer.word_index) + 1 

model = Sequential()

# Ù„Ø§ÛŒÙ‡ Embedding Ù‚Ø§Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´
model.add(Embedding(input_dim=VOCAB_SIZE, 
                    output_dim=EMBEDDING_DIM, 
                    input_length=MAX_SEQUENCE_LENGTH, 
                    trainable=True)) 

# ğŸš¨ Ù„Ø§ÛŒÙ‡ LSTM
model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)) # ÙˆØ§Ø­Ø¯Ù‡Ø§ÛŒ LSTM

# Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ú†Ú¯Ø§Ù„ (Dense) Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(NUM_LABELS, activation='softmax'))

model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])

print("ğŸš€ Ø®Ù„Ø§ØµÙ‡ Ù…Ø¯Ù„ LSTM:")
model.summary()

# --- Û³. Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ ---
grid_search_start = time.time() 

# ØªÙ†Ø¸ÛŒÙ… Callbacks
callbacks = [EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)]

history = model.fit(
    X_train_seq, y_train_one_hot,
    epochs=15, 
    batch_size=32, 
    validation_split=0.1, 
    callbacks=callbacks,
    verbose=1
)

grid_search_end = time.time()
training_time = grid_search_end - grid_search_start

print(f"\nâœ… Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯. Ø²Ù…Ø§Ù† Ø¢Ù…ÙˆØ²Ø´: {training_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

# --- Û´. Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Test ---
print("\nğŸ“Š Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª...")
y_prob_one_hot = model.predict(X_test_seq)
y_prob = y_prob_one_hot 
y_pred = np.argmax(y_prob_one_hot, axis=1)

print("\nØ²Ù…Ø§Ù† Ú©Ù„ Ø§Ø¬Ø±Ø§ÛŒ Ø³Ù„ÙˆÙ„ Û² (Ù…Ø¯Ù„Ø³Ø§Ø²ÛŒ):")
