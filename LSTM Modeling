# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (TensorFlow, Keras, Parsivar)
!pip install tensorflow parsivar scikit-learn numpy pandas openpyxl

import pandas as pd
import numpy as np
import re
import io
import matplotlib.pyplot as plt
import seaborn as sns
import math

from google.colab import files
from parsivar import Normalizer, Tokenizer # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Parsivar
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, confusion_matrix, f1_score, matthews_corrcoef, 
    cohen_kappa_score, mean_absolute_error, recall_score, roc_auc_score, roc_curve
)

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¹Ù…ÛŒÙ‚
import tensorflow as tf
from tensorflow.keras.models import Sequential
# **Ù„Ø§ÛŒÙ‡ LSTM Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Conv1D Ù…ÛŒâ€ŒØ´ÙˆØ¯**
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout 
from tensorflow.keras.preprocessing.text import Tokenizer as KerasTokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
# --- Ø§Ù„Ù: Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ (ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª) ---
print("Ù„Ø·ÙØ§ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ (Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª):")
uploaded = files.upload()
file_name = next(iter(uploaded))

df = pd.read_excel(io.BytesIO(uploaded[file_name]))
df.columns = ['text', 'label'] 

# --- Ø¨: Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ (Ù‡Ù…Ø§Ù† ØªØ§Ø¨Ø¹ Ù‚Ø¨Ù„ÛŒ) ---
normalizer = Normalizer()
tokenizer = Tokenizer()
stop_words = set([
    "Ø§Ø²", "Ø¨Ù‡", "Ø¨Ø§", "Ø¯Ø±", "Ø¨Ø±", "Ø¨Ø±Ø§ÛŒ", "Ú©Ù‡", "Ùˆ", "ÛŒØ§", "ÛŒÚ©", "Ø§ÛŒÙ†", "Ø¢Ù†",
    "Ù‡Ø§", "Ø§ÛŒ", "Ø±Ø§", "Ù‡Ù…", "Ø¨ÙˆØ¯", "Ø§Ø³Øª", "Ø¨Ø§Ø´Ø¯", "Ø´Ø¯", "Ù…ÛŒ", "Ù‡Ù…ÛŒÙ†", "Ú†Ù†ÛŒÙ†",
    "Ø§Ù…Ø§", "Ø§Ú¯Ø±", "Ú†ÙˆÙ†", "ØªØ§", "Ù…Ø§", "Ù…Ù†", "ØªÙˆ", "Ø§Ùˆ", "Ø´Ù…Ø§", "Ø§ÛŒØ´Ø§Ù†"
])

def preprocess_text(text):
    if pd.isna(text) or not text:
        return ""
    text = normalizer.normalize(str(text))
    text = re.sub(r'http\S+|www\S+|#\w+|@\w+', '', text, flags=re.MULTILINE)
    text = re.sub(r'[^\u0600-\u06FF\s]', '', text) 
    text = re.sub(r'\s+', ' ', text).strip()
    
    tokens = tokenizer.tokenize_words(text)
    filtered_tokens = [token for token in tokens if token not in stop_words and len(token) > 1]
    
    return ' '.join(filtered_tokens)

df['cleaned_text'] = df['text'].apply(preprocess_text)
print("âœ… Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ Parsivar Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")

# --- Ø¬: ØªÙˆÚ©Ù†â€ŒØ³Ø§Ø²ÛŒ Ùˆ Padding Ø¨Ø±Ø§ÛŒ LSTM ---
MAX_WORDS = 5000     # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ù…Ø§Øª Ø¯Ø± ÙˆØ§Ú˜Ù‡â€ŒÙ†Ø§Ù…Ù‡
MAX_SEQ_LENGTH = 100 # Ø­Ø¯Ø§Ú©Ø«Ø± Ø·ÙˆÙ„ Ù‡Ø± ØªÙˆØ¦ÛŒØª (Ø¨Ø±Ø§ÛŒ Padding)
EMBEDDING_DIM = 100  # Ø§Ø¨Ø¹Ø§Ø¯ ÙØ¶Ø§ÛŒ ØªØ¹Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ

keras_tokenizer = KerasTokenizer(num_words=MAX_WORDS, oov_token="<unk>")
keras_tokenizer.fit_on_texts(df['cleaned_text'])

sequences = keras_tokenizer.texts_to_sequences(df['cleaned_text'])

# ÛŒÚ©Ø³Ø§Ù†â€ŒØ³Ø§Ø²ÛŒ Ø·ÙˆÙ„ Ø¯Ù†Ø¨Ø§Ù„Ù‡â€ŒÙ‡Ø§
X = pad_sequences(sequences, maxlen=MAX_SEQ_LENGTH)
y = df['label'].values

num_classes = len(np.unique(y))
y_one_hot = to_categorical(y, num_classes=num_classes)

print(f"âœ… Ø§Ø¨Ø¹Ø§Ø¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ (X): {X.shape}")
print(f"âœ… Ø§Ø¨Ø¹Ø§Ø¯ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ (y_one_hot): {y_one_hot.shape}")
# --- Ø§Ù„Ù: ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ (Û¶Û°:Û³Û°:Û±Û°) ---
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y_one_hot, test_size=0.4, random_state=42, stratify=y
)
validation_ratio_from_temp = 0.25 
X_test, X_val, y_test, y_val = train_test_split(
    X_temp, y_temp, 
    test_size=validation_ratio_from_temp, 
    random_state=42, 
    # stratify Ø¨Ø± Ø§Ø³Ø§Ø³ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
    stratify=np.argmax(y_temp, axis=1) 
)

print(f"âœ… ØªÙ‚Ø³ÛŒÙ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: Train: {len(X_train)}, Test: {len(X_test)}, Validation: {len(X_val)}")

# --- Ø¨: Ø³Ø§Ø®Øª Ù…Ø¯Ù„ LSTM ---
model = Sequential()

# Û±. Ù„Ø§ÛŒÙ‡ ØªØ¹Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ
model.add(Embedding(input_dim=MAX_WORDS, 
                    output_dim=EMBEDDING_DIM, 
                    input_length=MAX_SEQ_LENGTH, 
                    name='embedding_layer'))

# Û². Ù„Ø§ÛŒÙ‡ LSTM (Ø¨Ø§ ØªÚ©Ù†ÛŒÚ©â€ŒÙ‡Ø§ÛŒ Ø¶Ø¯ Overfitting)
model.add(LSTM(
    units=128, 
    dropout=0.4,          # Dropout Ø¨Ø± Ø±ÙˆÛŒ Ø§ØªØµØ§Ù„Ø§Øª ÙˆØ±ÙˆØ¯ÛŒ
    recurrent_dropout=0.4, # Dropout Ø¨Ø± Ø±ÙˆÛŒ Ø§ØªØµØ§Ù„Ø§Øª Ø¨Ø§Ø²Ú¯Ø´ØªÛŒ (Ø¨Ø³ÛŒØ§Ø± Ù…Ø¤Ø«Ø± Ø¨Ø±Ø§ÛŒ LSTM)
    return_sequences=False # Ø®Ø±ÙˆØ¬ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ú¯Ø§Ù… Ø²Ù…Ø§Ù†ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
))

# Û³. Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Dense (ÙØ´Ø±Ø¯Ù‡) Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5)) # Dropout Ø¨ÛŒØ´ØªØ±

# Û´. Ù„Ø§ÛŒÙ‡ Ø®Ø±ÙˆØ¬ÛŒ
model.add(Dense(num_classes, activation='softmax'))

# Ú©Ø§Ù…Ù¾Ø§ÛŒÙ„ Ù…Ø¯Ù„
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

model.summary()
print("\nğŸš€ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM Ø¢ØºØ§Ø² Ø´Ø¯...")

# --- Ø¬: Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø¨Ø§ Early Stopping ---
# Early Stopping Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø®ÙˆØ¯Ú©Ø§Ø± Ø§Ø² Overfitting
early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor='val_loss', 
    patience=4,         # Ø§ÙØ²Ø§ÛŒØ´ ØµØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„Ù‡Ø§ÛŒ Ù…ØªÙˆØ§Ù„ÛŒ
    restore_best_weights=True
)

history = model.fit(
    X_train, y_train,
    epochs=50, # ØªØ¹Ø¯Ø§Ø¯ Ø¯ÙˆØ±Ù‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø± Ø²ÛŒØ§Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Early Stopping Ú©Ø§Ø± Ú©Ù†Ø¯
    batch_size=32,
    validation_data=(X_val, y_val), 
    callbacks=[early_stopping],
    verbose=1
)

print("âœ… Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ LSTM Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯.")

# --- Ø¯: Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ³Øª ---
y_prob_lstm = model.predict(X_test)
y_pred_lstm = np.argmax(y_prob_lstm, axis=1) 
y_test_labels = np.argmax(y_test, axis=1)
from sklearn.preprocessing import LabelBinarizer

# --- ØªØ¹Ø±ÛŒÙ Ù†Ø§Ù… Ø¯Ø³ØªÙ‡â€ŒÙ‡Ø§ ---
LABELS = [0, 1, 2]
CLASS_NAMES = {
    0: 'Non-related', 
    1: 'Indirect Suicide Signs', 
    2: 'Direct Suicide Signs'
}

# --- Ø§Ù„Ù: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø§Ú©Ø³Ù„ ---

# Û±. Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ
accuracy = accuracy_score(y_test_labels, y_pred_lstm)
kappa = cohen_kappa_score(y_test_labels, y_pred_lstm)
mcc = matthews_corrcoef(y_test_labels, y_pred_lstm)
f1_macro = f1_score(y_test_labels, y_pred_lstm, average='macro')
f1_weighted = f1_score(y_test_labels, y_pred_lstm, average='weighted')
mae = mean_absolute_error(y_test_labels, y_pred_lstm)
rmse = math.sqrt(np.mean((y_test_labels - y_pred_lstm)**2)) 
recall_macro = recall_score(y_test_labels, y_pred_lstm, average='macro')

# Û². AUC-ROC (OVR)
try:
    auc_roc_ovr = roc_auc_score(y_test, y_prob_lstm, multi_class='ovr')
except ValueError:
    auc_roc_ovr = "N/A"

# Û³. Specificity (Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³)
tn_rate = {}
for i in LABELS:
    tn, fp, fn, tp = confusion_matrix(y_test_labels == i, y_pred_lstm == i).ravel()
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0 
    tn_rate[i] = specificity

# Û´. Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø§Ú©Ø³Ù„
results = {
    'Metric': [
        'Accuracy', "Cohen's Kappa", 'MCC', 'F1-score (Macro)', 
        'F1-score (Weighted)', 'MAE', 'RMSE', 'Macro Average Recall (Sensitivity)', 
        'AUC-ROC (OVR)'
    ],
    'Value': [
        accuracy, kappa, mcc, f1_macro, f1_weighted, mae, rmse, 
        recall_macro, auc_roc_ovr
    ]
}
results_df = pd.DataFrame(results)

for label, name in CLASS_NAMES.items():
    results_df = pd.concat([results_df, pd.DataFrame({
        'Metric': [f'Specificity ({name})'],
        'Value': [tn_rate[label]]
    })], ignore_index=True)

output_results_file = 'LSTM_Evaluation_Metrics.xlsx'
results_df.to_excel(output_results_file, index=False)
files.download(output_results_file)
print("âœ… Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ùˆ Ø¯Ø± ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
print("\n--- Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø§Ø² Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ ---")
print(results_df.head(10).to_string(index=False))

# --- Ø¨: ØªØ±Ø³ÛŒÙ… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ ---

# Û±. Confusion Matrix
cm = confusion_matrix(y_test_labels, y_pred_lstm, labels=LABELS)
cm_df = pd.DataFrame(cm, index=CLASS_NAMES.values(), columns=CLASS_NAMES.values())

plt.figure(figsize=(8, 6))
sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='black')
plt.title('Confusion Matrix', fontsize=16)
plt.ylabel('True Label (Actual)', fontsize=14)
plt.xlabel('Predicted Label', fontsize=14)
plt.show()


# Û². AUC-ROC Curve per Class
plt.figure(figsize=(10, 7))
lb = LabelBinarizer()
y_test_binarized = lb.fit_transform(y_test_labels) 

for i in range(len(LABELS)):
    fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob_lstm[:, i])
    auc_score = roc_auc_score(y_test_binarized[:, i], y_prob_lstm[:, i])
    
    plt.plot(fpr, tpr, 
             label=f"Class {LABELS[i]} ({CLASS_NAMES[LABELS[i]]}) AUC = {auc_score:.2f}")

plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)
plt.ylabel('True Positive Rate (Recall/Sensitivity)', fontsize=12)
plt.title('AUC-ROC Curve per Class (One-vs-Rest Strategy)', fontsize=16)
plt.legend(loc="lower right")
plt.grid(True)
plt.show()


# --- Ø¬: Ù†Ù…ÙˆØ¯Ø§Ø± Loss Ùˆ Accuracy Ø¢Ù…ÙˆØ²Ø´ ---
plt.figure(figsize=(12, 5))

# Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

# Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()


print("\nâœ… ØªÙ…Ø§Ù…ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³ØªÛŒ ØªÙˆÙ„ÛŒØ¯ Ùˆ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù†Ø¯.")
print("âœ… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù†ØµØ¨ Ùˆ ÙˆØ§Ø±Ø¯ Ø´Ø¯Ù†Ø¯.")
