import pandas as pd
import re
from google.colab import files
import io
from transformers import AutoModel, AutoTokenizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score, roc_curve, mean_squared_error
from sklearn.preprocessing import label_binarize
from sklearn.calibration import CalibrationDisplay
from scipy.stats import chi2_contingency
from imblearn.over_sampling import SMOTE
import time
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch

# Ø²Ù…Ø§Ù†â€ŒØ³Ù†Ø¬ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´
start_time = time.time()

# ØªØ¹Ø±ÛŒÙ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„ Ùˆ Ø¯ÙˆÙ…
category_1 = [
    "Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ø¢Ù…Ø§Ø¯Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù†Ù‚Ø´Ù‡ Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯ Ø®ÙˆØ¯Ø®ÙˆØ§Ø³ØªÙ‡", "Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡Ù… Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…",
    "ØªØ§ Ø§Ø¨Ø¯ Ø¨Ø®ÙˆØ§Ø¨Ù…", "Ù‡ÛŒÚ†ÙˆÙ‚Øª Ø¨ÛŒØ¯Ø§Ø± Ù†Ø´Ù…", "Ø¨Ù‡ Ø²Ù†Ø¯Ú¯ÛŒâ€ŒØ§Ù… Ù¾Ø§ÛŒØ§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ù…", "Ù†Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯Ù…",
    "Ù†Ù…ÛŒâ€ŒØ®ÙˆØ§Ù… Ø²Ù†Ø¯Ù‡ Ø¨Ø§Ø´Ù…", "Ø®ÙˆØ¯Ø³ÙˆØ²ÛŒ", "Ø²Ù†Ø¯Ú¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù† ØªÙ…Ø§Ù… Ø´Ø¯Ù‡", "Ø®ÙˆØ¯Ù… Ø±Ø§ Ø®Ù„Ø§Øµ Ú©Ù†Ù…",
    "Ø®ÙˆØ¯Ù…Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ø®ÙˆØ¯Ù… Ø±Ùˆ Ø®Ù„Ø§Øµ Ú©Ù†Ù…", "Ú†Ø±Ø§ Ù†Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯Ù… Ø±Ø§ Ø¨Ú©Ø´Ù…ØŸ", "#Ø®ÙˆØ¯Ú©Ø´ÛŒ"
]

category_2 = [
    "Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø¯Ø§Ø±Ù… Ø¹Ø°Ø§Ø¨ Ù…ÛŒâ€ŒÚ©Ø´Ù…", "Ù‡ÛŒÚ† Ø§Ù…ÛŒØ¯ÛŒ Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ù…ØªÙ†ÙØ±Ù…", "Ø§Ø² Ø®ÙˆØ¯Ù… Ø¨Ø¯Ù… Ù…ÛŒØ§Ø¯",
    "Ú†ÛŒØ²ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ø² Ø¯Ø³Øª Ø¯Ø§Ø¯Ù† Ù†Ø¯Ø§Ø±Ù…", "Ø§Ø² Ø²Ù†Ø¯Ú¯ÛŒ Ø®Ø³ØªÙ‡â€ŒØ§Ù…", "ØªÙ†Ù‡Ø§ Ù…ÛŒâ€ŒÙ…ÛŒØ±Ù…", "Ù…Ø±Ú¯", "Ø¯Ø±Ø¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ",
    "ØªÙ†Ù‡Ø§ÛŒÛŒ", "ØºÙ…", "Ø¨ÛŒâ€ŒØ§Ø±Ø²Ø´", "Ø®Ø³ØªÚ¯ÛŒ", "ÙØ´Ø§Ø± Ø±ÙˆÙ…Ù‡", "Ø§Ø³ØªØ±Ø³", "Ø§Ø³ØªØ±Ø³ Ø¯Ø§Ø±Ù…", "Ù‡Ù…Ù‡ Ú†ÛŒØ² Ø¨ÛŒâ€ŒÙØ§ÛŒØ¯Ù‡ Ø§Ø³Øª",
    "Ù‡ÛŒÚ†Ú©Ø³ Ù…Ø±Ø§ Ù†Ù…ÛŒâ€ŒÙÙ‡Ù…Ø¯", "Ø§Ø² Ù‡Ù…Ù‡ Ú†ÛŒØ² Ù…ØªÙ†ÙØ±Ù…", "#Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "#ØªÙ†Ù‡Ø§ÛŒÛŒ", "#Ø¯Ø±Ø¯", "#ØºÙ…", "#Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"
]

# Ú©Ù„Ù…Ø§Øª Ø²Ù…ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ØªÙÚ©ÛŒÚ© Ø®ÙˆØ¯Ø²Ù†ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Ùˆ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ
serious_context = [
    "Ù‚Ø±Øµ", "Ø®ÙˆÙ†", "Ø¢Ø³ÛŒØ¨", "Ú¯Ø±ÛŒÙ‡", "Ø¬ÛŒØº", "Ø¯Ø±Ø¯", "Ø§ÙØ³Ø±Ø¯Ú¯ÛŒ", "Ø®ÙˆØ¯Ú©Ø´ÛŒ", "Ù…Ø±Ú¯", "Ù†Ø§Ø§Ù…ÛŒØ¯ÛŒ"
]
humorous_context = [
    "ğŸ˜‚", "ğŸ˜", "ğŸ¤£", "Ø³ÙˆØªÛŒ", "Ú©ØµØ®Ù„", "Ø®Ù†Ø¯Ù‡", "Ø´ÙˆØ®ÛŒ", "ÙØ§Ù†", "Ù‡Ù‡Ù‡", "Ù‡Ø§Ù‡Ø§"
]

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ Ø±ÙˆØ´ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ
def label_tweet(tweet):
    tweet = str(tweet).lower()
    
    if "Ø®ÙˆØ¯Ø²Ù†ÛŒ" in tweet:
        if any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in serious_context):
            return 2, "Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.9
        elif any(re.search(r'\b' + re.escape(word) + r'\b', tweet) for word in humorous_context) or "ğŸ˜‚" in tweet or "ğŸ˜" in tweet:
            return 0, "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.3
        else:
            return 0, "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ø®ÙˆØ¯Ø²Ù†ÛŒ", 0.3
    
    for word in category_1:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return 2, "Ø¯Ø³ØªÙ‡ Ø§ÙˆÙ„", word, 0.9
    
    for word in category_2:
        if re.search(r'\b' + re.escape(word) + r'\b', tweet):
            return 1, "Ø¯Ø³ØªÙ‡ Ø¯ÙˆÙ…", word, 0.65
    
    return 0, "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", "Ù‡ÛŒÚ†â€ŒÚ©Ø¯Ø§Ù…", 0.3

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Specificity
def specificity_score(y_true, y_pred, classes):
    cm = confusion_matrix(y_true, y_pred, labels=classes)
    specificity_per_class = []
    for i in range(len(classes)):
        tn = np.sum(cm) - np.sum(cm[i, :]) - np.sum(cm[:, i]) + cm[i, i]
        fp = np.sum(cm[:, i]) - cm[i, i]
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        specificity_per_class.append(specificity)
    return np.mean(specificity_per_class)

# ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯ Ø¨Ø§ ParsBERT
def get_parsbert_embeddings(texts, model, tokenizer, batch_size=32, device='cuda' if torch.cuda.is_available() else 'cpu'):
    model.to(device)
    model.eval()
    embeddings = []
    
    for i in range(0, len(texts), batch_size):
        batch_texts = texts[i:i + batch_size]
        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=128)
        inputs = {key: val.to(device) for key, val in inputs.items()}
        
        with torch.no_grad():
            outputs = model(**inputs)
            batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token
        embeddings.append(batch_embeddings)
    
    return np.vstack(embeddings)

# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§
!pip install transformers imbalanced-learn scikit-learn matplotlib seaborn

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ 'labeled_tweets_3000.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded_train = files.upload()

if not uploaded_train:
    raise ValueError("ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ 'labeled_tweets_3000.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ
train_file = list(uploaded_train.keys())[0]
df_train = pd.read_excel(io.BytesIO(uploaded_train[train_file]))

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
required_columns = ['Ù…ØªÙ† ØªÙˆÛŒÛŒØª', 'Ø¨Ø±Ú†Ø³Ø¨']
if not all(col in df_train.columns for col in required_columns):
    raise ValueError("Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' ÛŒØ§ 'Ø¨Ø±Ú†Ø³Ø¨' Ø¯Ø± ÙØ§ÛŒÙ„ Ø¢Ù…ÙˆØ²Ø´ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
X_train_texts = df_train['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'].astype(str).tolist()
y_train = df_train['Ø¨Ø±Ú†Ø³Ø¨'].astype(int).values

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ùˆ ØªÙˆÚ©Ù†Ø§ÛŒØ²Ø± ParsBERT
model_name = "HooshvareLab/bert-fa-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModel.from_pretrained(model_name)

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¨Ø§ ParsBERT...")
X_train_embeddings = get_parsbert_embeddings(X_train_texts, model, tokenizer, batch_size=32)

# Ø§Ø¹Ù…Ø§Ù„ SMOTE Ø¨Ø±Ø§ÛŒ Ø±ÙØ¹ Ù†Ø§Ù…ØªÙˆØ§Ø²Ù†ÛŒ
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train_embeddings, y_train)

# Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ RandomForest
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train_balanced, y_train_balanced)

# Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
print("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ 'new_tweets.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯:")
uploaded_new = files.upload()

if not uploaded_new:
    raise ValueError("ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ Ø¢Ù¾Ù„ÙˆØ¯ Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ 'new_tweets.xlsx' Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯.")

# Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
new_file = list(uploaded_new.keys())[0]
df_new = pd.read_excel(io.BytesIO(uploaded_new[new_file]))

# Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ† "Ù…ØªÙ† ØªÙˆÛŒÛŒØª"
if 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' not in df_new.columns:
    raise ValueError("Ø³ØªÙˆÙ† 'Ù…ØªÙ† ØªÙˆÛŒÛŒØª' Ø¯Ø± ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

# ØªØ¨Ø¯ÛŒÙ„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ù‡ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯
print("Ø¯Ø± Ø­Ø§Ù„ ØªÙˆÙ„ÛŒØ¯ Ø§Ù…Ø¨Ø¯ÛŒÙ†Ú¯â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø§ ParsBERT...")
X_new_texts = df_new['Ù…ØªÙ† ØªÙˆÛŒÛŒØª'].astype(str).tolist()
X_new_embeddings = get_parsbert_embeddings(X_new_texts, model, tokenizer, batch_size=32)

# Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø¯Ù„
y_pred_new = classifier.predict(X_new_embeddings)

# Ø§ØµÙ„Ø§Ø­ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ¯Ø²Ù†ÛŒ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ
for i, (tweet, label) in enumerate(zip(X_new_texts, y_pred_new)):
    tweet_lower = tweet.lower()
    if "Ø®ÙˆØ¯Ø²Ù†ÛŒ" in tweet_lower:
        if any(word in tweet_lower for word in humorous_context) or "ğŸ˜‚" in tweet_lower or "ğŸ˜" in tweet_lower:
            y_pred_new[i] = 0  # Ø®ÙˆØ¯Ø²Ù†ÛŒ Ø§Ø³ØªØ¹Ø§Ø±ÛŒ: Ø¨Ø±Ú†Ø³Ø¨ 0

# Ù„ÛŒØ¨Ù„â€ŒÚ¯Ø°Ø§Ø±ÛŒ Ø¨Ø§ Ø±ÙˆØ´ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡
y_ref_new = []
ref_categories = []
ref_matched_words = []
ref_similarities = []
for tweet in X_new_texts:
    label, category, matched_word, similarity = label_tweet(tweet)
    y_ref_new.append(label)
    ref_categories.append(category)
    ref_matched_words.append(matched_word)
    ref_similarities.append(similarity)
y_ref_new = np.array(y_ref_new)

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
accuracy = accuracy_score(y_ref_new, y_pred_new)
precision_weighted = precision_score(y_ref_new, y_pred_new, average='weighted', zero_division=0)
recall_weighted = recall_score(y_ref_new, y_pred_new, average='weighted', zero_division=0)
f1_weighted = f1_score(y_ref_new, y_pred_new, average='weighted', zero_division=0)
precision_macro = precision_score(y_ref_new, y_pred_new, average='macro', zero_division=0)
recall_macro = recall_score(y_ref_new, y_pred_new, average='macro', zero_division=0)
f1_macro = f1_score(y_ref_new, y_pred_new, average='macro', zero_division=0)
specificity_macro = specificity_score(y_ref_new, y_pred_new, classes=[0, 1, 2])

# Ù…Ø­Ø§Ø³Ø¨Ù‡ AUC
y_ref_bin = label_binarize(y_ref_new, classes=[0, 1, 2])
y_pred_prob = classifier.predict_proba(X_new_embeddings)
auc = roc_auc_score(y_ref_bin, y_pred_prob, multi_class='ovr', average='weighted')

# Ù…Ø­Ø§Ø³Ø¨Ù‡ RMSE
rmse = np.sqrt(mean_squared_error(y_ref_new, y_pred_new))

# Ù…Ø­Ø§Ø³Ø¨Ù‡ Confusion Matrix
cm = confusion_matrix(y_ref_new, y_pred_new, labels=[0, 1, 2])

# Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ù†Ø­Ù†ÛŒ ROC
plt.figure(figsize=(8, 6))
for i in range(3):
    fpr, tpr, _ = roc_curve(y_ref_bin[:, i], y_pred_prob[:, i])
    plt.plot(fpr, tpr, label=f'ROC Ù…Ù†Ø­Ù†ÛŒ Ú©Ù„Ø§Ø³ {i} (AUC = {roc_auc_score(y_ref_bin[:, i], y_pred_prob[:, i]):.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Ù†Ø±Ø® Ù…Ø«Ø¨Øª Ú©Ø§Ø°Ø¨ (False Positive Rate)')
plt.ylabel('Ù†Ø±Ø® Ù…Ø«Ø¨Øª ÙˆØ§Ù‚Ø¹ÛŒ (True Positive Rate)')
plt.title('Ù…Ù†Ø­Ù†ÛŒ ROC Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³')
plt.legend(loc="lower right")
plt.savefig('roc_curve.png')
plt.show()
files.download('roc_curve.png')

# Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ†
plt.figure(figsize=(8, 6))
for i in range(3):
    disp = CalibrationDisplay.from_predictions(y_ref_bin[:, i], y_pred_prob[:, i], n_bins=10, name=f'Ú©Ù„Ø§Ø³ {i}')
    disp.plot(ax=plt.gca(), name=f'Ú©Ù„Ø§Ø³ {i}')
plt.title('Ù…Ø§ØªØ±ÛŒØ³ Ú©Ø§Ù„ÛŒØ¨Ø±Ø§Ø³ÛŒÙˆÙ† Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú©Ù„Ø§Ø³')
plt.savefig('calibration_plot.png')
plt.show()
files.download('calibration_plot.png')

# Ø±Ø³Ù… Ùˆ Ø°Ø®ÛŒØ±Ù‡ Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2'], yticklabels=['0', '1', '2'])
plt.xlabel('Ø¨Ø±Ú†Ø³Ø¨ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒâ€ŒØ´Ø¯Ù‡')
plt.ylabel('Ø¨Ø±Ú†Ø³Ø¨ Ù…Ø±Ø¬Ø¹ (ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ)')
plt.title('Ù…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ')
plt.savefig('confusion_matrix.png')
plt.show()
files.download('confusion_matrix.png')

# Ø¢Ø²Ù…ÙˆÙ† Ø¢Ù…Ø§Ø±ÛŒ Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ (Chi-Square)
cm_flat = cm.ravel()
if len(cm_flat) == 9:  # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ù…Ø§ØªØ±ÛŒØ³ 3x3
    chi2, p_value, _, _ = chi2_contingency(cm)
else:
    chi2, p_value = np.nan, np.nan

# Ú†Ø§Ù¾ Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ
print("Ù…Ø¹ÛŒØ§Ø±Ù‡Ø§ÛŒ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ Ù…Ø¯Ù„ Ø±ÙˆÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ (Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù„ÛŒØ¨Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ú©Ù„ÛŒØ¯ÛŒ):")
print(f"1. Ø¯Ù‚Øª (Accuracy): {accuracy:.2%}")
print(f"2. Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision, weighted): {precision_weighted:.2%}")
print(f"3. ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall, weighted): {recall_weighted:.2%}")
print(f"4. Ø§Ù…ØªÛŒØ§Ø² F1 (F1-Score, weighted): {f1_weighted:.2%}")
print(f"5. Ø¯Ù‚Øª Ù…Ø«Ø¨Øª (Precision, macro): {precision_macro:.2%}")
print(f"6. ÛŒØ§Ø¯Ø¢ÙˆØ±ÛŒ (Recall, macro): {recall_macro:.2%}")
print(f"7. Ø§Ù…ØªÛŒØ§Ø² F1 (F1-Score, macro): {f1_macro:.2%}")
print(f"8. ÙˆÛŒÚ˜Ú¯ÛŒ (Specificity, macro): {specificity_macro:.2%}")
print(f"AUC (weighted): {auc:.2%}")
print(f"RMSE: {rmse:.4f}")
print("\nÙ…Ø§ØªØ±ÛŒØ³ Ø¯Ø±Ù‡Ù…â€ŒØ±ÛŒØ®ØªÚ¯ÛŒ (Confusion Matrix):")
print(cm)
print(f"\nØ¢Ø²Ù…ÙˆÙ† Chi-Square: Ù…Ù‚Ø¯Ø§Ø± chi2 = {chi2:.2f}, Ù…Ù‚Ø¯Ø§Ø± p-value = {p_value:.4f}")
print("\nÚ¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ø·Ø¨Ù‚Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:")
print(classification_report(y_ref_new, y_pred_new, zero_division=0))

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø®Ø±ÙˆØ¬ÛŒ
df_output = df_new.copy()[['Ù…ØªÙ† ØªÙˆÛŒÛŒØª']]
df_output['Ø¨Ø±Ú†Ø³Ø¨'] = y_pred_new
df_output['Ù…Ø¬Ù…ÙˆØ¹Ù‡ ÙˆØ§Ú˜Ú¯Ø§Ù†'] = ref_categories
df_output['ÙˆØ§Ú˜Ù‡/Ø¹Ø¨Ø§Ø±Øª Ù…Ø´Ø§Ø¨Ù‡'] = ref_matched_words
df_output['Ø¢Ø³ØªØ§Ù†Ù‡ Ø´Ø¨Ø§Ù‡Øª'] = ref_similarities

# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø®Ø±ÙˆØ¬ÛŒ
output_excel = "new_labeled_tweets.xlsx"
df_output.to_excel(output_excel, index=False)

# Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø¨Ø§ Ø¬Ø¯Ø§Ú©Ù†Ù†Ø¯Ù‡ |
output_text = "new_labeled_tweets.txt"
df_output.to_csv(output_text, sep='|', index=False, encoding='utf-8')

print(f"ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_excel}")
print(f"ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {output_text}")
print(f"Ø²Ù…Ø§Ù† Ú©Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´: {time.time() - start_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")

# Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ
files.download(output_excel)
files.download(output_text)
